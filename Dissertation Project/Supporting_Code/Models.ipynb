{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "qQFt-Zjta5fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from IPython import display\n",
        "import math\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.distributions.bernoulli import Bernoulli\n"
      ],
      "metadata": {
        "id": "Q3JOPVcdBPtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoders\n",
        "\n",
        "Below are the classes for the autoencoders with both 2 and 3 hidden layers. The input size, dimensions of the hidden layers, the latent size are the tunable parameters of the models.\n",
        "\n",
        "Each class contains 3 functions:\n",
        "\n",
        "The encode function - which reduces input data to the dimensions of the specified latent size\n",
        "\n",
        "The decode function -  which decodes data from latent variable representation back into data with original dimensions.\n",
        "\n",
        "The forward function - which is a standard function in neural networks which when called, in this case returns the output from the whole process of encoding and decoding data."
      ],
      "metadata": {
        "id": "oJK-kDXbXcV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71s8zNaT34bA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class AE2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, latent_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, latent_size)\n",
        "        self.fc4 = nn.Linear(latent_size, hidden_size2)\n",
        "        self.fc5 = nn.Linear(hidden_size2, hidden_size1)\n",
        "        self.fc6 = nn.Linear(hidden_size1, input_size)\n",
        "        self.lrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        p_x = self.lrelu(self.fc1(x))\n",
        "        p_x = self.lrelu(self.fc2(p_x))\n",
        "        p_x = self.lrelu(self.fc3(p_x))\n",
        "\n",
        "        return p_x\n",
        "\n",
        "    def decode(self, z_x):\n",
        "        q_x = self.lrelu(self.fc4(z_x))\n",
        "        q_x = self.lrelu(self.fc5(q_x))\n",
        "        q_x = torch.sigmoid(self.fc6(q_x))\n",
        "\n",
        "        return q_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        p_x = self.encode(x)\n",
        "        q_z = self.decode(p_x)\n",
        "\n",
        "        return q_z, p_x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class AE3(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, latent_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, latent_size)\n",
        "        self.fc5 = nn.Linear(latent_size, hidden_size3)\n",
        "        self.fc6 = nn.Linear(hidden_size3, hidden_size2)\n",
        "        self.fc7 = nn.Linear(hidden_size2, hidden_size1)\n",
        "        self.fc8 = nn.Linear(hidden_size1, input_size)\n",
        "        self.lrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        p_x = self.lrelu(self.fc1(x))\n",
        "        p_x = self.lrelu(self.fc2(p_x))\n",
        "        p_x = self.lrelu(self.fc3(p_x))\n",
        "\n",
        "        return p_x\n",
        "\n",
        "    def decode(self, z_x):\n",
        "        q_x = self.lrelu(self.fc4(z_x))\n",
        "        q_x = self.lrelu(self.fc5(q_x))\n",
        "        q_x = torch.sigmoid(self.fc6(q_x))\n",
        "\n",
        "        return q_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        p_x = self.encode(x)\n",
        "        q_z = self.decode(p_x)\n",
        "\n",
        "        return q_z, p_x"
      ],
      "metadata": {
        "id": "GMJE5tT34aN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder\n",
        "\n",
        "Below is the class for the variational autoencoder with 2 hidden layers. The input size, dimensions of the hidden layers, the latent size are the tunable parameters of the models.\n",
        "\n",
        "The class contains the same 3 functions as the standard AE although they work slightly differently and produce different outputs:\n",
        "\n",
        "The encode function - this reduced the data to a mean and standard deviation of the latent vairables. The latent variables are then obtained by sampling from normal distributions with the means and standard deviations (which is multiplied by some noise or error taken from a normal distribution) of the latent variables. The encode function takes data as an input and produces the latent variables, mean, log variance, and standard deviation as the ouptut.\n",
        "\n",
        "The decode function -  takes latent variables as an input and decodes the data from latent variable representation back into data with original dimensions.\n",
        "\n",
        "The forward function - in this case the forward function takes the data and the beta weighting value as input. It runs the data through the encode and decode functions, and returns the optimization criterion (made up of the sum of the kl divergence term multiplied by beta, and negative cross entropy loss), the ouput of the decode function and the latent variables."
      ],
      "metadata": {
        "id": "wWS_PNiJZbZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class BernoulliVAE2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2,  latent_size):\n",
        "        super(BernoulliVAE2, self).__init__()\n",
        "        # Encoder parameters\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, latent_size)\n",
        "        self.lrelu = nn.LeakyReLU(0.1)\n",
        "        self.enc_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.enc_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "        # Distribution to sample for the reparameterization trick\n",
        "        self.normal_dist = MultivariateNormal(torch.zeros(latent_size),\n",
        "                                              torch.eye(latent_size))\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.fc4 = nn.Linear(latent_size, hidden_size2)\n",
        "        self.fc5 = nn.Linear(hidden_size2, hidden_size1)\n",
        "        self.fc6 = nn.Linear(hidden_size1, input_size)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Reconstruction loss: binary cross-entropy\n",
        "        self.criterion = nn.BCELoss(reduction='sum')\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Obtain the parameters of the latent variable distribution\n",
        "        h = self.lrelu(self.fc1(x))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.lrelu(self.fc3(h))\n",
        "        mu_e = self.enc_mu(h)\n",
        "        logvar_e = self.enc_logvar(h)\n",
        "        std = torch.exp(0.5 * logvar_e)\n",
        "        noise = torch.randn_like(std)\n",
        "\n",
        "        # Get a latent variable sample with the reparameterization trick\n",
        "        \n",
        "        z = mu_e + (std * noise)\n",
        "\n",
        "        return z, mu_e, logvar_e, std\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Obtain the parameters of the observation distribution\n",
        "        h = self.lrelu(self.fc4(z))\n",
        "        h = self.lrelu(self.fc5(h))\n",
        "        output = torch.sigmoid(self.fc6(h))\n",
        "\n",
        "        return output\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, beta):\n",
        "        \"\"\" Calculate the negative lower bound for the given input \"\"\"\n",
        "        z, mu_e, logvar_e, std = self.encode(x)\n",
        "        output = self.decode(z)\n",
        "        neg_cross_entropy = self.criterion(output, x)\n",
        "        kl_div = -0.5* (1 + logvar_e - mu_e**2 - torch.exp(logvar_e)).sum()\n",
        "        beta = beta\n",
        "\n",
        "        # Since the optimizer minimizes, we return the negative\n",
        "        # of the lower bound that we need to maximize\n",
        "        return neg_cross_entropy + kl_div*beta, output, z"
      ],
      "metadata": {
        "id": "FnN8PU3l5aOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BernoulliVAE2_1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2,  latent_size):\n",
        "        super(BernoulliVAE2_1, self).__init__()\n",
        "        # Encoder parameters\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, latent_size)\n",
        "        self.lrelu = nn.LeakyReLU(0.1)\n",
        "        self.enc_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.enc_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "        # Distribution to sample for the reparameterization trick\n",
        "        self.normal_dist = MultivariateNormal(torch.zeros(latent_size),\n",
        "                                              torch.eye(latent_size))\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.fc4 = nn.Linear(latent_size, hidden_size2)\n",
        "        self.fc5 = nn.Linear(hidden_size2, hidden_size1)\n",
        "        self.fc6 = nn.Linear(hidden_size1, input_size)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Reconstruction loss: binary cross-entropy\n",
        "        self.criterion = nn.BCELoss(reduction='sum')\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Obtain the parameters of the latent variable distribution\n",
        "        h = self.lrelu(self.fc1(x))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.lrelu(self.fc3(h))\n",
        "        mu_e = self.enc_mu(h)\n",
        "        logvar_e = self.enc_logvar(h)\n",
        "        std = torch.exp(0.5 * logvar_e)\n",
        "        noise = torch.randn_like(std)\n",
        "\n",
        "        # Get a latent variable sample with the reparameterization trick\n",
        "        \n",
        "        z = mu_e + (std * noise)\n",
        "\n",
        "        return z, mu_e, logvar_e, std\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Obtain the parameters of the observation distribution\n",
        "        h = self.lrelu(self.fc4(z))\n",
        "        h = self.lrelu(self.fc5(h))\n",
        "        output = torch.sigmoid(self.fc6(h))\n",
        "\n",
        "        return output\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, beta):\n",
        "        \"\"\" Calculate the negative lower bound for the given input \"\"\"\n",
        "        z, mu_e, logvar_e, std = self.encode(x)\n",
        "        output = self.decode(z)\n",
        "        neg_cross_entropy = self.criterion(output, x)\n",
        "        kl_div = -0.5* (1 + logvar_e - mu_e**2 - torch.exp(logvar_e)).sum()\n",
        "        beta = beta\n",
        "\n",
        "        # Since the optimizer minimizes, we return the negative\n",
        "        # of the lower bound that we need to maximize\n",
        "        return neg_cross_entropy + kl_div*beta, output, z"
      ],
      "metadata": {
        "id": "4wsCj3rOZHuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
