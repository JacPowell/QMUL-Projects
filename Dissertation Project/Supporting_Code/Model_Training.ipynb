{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL80Wz53SOVr",
        "outputId": "39bfb5a2-875d-4019-b5a1-8b63b5db1a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Setting up google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import models as models\n",
        "import five_fold_training\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from IPython import display\n",
        "import math\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb"
      ],
      "metadata": {
        "id": "xfEH5O5YSW7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Data/MM_train_data'\n",
        "train_data = torch.load(train_path)\n",
        "test_path = '/content/drive/MyDrive/Data/MM_test_data'\n",
        "test_data = torch.load(test_path)\n"
      ],
      "metadata": {
        "id": "HFeyqWm4SbU1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ConcatDataset([test_data, train_data])"
      ],
      "metadata": {
        "id": "rOkyXrDLSmhM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net50 = models.AE2(input_size=204, \n",
        "      hidden_size1=196, \n",
        "      hidden_size2=128, \n",
        "      latent_size=50)"
      ],
      "metadata": {
        "id": "4g6pcS17StER"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net15b2 = models.BernoulliVAE2(input_size=204, \n",
        "      hidden_size1=196, \n",
        "      hidden_size2=128, \n",
        "      latent_size=15)"
      ],
      "metadata": {
        "id": "0Azs_7PMm9wl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net25b2 = models.BernoulliVAE2(input_size=204, \n",
        "      hidden_size1=196, \n",
        "      hidden_size2=128, \n",
        "      latent_size=25)"
      ],
      "metadata": {
        "id": "VqeCJcC4_lCV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net50b02 = models.BernoulliVAE2(input_size=204, \n",
        "      hidden_size1=196, \n",
        "      hidden_size2=128, \n",
        "      latent_size=50)"
      ],
      "metadata": {
        "id": "RJ8D22VLFRol"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()"
      ],
      "metadata": {
        "id": "AgYtWAb8U4dm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_AE_model(net, dataset, k, epochs, batch_size):\n",
        "  #https://github.com/christianversloot/machine-learning-articles/blob/main/\n",
        "  #how-to-use-k-fold-cross-validation-with-pytorch.md\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    \n",
        "    # Configuration options\n",
        "    k_folds = k\n",
        "    epochs = epochs\n",
        "    #loss_function = nn.CrossEntropyLoss()\n",
        "    loss_function = nn.MSELoss()\n",
        "    \n",
        "    # For fold results\n",
        "    train_results = {}\n",
        "    val_results = {}\n",
        "    \n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "\n",
        "    dataset = dataset\n",
        "    \n",
        "    # Define the K-fold Cross Validator\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "      \n",
        "    # Start print\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # K-fold Cross Validation model evaluation\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "      \n",
        "      # Print\n",
        "      print(f'FOLD {fold}')\n",
        "      print('--------------------------------')\n",
        "      \n",
        "      # Sample elements randomly from a given list of ids, no replacement.\n",
        "      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "      \n",
        "      # Define data loaders for training and testing data in this fold\n",
        "      trainloader = torch.utils.data.DataLoader(\n",
        "                        dataset, \n",
        "                        batch_size=batch_size, sampler=train_subsampler)\n",
        "      testloader = torch.utils.data.DataLoader(\n",
        "                        dataset,\n",
        "                        batch_size=batch_size, sampler=test_subsampler)\n",
        "      \n",
        "\n",
        "      net = net\n",
        "      net.apply(reset_weights)\n",
        "\n",
        "      # create an optimizer object\n",
        "      # Adam optimizer with learning rate 1e-3\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "      # Run the training loop for defined number of epochs\n",
        "      for epoch in range(1, epochs + 1):\n",
        "          print(f'\\n Epoch {epoch}')\n",
        "          loss = 0\n",
        "          training_loss = 0\n",
        "          test_loss = 0\n",
        "          val_latents = []\n",
        "          val_outputs = []\n",
        "          train_outputs = []\n",
        "          for i, (batch_features) in enumerate(trainloader):\n",
        "              optimizer.zero_grad()\n",
        "              # Reshape data so each image is an array with 784 elements\n",
        "              batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "              output, latent = net(batch_features)\n",
        "              criterion = loss_function(output, batch_features)\n",
        "              criterion.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              #avg_loss += loss.item()/len(train_data)\n",
        "\n",
        "              #z, mu_e, logvar_e = net.encode(batch_features)\n",
        "              #output = net.decode(z)\n",
        "              \n",
        "              train_outputs.append(output.detach().numpy())\n",
        "              \n",
        "              train_loss = loss_function(output, batch_features)\n",
        "              \n",
        "              # Print statistics\n",
        "              loss += train_loss.item()\n",
        "              training_loss += train_loss.item()\n",
        "              \n",
        "              \n",
        "\n",
        "              if i % 100 == 0:\n",
        "                  # Print average loss per sample in batch\n",
        "                  batch_loss = loss/len(batch_features)\n",
        "                  print(f'\\r[{i:d}/{len(batch_features):d}] batch loss: {batch_loss} ',\n",
        "                        end='', flush=True)\n",
        "              loss = 0\n",
        "            \n",
        "\n",
        "      # Process is complete.\n",
        "      print('Training process has finished. Saving trained model.')\n",
        "\n",
        "      # Saving the model\n",
        "      save_path = f'/content/drive/MyDrive/Data/AE50_batch{batch_size}_fold{fold}'\n",
        "      torch.save(net.state_dict(), save_path)\n",
        "\n",
        "      # Print about testing\n",
        "      #print('Starting testing')\n",
        "\n",
        "      # Evaluation for this fold\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Iterate over the test data and generate predictions\n",
        "        for i, (batch_features) in enumerate(testloader):\n",
        "\n",
        "            # Reshape data so each image is an array with 784 elements\n",
        "            batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "            #test_loss = net(batch_features)\n",
        "            output, latent = net(batch_features)\n",
        "            \n",
        "            val_outputs.append(output.detach().numpy())\n",
        "            val_latents.append(latent.detach().numpy())\n",
        "            val_loss = loss_function(output, batch_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            test_loss += val_loss.item()\n",
        "\n",
        "\n",
        "        train_outputs = np.concatenate( train_outputs, axis=0 )\n",
        "        val_outputs = np.concatenate( val_outputs, axis=0 )\n",
        "        val_latents = np.concatenate( val_latents, axis=0 )\n",
        "        model_train_loss = training_loss / len(train_outputs\n",
        "                          )\n",
        "        test_loss = test_loss / len(val_outputs\n",
        "                          )\n",
        "        #animator.add(epoch, (loss, test_loss))\n",
        "        #print(train_iter[0][:5])\n",
        "        #print(outputs[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Print fold loss\n",
        "        print(f'Training loss for fold {fold}: {model_train_loss}')\n",
        "        print(f'Validation loss for fold {fold}: {test_loss}')\n",
        "        print('--------------------------------')\n",
        "        train_results[fold] = model_train_loss\n",
        "        val_results[fold] = test_loss\n",
        "    # Print fold results\n",
        "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "    print('--------------------------------')\n",
        "    train_sum = 0.0\n",
        "    val_sum = 0.0\n",
        "    for key, value in train_results.items():\n",
        "      print(f'Training Fold {key} Loss: {value}')\n",
        "      train_sum += value\n",
        "    print(f'Trainig Average Loss: {train_sum/len(train_results.items())}')\n",
        "    train_avg_loss = train_sum/len(train_results.items())\n",
        "    for key, value in val_results.items():\n",
        "      print(f'Validation Fold {key} Loss: {value} ')\n",
        "      val_sum += value\n",
        "    print(f'Vaidation Average Loss: {val_sum/len(val_results.items())}')\n",
        "    val_avg_loss = val_sum/len(val_results.items())\n",
        "\n",
        " \n",
        "    full_outputs = []\n",
        "    full_outputs.append(train_avg_loss)\n",
        "    full_outputs.append(val_avg_loss)\n",
        "    full_outputs.append(val_latents)\n",
        "    full_outputs.append(val_outputs)\n",
        "    full_outputs.append(train_outputs)\n",
        "\n",
        "    \n",
        "\n",
        "    return full_outputs"
      ],
      "metadata": {
        "id": "s9CjNDUhS4oN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8192\n",
        "epochs = 30\n",
        "k = 5"
      ],
      "metadata": {
        "id": "JO0_6zeIUKs_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_50 = train_AE_model(net=net50,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Wn9mHnS_0D",
        "outputId": "84f3e75a-1c8d-4084-a098-64d16b82b85f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.067620127694681e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9660883228643797e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.7814106942969374e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.3081167455529794e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.3754688552580774e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.153146154654678e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.682006071263459e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.491478299925802e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.317632935155416e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.3592977969383355e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.296958756342065e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.217750408017309e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.202632332337089e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.223503416753374e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.149682460294571e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.24223582437844e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.10975609763409e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.148171334847575e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.100667062331922e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.085369816195453e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.151976660679793e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.1257841227634344e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.101546543708537e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.1396420935343485e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.085056495270692e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.160074346757028e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 7.116790129657602e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 7.114510481187608e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 7.085205197654432e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 7.105547410901636e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 7.175007842222236e-06\n",
            "Validation loss for fold 0: 7.2688693680055115e-06\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.0628052627434954e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9579980036942288e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.7450094421510585e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.2198300939635374e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.2790948858310003e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.071558113442734e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.5782163548865356e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.539362286479445e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.48683987694676e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.4843678703473415e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.2478101174056064e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.26395592209883e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.124752301024273e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.282970273081446e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.123624527594075e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.129218374757329e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.1209974521480035e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.104792985046515e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.125189313228475e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.170565368141979e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.10901213096804e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.168605407059658e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.049957275739871e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.096381978044519e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.080581326590618e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.150733381422469e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 7.083779109962052e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 7.103958068910288e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 7.1189438131114e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 7.125011507014278e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 7.146700353696079e-06\n",
            "Validation loss for fold 1: 7.1862290450193455e-06\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.044310687982943e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9331707992241718e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.736905480560381e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.280563785461709e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.3795184713671915e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.194961992558092e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.780760824971367e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.608055057062302e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.5484767876332626e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.380361239484046e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.389445272565354e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.298723630810855e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.367855687334668e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.30375859347987e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.3014771260204725e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.262340659508482e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.248766451084521e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.313374680961715e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.2726506914477795e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.245424967550207e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.266970442287857e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.200341769930674e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.203205313999206e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.2750422077660915e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.228887170640519e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.211812771856785e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 7.196156275313115e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 7.274166364368284e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 7.254736829054309e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 7.178511168604018e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 7.281111244069103e-06\n",
            "Validation loss for fold 2: 7.171656964942795e-06\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.052002648473717e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.955334639409557e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.7584688723436557e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.2724880182067864e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.3587014109361917e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.24284870759584e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.705687494308222e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.3908277045120485e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.390539849438937e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.397643003059784e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.33429715182865e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.249292593769496e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.267181899806019e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.162080692069139e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.142304184526438e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.171963716245955e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.1628487603447866e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.182629815360997e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.145302333810832e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.213302069430938e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.144826213334454e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.127246135496534e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.167333478719229e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.1345675678458065e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.126318905648077e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.055937203404028e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 7.108616500772769e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 7.0615546974295285e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 7.095831733749947e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 7.047220151434885e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 7.150941258691913e-06\n",
            "Validation loss for fold 3: 7.183664053317674e-06\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.067456418648362e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9731621907558292e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.8026141080772504e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.394170587649569e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.5378856915049255e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.704521860636305e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.779183761158492e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.513484888477251e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.5291313805792015e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.34797322365921e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.419301255140454e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.3282767516502645e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.317813924601069e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.2651127993594855e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.28196164345718e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.253556759678759e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.231365998450201e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.219577582873171e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.2548714342701714e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.187584287748905e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.204117537185084e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.173866379162064e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.238355010485975e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.241494131449144e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.211439424281707e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.09028790879529e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 7.162860129028559e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 7.143205493775895e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 7.102567451511277e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 7.058255960146198e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 7.200846575110348e-06\n",
            "Validation loss for fold 4: 7.11521977298887e-06\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 7.175007842222236e-06\n",
            "Training Fold 1 Loss: 7.146700353696079e-06\n",
            "Training Fold 2 Loss: 7.281111244069103e-06\n",
            "Training Fold 3 Loss: 7.150941258691913e-06\n",
            "Training Fold 4 Loss: 7.200846575110348e-06\n",
            "Trainig Average Loss: 7.190921454757936e-06\n",
            "Validation Fold 0 Loss: 7.2688693680055115e-06 \n",
            "Validation Fold 1 Loss: 7.1862290450193455e-06 \n",
            "Validation Fold 2 Loss: 7.171656964942795e-06 \n",
            "Validation Fold 3 Loss: 7.183664053317674e-06 \n",
            "Validation Fold 4 Loss: 7.11521977298887e-06 \n",
            "Vaidation Average Loss: 7.185127840854839e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dBph1jR_niOz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_VAE_model(net, dataset, k, epochs, batch_size, beta, latent_size):\n",
        "  #https://github.com/christianversloot/machine-learning-articles/blob/main/\n",
        "  #how-to-use-k-fold-cross-validation-with-pytorch.md\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    \n",
        "    # Configuration options\n",
        "    k_folds = k\n",
        "    epochs = epochs\n",
        "    #loss_function = nn.CrossEntropyLoss()\n",
        "    loss_function = nn.MSELoss()\n",
        "    \n",
        "    # For fold results\n",
        "    train_results = {}\n",
        "    val_results = {}\n",
        "    \n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "\n",
        "    dataset = dataset\n",
        "    \n",
        "    # Define the K-fold Cross Validator\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "      \n",
        "    # Start print\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # K-fold Cross Validation model evaluation\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "      \n",
        "      # Print\n",
        "      print(f'FOLD {fold}')\n",
        "      print('--------------------------------')\n",
        "      \n",
        "      # Sample elements randomly from a given list of ids, no replacement.\n",
        "      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "      \n",
        "      # Define data loaders for training and testing data in this fold\n",
        "      trainloader = torch.utils.data.DataLoader(\n",
        "                        dataset, \n",
        "                        batch_size=batch_size, sampler=train_subsampler)\n",
        "      testloader = torch.utils.data.DataLoader(\n",
        "                        dataset,\n",
        "                        batch_size=batch_size, sampler=test_subsampler)\n",
        "      \n",
        "\n",
        "      net = net\n",
        "      net.apply(reset_weights)\n",
        "\n",
        "      # create an optimizer object\n",
        "      # Adam optimizer with learning rate 1e-3\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "      # Run the training loop for defined number of epochs\n",
        "      for epoch in range(1, epochs + 1):\n",
        "          print(f'\\n Epoch {epoch}')\n",
        "          loss = 0\n",
        "          training_loss = 0\n",
        "          test_loss = 0\n",
        "          val_latents = []\n",
        "          val_outputs = []\n",
        "          train_outputs = []\n",
        "          for i, (batch_features) in enumerate(trainloader):\n",
        "              optimizer.zero_grad()\n",
        "              # Reshape data so each image is an array with 784 elements\n",
        "              batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "              criterion, output, latent = net(batch_features, beta)\n",
        "              criterion.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              #avg_loss += loss.item()/len(train_data)\n",
        "\n",
        "              #z, mu_e, logvar_e = net.encode(batch_features)\n",
        "              #output = net.decode(z)\n",
        "              \n",
        "              train_outputs.append(output.detach().numpy())\n",
        "              \n",
        "              train_loss = loss_function(output, batch_features)\n",
        "              \n",
        "              # Print statistics\n",
        "              loss += train_loss.item()\n",
        "              training_loss += train_loss.item()\n",
        "              \n",
        "              \n",
        "\n",
        "              if i % 100 == 0:\n",
        "                  # Print average loss per sample in batch\n",
        "                  batch_loss = loss/len(batch_features)\n",
        "                  print(f'\\r[{i:d}/{len(batch_features):d}] batch loss: {batch_loss} ',\n",
        "                        end='', flush=True)\n",
        "              loss = 0\n",
        "            \n",
        "\n",
        "      # Process is complete.\n",
        "      print('Training process has finished. Saving trained model.')\n",
        "      # Saving the model\n",
        "      save_path = f'/content/drive/MyDrive/Data/VAE{latent_size}_beta{beta}_batch{batch_size}_fold{fold}'\n",
        "      torch.save(net.state_dict(), save_path)\n",
        "\n",
        "      # Print about testing\n",
        "      #print('Starting testing')\n",
        "\n",
        "      # Evaluation for this fold\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Iterate over the test data and generate predictions\n",
        "        for i, (batch_features) in enumerate(testloader):\n",
        "\n",
        "            # Reshape data so each image is an array with 784 elements\n",
        "            batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "            #test_loss = net(batch_features)\n",
        "            criterion, output, latent = net(batch_features, beta)\n",
        "            \n",
        "            val_outputs.append(output.detach().numpy())\n",
        "            val_latents.append(latent.detach().numpy())\n",
        "            val_loss = loss_function(output, batch_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            test_loss += val_loss.item()\n",
        "\n",
        "\n",
        "        train_outputs = np.concatenate( train_outputs, axis=0 )\n",
        "        val_outputs = np.concatenate( val_outputs, axis=0 )\n",
        "        val_latents = np.concatenate( val_latents, axis=0 )\n",
        "        model_train_loss = training_loss / len(train_outputs\n",
        "                          )\n",
        "        test_loss = test_loss / len(val_outputs\n",
        "                          )\n",
        "        #animator.add(epoch, (loss, test_loss))\n",
        "        #print(train_iter[0][:5])\n",
        "        #print(outputs[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Print fold loss\n",
        "        print(f'Training loss for fold {fold}: {model_train_loss}')\n",
        "        print(f'Validation loss for fold {fold}: {test_loss}')\n",
        "        print('--------------------------------')\n",
        "        train_results[fold] = model_train_loss\n",
        "        val_results[fold] = test_loss\n",
        "    # Print fold results\n",
        "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "    print('--------------------------------')\n",
        "    train_sum = 0.0\n",
        "    val_sum = 0.0\n",
        "    for key, value in train_results.items():\n",
        "      print(f'Training Fold {key} Loss: {value}')\n",
        "      train_sum += value\n",
        "    print(f'Trainig Average Loss: {train_sum/len(train_results.items())}')\n",
        "    train_avg_loss = train_sum/len(train_results.items())\n",
        "    for key, value in val_results.items():\n",
        "      print(f'Validation Fold {key} Loss: {value} ')\n",
        "      val_sum += value\n",
        "    print(f'Vaidation Average Loss: {val_sum/len(val_results.items())}')\n",
        "    val_avg_loss = val_sum/len(val_results.items())\n",
        "\n",
        "    full_outputs = []\n",
        "    full_outputs.append(train_avg_loss)\n",
        "    full_outputs.append(val_avg_loss)\n",
        "    full_outputs.append(val_latents)\n",
        "    full_outputs.append(val_outputs)\n",
        "    full_outputs.append(train_outputs)\n",
        "\n",
        "    \n",
        "\n",
        "    return full_outputs\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "yli-xGhddLrw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "k = 5\n"
      ],
      "metadata": {
        "id": "PoGod731PqmG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_15b2 = train_VAE_model(net=net15b2,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 0.2,\n",
        "                                 latent_size = 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SS1ZlxqbnTEF",
        "outputId": "9b6cbeb8-76f1-498b-85f5-bfcbabafa9ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0003535336581990123 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003474996774457395 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.000325811211951077 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00029503897530958056 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00031730704358778894 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00033526262268424034 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0003164500812999904 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0002775369503069669 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0002704900689423084 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00029732106486335397 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0003010005166288465 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0003083365736529231 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00029096685466356575 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002745419042184949 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.0002919573453254998 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.0002738907060120255 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.000284273992292583 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0002696074661798775 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002829328295774758 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00028553008451126516 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00027639803010970354 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00027905034949071705 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00026471953606233 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00029965752037242055 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00026373949367552996 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00024356455833185464 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00026438391068950295 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00025498290779069066 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00021969471708871424 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00024649579427205026 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00026857591001316905 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0002697862219065428 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.0002620569139253348 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002353394083911553 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00026200353750027716 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.000260603817878291 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00025791878579184413 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00025962101062759757 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.0002717446768656373 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00024274599854834378 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00024941563606262207 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.0002595789555925876 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00021643542277161032 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002474610519129783 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.0002414480404695496 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00024876324459910393 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00021939590806141496 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002482488052919507 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.000228856602916494 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.0002331475989194587 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.0002449655154842207\n",
            "Validation loss for fold 0: 0.0002543350551871126\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.000423190591391176 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003710708988364786 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00032640062272548676 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0003031682572327554 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.000314887089189142 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00032798683969303966 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0002675683645065874 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00026801720377989113 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00030481547582894564 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0002899142273236066 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00029909724253229797 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00029501918470487 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00028363498859107494 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00026694993721321225 \n",
            " Epoch 15\n",
            "[100/128] batch loss: 0.0002780079375952482 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4be009f1cd37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                  \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                  latent_size = 15)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-3b50a2c7e50f>\u001b[0m in \u001b[0;36mtrain_VAE_model\u001b[0;34m(net, dataset, k, epochs, batch_size, beta, latent_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m               \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m               \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_25b2 = train_VAE_model(net=net25b2,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 0.2,\n",
        "                                 latent_size = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ0WU-4O_tWx",
        "outputId": "6dac968f-e34d-41e8-8844-d29ceb21dd73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00040188070852309465 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00036357383942231536 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003739433886948973 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00032325080246664584 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00032412828295491636 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0003024162142537534 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.000281431304756552 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.000306497240671888 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.000299842533422634 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0003074370324611664 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0002903557615354657 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0002835997147485614 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0002745051751844585 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002718432806432247 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00026247368077747524 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00027950332150794566 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0002566492184996605 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0002855782222468406 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002448014565743506 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00024681861395947635 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00022968427219893783 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0002371557056903839 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00024956068955361843 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.0002374133764533326 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00023260724265128374 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00025757899857126176 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00023460746160708368 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002524548617657274 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00024059077259153128 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002407876745564863 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00022027433442417532 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00022897303279023618 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00025940476916730404 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002296505990670994 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00022567927953787148 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00022487729438580573 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00024334802583325654 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00024412851780653 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00023085041902959347 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00024433128419332206 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00021478919370565563 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00024749041767790914 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00023548532044515014 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002262388588860631 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.0002279626642121002 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.000267954746959731 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00022049181279726326 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00024195638252422214 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002156595146516338 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00023207758204080164 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.00022413476081888648\n",
            "Validation loss for fold 0: 0.0002336214499721593\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00039561421726830304 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003501527535263449 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00037185451947152615 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00035676793777383864 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00034057293669320643 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0003166849783156067 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00029422633815556765 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0003185938694514334 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00033721988438628614 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.000271245080512017 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00028049704269506037 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0002722722420003265 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00029575586086139083 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002678440941963345 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00027158387820236385 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00026039002113975585 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0002569382486399263 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00027691738796420395 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002730561245698482 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00030779707594774663 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00026886622072197497 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0002359615609748289 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00025769471540115774 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00024205427325796336 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00023874551698099822 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00024490320356562734 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.0002264890499645844 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002327828697161749 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.0002470080798957497 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00023399350175168365 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0002472098567523062 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00023592694196850061 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00023798465554136783 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00022278523829299957 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00024063749879132956 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00025607572752051055 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.0002337852056371048 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0002521197311580181 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00022608935250900686 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00023183059238363057 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0002074970252579078 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00023591839999426156 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00022257269301917404 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002136091497959569 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00022092585277277976 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002188641083193943 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00021915422985330224 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0001983028487302363 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002159083669539541 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00022956568864174187 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 0.00021996186276660462\n",
            "Validation loss for fold 1: 0.00022849971315751466\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0003789158654399216 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003857768897432834 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003292637411504984 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0003620532515924424 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0003303473931737244 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00032406041282229125 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00029557637753896415 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00029628261108882725 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00024675618624314666 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00027197625604458153 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0002779212954919785 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00027885756571777165 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.000261570094153285 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002747420803643763 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.000249669625191018 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00025547572295181453 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00023579892877023667 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0002796732587739825 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002781160001177341 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.0002686879597604275 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0002635737182572484 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0002761902578640729 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0002580614236649126 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00025258903042413294 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00024662536452524364 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0002564701426308602 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00023827370023354888 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00023577350657433271 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00023996106756385416 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002592356177046895 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00024397156084887683 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0002343953965464607 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.0002320105559192598 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002497290843166411 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00022137604537419975 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.0002447788428980857 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.0002372270100750029 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.000244612485403195 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.0002464144490659237 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00022149736469145864 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00023741976474411786 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00025775059475563467 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.000239137327298522 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00024165470676962286 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.0002232362749055028 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00021493392705451697 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00022688695753458887 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002113155205734074 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00023229408543556929 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00021448588813655078 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 0.0002320062770116283\n",
            "Validation loss for fold 2: 0.00024327609806788948\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0003989029210060835 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003693382313940674 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003396707761567086 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00036526561598293483 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0003125607909169048 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0003035252448171377 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00031927795498631895 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00028159384964965284 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00026705829077400267 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0002697284799069166 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00030858683749102056 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00025023953639902174 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0002636244462337345 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002496040251571685 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.0002484378346707672 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.000267356721451506 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00023815134773030877 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00024896973627619445 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00025010257377289236 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00027137124561704695 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00024161710462067276 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.000268462928943336 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0002570085634943098 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00025063793873414397 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00023909634910523891 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.000245892268139869 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.0002856298815459013 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00023460436204914004 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00026540597900748253 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002397626085439697 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00021937409474048764 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00023667214554734528 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.0002409077133052051 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002469804894644767 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00020777534518856555 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00023266651260200888 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00021573662525042892 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00021969374211039394 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00023676277487538755 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00024289840075653046 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00020915124332532287 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.0002447161532472819 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00021337199723348022 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00022162347158882767 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00020615142420865595 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002163887256756425 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00019858457380905747 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002455438079778105 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002243847120553255 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00019839305605273694 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 0.00021955908909727709\n",
            "Validation loss for fold 3: 0.00022642156866330525\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=25, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=25, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00035789349931292236 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003772418131120503 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00034505801158957183 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00034042660263366997 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0003584872465580702 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0003403007867746055 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0002998234995175153 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0002864982816390693 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00029723005718551576 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0002736224851105362 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0002621451858431101 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.000268335483269766 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0002563822199590504 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0002793400199152529 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.0002976086107082665 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00028623780235648155 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0002554492966737598 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0002570095530245453 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002430428721709177 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00025355725665576756 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00022939029440749437 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00024926886544562876 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0002652280672919005 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00024231830320786685 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.000250088021857664 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0002553525846451521 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00026365937083028257 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002406334097031504 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.0002560447028372437 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002423474652459845 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00024177708837669343 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00022285511658992618 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00025179103249683976 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.000240201989072375 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.0002528522745706141 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.0002391795424045995 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00022961627109907568 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00023898390645626932 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.0002451644104439765 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00019276778039056808 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0002327261317986995 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.0002102450525853783 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00020658050198107958 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00019672497001010925 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00022531936701852828 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002298505714861676 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.0002385879633948207 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00023369700647890568 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002350598224438727 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00022165205155033618 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 0.00021801906420837987\n",
            "Validation loss for fold 4: 0.0002290403649227501\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 0.00022413476081888648\n",
            "Training Fold 1 Loss: 0.00021996186276660462\n",
            "Training Fold 2 Loss: 0.0002320062770116283\n",
            "Training Fold 3 Loss: 0.00021955908909727709\n",
            "Training Fold 4 Loss: 0.00021801906420837987\n",
            "Trainig Average Loss: 0.00022273621078055527\n",
            "Validation Fold 0 Loss: 0.0002336214499721593 \n",
            "Validation Fold 1 Loss: 0.00022849971315751466 \n",
            "Validation Fold 2 Loss: 0.00024327609806788948 \n",
            "Validation Fold 3 Loss: 0.00022642156866330525 \n",
            "Validation Fold 4 Loss: 0.0002290403649227501 \n",
            "Vaidation Average Loss: 0.00023217183895672374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_15b1 = train_VAE_model(net=net15b2,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 1,\n",
        "                                 latent_size = 15)"
      ],
      "metadata": {
        "id": "u5VXnS-BMNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_15b5 = train_VAE_model(net=net15b2,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 0.5,\n",
        "                                 latent_size = 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN8mvxGsQiLI",
        "outputId": "54bf54c8-04fd-4f25-b291-0dec3227815c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00038746255449950695 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003305884310975671 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00035569979809224606 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00036745480610989034 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0003655980108305812 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0003434907703194767 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00031350809149444103 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00032884455868043005 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0003286882711108774 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0003361905983183533 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0003203771193511784 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0003180267522111535 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00029549101600423455 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.000305375549942255 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00031174588366411626 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00031074308208189905 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00030052437796257436 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0003025055630132556 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.000303375709336251 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.0003243640821892768 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0003148483519908041 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0003240893711335957 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00028654703055508435 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00031634524930268526 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.0002906278532464057 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00027864775620400906 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.0002959727426059544 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002819353831000626 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.000276703794952482 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002664306084625423 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00027624264475889504 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0003111676487606019 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00028967010439373553 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002622022293508053 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00025148512213490903 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.0002805108670145273 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.0002883901179302484 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00027317332569509745 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00027135160053148866 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.0002957918622996658 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0002918516693171114 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00027505127945914865 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00029716320568695664 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002898432721849531 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00028068770188838243 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002889283059630543 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00025659528910182416 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002862746478058398 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0003054493572562933 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00030727224657312036 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.00027816384363358546\n",
            "Validation loss for fold 0: 0.0002867414358076355\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00034859514562413096 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003552766575012356 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003463302564341575 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0003599478804972023 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00035313377156853676 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00032141993870027363 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0003138569591101259 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0003214226453565061 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00030917549156583846 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0003188675327692181 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00029541656840592623 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00032250810181722045 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00032479214132763445 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0003092887927778065 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00028398516587913036 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00031267054146155715 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0002999507123604417 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0002938626566901803 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002938013640232384 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.000277840270427987 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00030684942612424493 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0002972070942632854 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00029662964516319335 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00029418026679195464 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00030157616129145026 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00027950096409767866 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.000282695225905627 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00028977825422771275 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.0003218303609173745 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00029014828032813966 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0002850228047464043 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00030971653177402914 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00029439281206578016 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002711440611165017 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00027290821890346706 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00027991514070890844 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.0002876835351344198 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0002686187799554318 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00027862138813361526 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00026309830718673766 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0002544965536799282 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00027570800739340484 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00030063404119573534 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002812756283674389 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00028617182397283614 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002485202858224511 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.0002864640555344522 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00026726003852672875 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002963976003229618 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00028924146317876875 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 0.00027867834672734543\n",
            "Validation loss for fold 1: 0.0002848192665579586\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00038611353375017643 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.000370891357306391 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00035908169229514897 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0003492760588414967 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00033461174461990595 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00032263394678011537 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0003422480367589742 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0002974270028062165 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0003310835745651275 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00032706864294596016 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00032236057450063527 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00029900053050369024 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0002851223689503968 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00029007229022681713 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00029441542574204504 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.0002825492701958865 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00027946371119469404 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.000308723421767354 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0002723689249251038 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.0002830582670867443 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0002720417396631092 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0003092741244472563 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0003159284242428839 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00027408002642914653 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00028029983513988554 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0002906180452555418 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.0002843023685272783 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002975359093397856 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.0003117590968031436 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002921238192357123 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0002888073504436761 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.000285311572952196 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00026627694023773074 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002795942418742925 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00028407463105395436 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.0002773758315015584 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00029158368124626577 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0002594889956526458 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.0002787345729302615 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00027161891921423376 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00028642528923228383 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.000295530742732808 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.0002667858498170972 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0002777315676212311 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.0003007779305335134 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00024447747273370624 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.0002703151840250939 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002599665313027799 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002719287294894457 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00029672670643776655 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 0.000275015491546351\n",
            "Validation loss for fold 2: 0.000282540330573574\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0003841821162495762 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00038462909287773073 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.000417951523559168 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00038253713864833117 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00035017181653529406 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00036675191950052977 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0003302636614535004 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00031429342925548553 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0003269207081757486 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.0003455346741247922 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00030280344071798027 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0002892679476644844 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0002910250041168183 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00027975067496299744 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.0003075989370699972 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00029165431624278426 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0003026010235771537 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00031441947794519365 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00029799589538015425 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.000311536539811641 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0002695358416531235 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0002730047272052616 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.000297843711450696 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00031169693102128804 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00029930868186056614 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0002654551644809544 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00027454912196844816 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0002841687819454819 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00029468588763847947 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.0002999169228132814 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0002781585790216923 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00029348343377932906 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.000291608739644289 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00027351529570296407 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.0002975806128233671 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00029276226996444166 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00027441896963864565 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00027650920674204826 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.0002826825075317174 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00027138166478835046 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.000285744114080444 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00027694448363035917 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.0002837952342815697 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00027603725902736187 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00028680957620963454 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0002916124649345875 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.0002426222199574113 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0002753211883828044 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00027121082530356944 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.0002912765194196254 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 0.0002776858613297746\n",
            "Validation loss for fold 3: 0.0002837337091867058\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00039764176472090185 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003652735031209886 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00036961885052733123 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0003560430195648223 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00032665382605046034 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00034119634074158967 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0003315037174616009 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.0003089593956246972 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00032084574922919273 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00028170301811769605 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00034469139063730836 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00030883518047630787 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.000318326783599332 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00031420018058270216 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00030492644873447716 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.0002944542793557048 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0003119453613180667 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00029416766483336687 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00030982165480963886 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00028287930763326585 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0003176146710757166 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00031129547278396785 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0003115657018497586 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.0002874423807952553 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.0002801551017910242 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00031612199381925166 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.0002884984714910388 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00030839641112834215 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00029306404758244753 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00029856449691578746 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0002930257760453969 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0002779834030661732 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00028985418612137437 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0002728063554968685 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.0002774719032458961 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00027569220401346684 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00027701962972059846 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0002908558235503733 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00026130370679311454 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.0002788680430967361 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0002862400433514267 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.0002913548960350454 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.0002992649970110506 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00028214172925800085 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00028401563758961856 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00026807180256582797 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00026471386081539094 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0003075219865422696 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0002825888223014772 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.0002631768584251404 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 0.00027742980161742987\n",
            "Validation loss for fold 4: 0.0002835532177382813\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 0.00027816384363358546\n",
            "Training Fold 1 Loss: 0.00027867834672734543\n",
            "Training Fold 2 Loss: 0.000275015491546351\n",
            "Training Fold 3 Loss: 0.0002776858613297746\n",
            "Training Fold 4 Loss: 0.00027742980161742987\n",
            "Trainig Average Loss: 0.00027739466897089727\n",
            "Validation Fold 0 Loss: 0.0002867414358076355 \n",
            "Validation Fold 1 Loss: 0.0002848192665579586 \n",
            "Validation Fold 2 Loss: 0.000282540330573574 \n",
            "Validation Fold 3 Loss: 0.0002837337091867058 \n",
            "Validation Fold 4 Loss: 0.0002835532177382813 \n",
            "Vaidation Average Loss: 0.00028427759197283104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_50_1 = train_VAE_model(net=net50b02,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 0.02,\n",
        "                                 latent_size = 50)"
      ],
      "metadata": {
        "id": "yu6v521nEgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_50_1 = train_VAE_model(net=net50b02,\n",
        "                                 dataset=dataset,\n",
        "                                 k=k, \n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 beta = 0.5,\n",
        "                                 latent_size = 50)"
      ],
      "metadata": {
        "id": "aBVGinvwRbrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
