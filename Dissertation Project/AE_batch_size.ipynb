{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87bvzlVH8tL6",
        "outputId": "2acea466-0aa6-4c05-b21a-6db72dee266f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Setting up google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaA4R5mm8bK1"
      },
      "outputs": [],
      "source": [
        "import models as models\n",
        "import five_fold_training\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from IPython import display\n",
        "import math\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA6gl791C1Uo"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Data/MM_train_data'\n",
        "train_data = torch.load(train_path)\n",
        "test_path = '/content/drive/MyDrive/Data/MM_test_data'\n",
        "test_data = torch.load(test_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOPgmgenDDNE"
      },
      "outputs": [],
      "source": [
        "dataset = ConcatDataset([test_data, train_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqlDltOcEigI"
      },
      "outputs": [],
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GVW_j17APTW"
      },
      "outputs": [],
      "source": [
        "def train_AE_model(net, dataset, k, epochs, batch_size):\n",
        "  #https://github.com/christianversloot/machine-learning-articles/blob/main/\n",
        "  #how-to-use-k-fold-cross-validation-with-pytorch.md\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    \n",
        "    # Configuration options\n",
        "    k_folds = k\n",
        "    epochs = epochs\n",
        "    #loss_function = nn.CrossEntropyLoss()\n",
        "    loss_function = nn.MSELoss()\n",
        "    \n",
        "    # For fold results\n",
        "    train_results = {}\n",
        "    val_results = {}\n",
        "    \n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "\n",
        "    dataset = dataset\n",
        "    \n",
        "    # Define the K-fold Cross Validator\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "      \n",
        "    # Start print\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # K-fold Cross Validation model evaluation\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "      \n",
        "      # Print\n",
        "      print(f'FOLD {fold}')\n",
        "      print('--------------------------------')\n",
        "      \n",
        "      # Sample elements randomly from a given list of ids, no replacement.\n",
        "      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "      \n",
        "      # Define data loaders for training and testing data in this fold\n",
        "      trainloader = torch.utils.data.DataLoader(\n",
        "                        dataset, \n",
        "                        batch_size=batch_size, sampler=train_subsampler)\n",
        "      testloader = torch.utils.data.DataLoader(\n",
        "                        dataset,\n",
        "                        batch_size=batch_size, sampler=test_subsampler)\n",
        "      \n",
        "\n",
        "      net = net\n",
        "      net.apply(reset_weights)\n",
        "\n",
        "      # create an optimizer object\n",
        "      # Adam optimizer with learning rate 1e-3\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "      # Run the training loop for defined number of epochs\n",
        "      for epoch in range(1, epochs + 1):\n",
        "          print(f'\\n Epoch {epoch}')\n",
        "          loss = 0\n",
        "          training_loss = 0\n",
        "          test_loss = 0\n",
        "          val_latents = []\n",
        "          val_outputs = []\n",
        "          train_outputs = []\n",
        "          for i, (batch_features) in enumerate(trainloader):\n",
        "              optimizer.zero_grad()\n",
        "              # Reshape data so each image is an array with 784 elements\n",
        "              batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "              output, latent = net(batch_features)\n",
        "              criterion = loss_function(output, batch_features)\n",
        "              criterion.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              #avg_loss += loss.item()/len(train_data)\n",
        "\n",
        "              #z, mu_e, logvar_e = net.encode(batch_features)\n",
        "              #output = net.decode(z)\n",
        "              \n",
        "              train_outputs.append(output.detach().numpy())\n",
        "              \n",
        "              train_loss = loss_function(output, batch_features)\n",
        "              \n",
        "              # Print statistics\n",
        "              loss += train_loss.item()\n",
        "              training_loss += train_loss.item()\n",
        "              \n",
        "              \n",
        "\n",
        "              if i % 100 == 0:\n",
        "                  # Print average loss per sample in batch\n",
        "                  batch_loss = loss/len(batch_features)\n",
        "                  print(f'\\r[{i:d}/{len(batch_features):d}] batch loss: {batch_loss} ',\n",
        "                        end='', flush=True)\n",
        "              loss = 0\n",
        "            \n",
        "\n",
        "      # Process is complete.\n",
        "      print('Training process has finished. Saving trained model.')\n",
        "\n",
        "      # Print about testing\n",
        "      #print('Starting testing')\n",
        "\n",
        "      # Evaluation for this fold\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Iterate over the test data and generate predictions\n",
        "        for i, (batch_features) in enumerate(testloader):\n",
        "\n",
        "            # Reshape data so each image is an array with 784 elements\n",
        "            batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "            #test_loss = net(batch_features)\n",
        "            output, latent = net(batch_features)\n",
        "            \n",
        "            val_outputs.append(output.detach().numpy())\n",
        "            val_latents.append(latent.detach().numpy())\n",
        "            val_loss = loss_function(output, batch_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            test_loss += val_loss.item()\n",
        "\n",
        "\n",
        "        train_outputs = np.concatenate( train_outputs, axis=0 )\n",
        "        val_outputs = np.concatenate( val_outputs, axis=0 )\n",
        "        val_latents = np.concatenate( val_latents, axis=0 )\n",
        "        model_train_loss = training_loss / len(train_outputs\n",
        "                          )\n",
        "        test_loss = test_loss / len(val_outputs\n",
        "                          )\n",
        "        #animator.add(epoch, (loss, test_loss))\n",
        "        #print(train_iter[0][:5])\n",
        "        #print(outputs[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Print fold loss\n",
        "        print(f'Training loss for fold {fold}: {model_train_loss}')\n",
        "        print(f'Validation loss for fold {fold}: {test_loss}')\n",
        "        print('--------------------------------')\n",
        "        train_results[fold] = model_train_loss\n",
        "        val_results[fold] = test_loss\n",
        "    # Print fold results\n",
        "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "    print('--------------------------------')\n",
        "    train_sum = 0.0\n",
        "    val_sum = 0.0\n",
        "    for key, value in train_results.items():\n",
        "      print(f'Training Fold {key} Loss: {value}')\n",
        "      train_sum += value\n",
        "    print(f'Trainig Average Loss: {train_sum/len(train_results.items())}')\n",
        "    train_avg_loss = train_sum/len(train_results.items())\n",
        "    for key, value in val_results.items():\n",
        "      print(f'Validation Fold {key} Loss: {value} ')\n",
        "      val_sum += value\n",
        "    print(f'Vaidation Average Loss: {val_sum/len(val_results.items())}')\n",
        "    val_avg_loss = val_sum/len(val_results.items())\n",
        "\n",
        " \n",
        "    full_outputs = []\n",
        "    full_outputs.append(train_avg_loss)\n",
        "    full_outputs.append(val_avg_loss)\n",
        "    full_outputs.append(val_latents)\n",
        "    full_outputs.append(val_outputs)\n",
        "    full_outputs.append(train_outputs)\n",
        "\n",
        "    \n",
        "\n",
        "    return full_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoGod731PqmG"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "k = 5\n",
        "batch_sizes = [64, 128, 512, 1024, 4096, 8192]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsElv7QkAw1b"
      },
      "outputs": [],
      "source": [
        "AEnet15 = models.AE2(input_size=204, \n",
        "      hidden_size1=196, \n",
        "      hidden_size2=128, \n",
        "      latent_size=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1c8u8nQCH2v",
        "outputId": "703ffcbc-e7d7-49e4-c732-6bbce69ea074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[500/64] batch loss: 0.0006473666289821267 \n",
            " Epoch 2\n",
            "[500/64] batch loss: 0.0005562445148825645 \n",
            " Epoch 3\n",
            "[500/64] batch loss: 0.0005015531205572188 \n",
            " Epoch 4\n",
            "[500/64] batch loss: 0.0005075919325463474 \n",
            " Epoch 5\n",
            "[500/64] batch loss: 0.0004133160400670022 \n",
            " Epoch 6\n",
            "[500/64] batch loss: 0.000454517372418195 \n",
            " Epoch 7\n",
            "[500/64] batch loss: 0.000448388367658481 \n",
            " Epoch 8\n",
            "[500/64] batch loss: 0.0005197958671487868 \n",
            " Epoch 9\n",
            "[500/64] batch loss: 0.00037530227564275265 \n",
            " Epoch 10\n",
            "[500/64] batch loss: 0.0003943778283428401 \n",
            " Epoch 11\n",
            "[500/64] batch loss: 0.00041167219751514494 \n",
            " Epoch 12\n",
            "[500/64] batch loss: 0.0004251000063959509 \n",
            " Epoch 13\n",
            "[500/64] batch loss: 0.0004653721407521516 \n",
            " Epoch 14\n",
            "[500/64] batch loss: 0.0003806847089435905 \n",
            " Epoch 15\n",
            "[500/64] batch loss: 0.0003901165910065174 \n",
            " Epoch 16\n",
            "[500/64] batch loss: 0.0003798626712523401 \n",
            " Epoch 17\n",
            "[500/64] batch loss: 0.00038137807860039175 \n",
            " Epoch 18\n",
            "[500/64] batch loss: 0.00032965929131023586 \n",
            " Epoch 19\n",
            "[500/64] batch loss: 0.000389126013033092 \n",
            " Epoch 20\n",
            "[500/64] batch loss: 0.0003517005534376949 \n",
            " Epoch 21\n",
            "[500/64] batch loss: 0.00032431501313112676 \n",
            " Epoch 22\n",
            "[500/64] batch loss: 0.00035176382516510785 \n",
            " Epoch 23\n",
            "[500/64] batch loss: 0.0003423637244850397 \n",
            " Epoch 24\n",
            "[500/64] batch loss: 0.0003163352084811777 \n",
            " Epoch 25\n",
            "[500/64] batch loss: 0.00030967616476118565 \n",
            " Epoch 26\n",
            "[500/64] batch loss: 0.0002745804958976805 \n",
            " Epoch 27\n",
            "[500/64] batch loss: 0.00040332842036150396 \n",
            " Epoch 28\n",
            "[500/64] batch loss: 0.00035168876638635993 \n",
            " Epoch 29\n",
            "[500/64] batch loss: 0.00037957969470880926 \n",
            " Epoch 30\n",
            "[500/64] batch loss: 0.0003332726191729307 \n",
            " Epoch 31\n",
            "[500/64] batch loss: 0.0003488739603199065 \n",
            " Epoch 32\n",
            "[500/64] batch loss: 0.00032671811641193926 \n",
            " Epoch 33\n",
            "[500/64] batch loss: 0.000302494183415547 \n",
            " Epoch 34\n",
            "[500/64] batch loss: 0.0002906271838583052 \n",
            " Epoch 35\n",
            "[500/64] batch loss: 0.0003539945464581251 \n",
            " Epoch 36\n",
            "[500/64] batch loss: 0.0003069404629059136 \n",
            " Epoch 37\n",
            "[500/64] batch loss: 0.0003140101907774806 \n",
            " Epoch 38\n",
            "[500/64] batch loss: 0.0002823812828864902 \n",
            " Epoch 39\n",
            "[500/64] batch loss: 0.00031610095174983144 \n",
            " Epoch 40\n",
            "[500/64] batch loss: 0.00037115468876436353 \n",
            " Epoch 41\n",
            "[500/64] batch loss: 0.00028798883431591094 \n",
            " Epoch 42\n",
            "[500/64] batch loss: 0.0003210585273336619 \n",
            " Epoch 43\n",
            "[500/64] batch loss: 0.00029953065677545965 \n",
            " Epoch 44\n",
            "[500/64] batch loss: 0.00030065211467444897 \n",
            " Epoch 45\n",
            "[500/64] batch loss: 0.0003460599691607058 \n",
            " Epoch 46\n",
            "[500/64] batch loss: 0.00027889784541912377 \n",
            " Epoch 47\n",
            "[500/64] batch loss: 0.00025705414009280503 \n",
            " Epoch 48\n",
            "[500/64] batch loss: 0.0002917585370596498 \n",
            " Epoch 49\n",
            "[500/64] batch loss: 0.0002685531508177519 \n",
            " Epoch 50\n",
            "[500/64] batch loss: 0.00035207843757234514 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.00028598587019333047\n",
            "Validation loss for fold 0: 0.0003187963752845113\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[500/64] batch loss: 0.0007101951632648706 \n",
            " Epoch 2\n",
            "[500/64] batch loss: 0.0005304267979227006 \n",
            " Epoch 3\n",
            "[500/64] batch loss: 0.00048610911471769214 \n",
            " Epoch 4\n",
            "[500/64] batch loss: 0.00045420642709359527 \n",
            " Epoch 5\n",
            "[500/64] batch loss: 0.00036892524803988636 \n",
            " Epoch 6\n",
            "[500/64] batch loss: 0.00046527161612175405 \n",
            " Epoch 7\n",
            "[500/64] batch loss: 0.00042182279867120087 \n",
            " Epoch 8\n",
            "[500/64] batch loss: 0.00041280928417108953 \n",
            " Epoch 9\n",
            "[500/64] batch loss: 0.00039438498788513243 \n",
            " Epoch 10\n",
            "[500/64] batch loss: 0.0003371106868144125 \n",
            " Epoch 11\n",
            "[500/64] batch loss: 0.00037744035944342613 \n",
            " Epoch 12\n",
            "[500/64] batch loss: 0.0003750697651412338 \n",
            " Epoch 13\n",
            "[500/64] batch loss: 0.00033334444742649794 \n",
            " Epoch 14\n",
            "[500/64] batch loss: 0.0003508508380036801 \n",
            " Epoch 15\n",
            "[500/64] batch loss: 0.00039712004945613444 \n",
            " Epoch 16\n",
            "[500/64] batch loss: 0.00032669297070242465 \n",
            " Epoch 17\n",
            "[500/64] batch loss: 0.0003720786771737039 \n",
            " Epoch 18\n",
            "[500/64] batch loss: 0.00040523387724533677 \n",
            " Epoch 19\n",
            "[500/64] batch loss: 0.0003315476351417601 \n",
            " Epoch 20\n",
            "[500/64] batch loss: 0.0004117848293390125 \n",
            " Epoch 21\n",
            "[500/64] batch loss: 0.000318237638566643 \n",
            " Epoch 22\n",
            "[500/64] batch loss: 0.00032943772384896874 \n",
            " Epoch 23\n",
            "[500/64] batch loss: 0.0003484537883196026 \n",
            " Epoch 24\n",
            "[500/64] batch loss: 0.00034556485479697585 \n",
            " Epoch 25\n",
            "[500/64] batch loss: 0.00037614465691149235 \n",
            " Epoch 26\n",
            "[500/64] batch loss: 0.0003138153988402337 \n",
            " Epoch 27\n",
            "[500/64] batch loss: 0.00032414757879450917 \n",
            " Epoch 28\n",
            "[500/64] batch loss: 0.00026257417630404234 \n",
            " Epoch 29\n",
            "[500/64] batch loss: 0.00029563496354967356 \n",
            " Epoch 30\n",
            "[500/64] batch loss: 0.00031068120733834803 \n",
            " Epoch 31\n",
            "[500/64] batch loss: 0.00032675941474735737 \n",
            " Epoch 32\n",
            "[500/64] batch loss: 0.00029599384288303554 \n",
            " Epoch 33\n",
            "[500/64] batch loss: 0.00030025295563973486 \n",
            " Epoch 34\n",
            "[500/64] batch loss: 0.0002620666055008769 \n",
            " Epoch 35\n",
            "[500/64] batch loss: 0.0003067523066420108 \n",
            " Epoch 36\n",
            "[500/64] batch loss: 0.00032909525907598436 \n",
            " Epoch 37\n",
            "[500/64] batch loss: 0.00024889735504984856 \n",
            " Epoch 38\n",
            "[500/64] batch loss: 0.00029554733191616833 \n",
            " Epoch 39\n",
            "[500/64] batch loss: 0.0002639639424160123 \n",
            " Epoch 40\n",
            "[500/64] batch loss: 0.00027476297691464424 \n",
            " Epoch 41\n",
            "[500/64] batch loss: 0.00031453618430532515 \n",
            " Epoch 42\n",
            "[500/64] batch loss: 0.00024338798539247364 \n",
            " Epoch 43\n",
            "[500/64] batch loss: 0.0002700859331525862 \n",
            " Epoch 44\n",
            "[500/64] batch loss: 0.0003102753253187984 \n",
            " Epoch 45\n",
            "[500/64] batch loss: 0.00028180956724099815 \n",
            " Epoch 46\n",
            "[500/64] batch loss: 0.0002472918713465333 \n",
            " Epoch 47\n",
            "[500/64] batch loss: 0.00029089683084748685 \n",
            " Epoch 48\n",
            "[500/64] batch loss: 0.00030760440859012306 \n",
            " Epoch 49\n",
            "[500/64] batch loss: 0.000277089886367321 \n",
            " Epoch 50\n",
            "[500/64] batch loss: 0.000289816438453272 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 0.00027589076239630065\n",
            "Validation loss for fold 1: 0.00031115759250622545\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[500/64] batch loss: 0.0006715572089888155 \n",
            " Epoch 2\n",
            "[500/64] batch loss: 0.0005126051255501807 \n",
            " Epoch 3\n",
            "[500/64] batch loss: 0.0005023123812861741 \n",
            " Epoch 4\n",
            "[500/64] batch loss: 0.0005339149502106011 \n",
            " Epoch 5\n",
            "[500/64] batch loss: 0.000482161995023489 \n",
            " Epoch 6\n",
            "[500/64] batch loss: 0.0004288571944925934 \n",
            " Epoch 7\n",
            "[500/64] batch loss: 0.00038512091850861907 \n",
            " Epoch 8\n",
            "[500/64] batch loss: 0.0004139855445828289 \n",
            " Epoch 9\n",
            "[500/64] batch loss: 0.00041125729330815375 \n",
            " Epoch 10\n",
            "[500/64] batch loss: 0.0004238669644109905 \n",
            " Epoch 11\n",
            "[500/64] batch loss: 0.00033708594855852425 \n",
            " Epoch 12\n",
            "[500/64] batch loss: 0.0003606030950322747 \n",
            " Epoch 13\n",
            "[500/64] batch loss: 0.0003833072551060468 \n",
            " Epoch 14\n",
            "[500/64] batch loss: 0.00034389240317977965 \n",
            " Epoch 15\n",
            "[500/64] batch loss: 0.00038460182258859277 \n",
            " Epoch 16\n",
            "[500/64] batch loss: 0.0003428532218094915 \n",
            " Epoch 17\n",
            "[500/64] batch loss: 0.0002864787238650024 \n",
            " Epoch 18\n",
            "[500/64] batch loss: 0.00034797191619873047 \n",
            " Epoch 19\n",
            "[500/64] batch loss: 0.0003223264357075095 \n",
            " Epoch 20\n",
            "[500/64] batch loss: 0.0003660731017589569 \n",
            " Epoch 21\n",
            "[500/64] batch loss: 0.0002856758364941925 \n",
            " Epoch 22\n",
            "[500/64] batch loss: 0.00035859234048984945 \n",
            " Epoch 23\n",
            "[500/64] batch loss: 0.00028469780227169394 \n",
            " Epoch 24\n",
            "[500/64] batch loss: 0.0003214881580788642 \n",
            " Epoch 25\n",
            "[500/64] batch loss: 0.0002931062481366098 \n",
            " Epoch 26\n",
            "[500/64] batch loss: 0.00030137793510220945 \n",
            " Epoch 27\n",
            "[500/64] batch loss: 0.0003085418720729649 \n",
            " Epoch 28\n",
            "[500/64] batch loss: 0.0003061428142245859 \n",
            " Epoch 29\n",
            "[500/64] batch loss: 0.0002784755197353661 \n",
            " Epoch 30\n",
            "[500/64] batch loss: 0.00030031963251531124 \n",
            " Epoch 31\n",
            "[500/64] batch loss: 0.000356770382495597 \n",
            " Epoch 32\n",
            "[500/64] batch loss: 0.0002482145500835031 \n",
            " Epoch 33\n",
            "[500/64] batch loss: 0.00029611497302539647 \n",
            " Epoch 34\n",
            "[500/64] batch loss: 0.0002328384289285168 \n",
            " Epoch 35\n",
            "[500/64] batch loss: 0.00025650140014477074 \n",
            " Epoch 36\n",
            "[500/64] batch loss: 0.0002701079356484115 \n",
            " Epoch 37\n",
            "[500/64] batch loss: 0.00026323189376853406 \n",
            " Epoch 38\n",
            "[500/64] batch loss: 0.00029976468067616224 \n",
            " Epoch 39\n",
            "[500/64] batch loss: 0.00028752541402354836 \n",
            " Epoch 40\n",
            "[500/64] batch loss: 0.0003339419199619442 \n",
            " Epoch 41\n",
            "[500/64] batch loss: 0.0002636839635670185 \n",
            " Epoch 42\n",
            "[500/64] batch loss: 0.0002709973487071693 \n",
            " Epoch 43\n",
            "[500/64] batch loss: 0.0002468320308253169 \n",
            " Epoch 44\n",
            "[500/64] batch loss: 0.00027150477399118245 \n",
            " Epoch 45\n",
            "[500/64] batch loss: 0.00023944924760144204 \n",
            " Epoch 46\n",
            "[500/64] batch loss: 0.00030493453959934413 \n",
            " Epoch 47\n",
            "[500/64] batch loss: 0.00029265828197821975 \n",
            " Epoch 48\n",
            "[500/64] batch loss: 0.0002595144906081259 \n",
            " Epoch 49\n",
            "[500/64] batch loss: 0.000277464569080621 \n",
            " Epoch 50\n",
            "[500/64] batch loss: 0.0002798190980684012 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 0.00027264169502902674\n",
            "Validation loss for fold 2: 0.0003037763578154023\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[500/64] batch loss: 0.0005600717850029469 \n",
            " Epoch 2\n",
            "[500/64] batch loss: 0.000559382897336036 \n",
            " Epoch 3\n",
            "[500/64] batch loss: 0.0005189045914448798 \n",
            " Epoch 4\n",
            "[500/64] batch loss: 0.00042877986561506987 \n",
            " Epoch 5\n",
            "[500/64] batch loss: 0.0005309798871167004 \n",
            " Epoch 6\n",
            "[500/64] batch loss: 0.00044626020826399326 \n",
            " Epoch 7\n",
            "[500/64] batch loss: 0.0003957196313422173 \n",
            " Epoch 8\n",
            "[500/64] batch loss: 0.00042786129051819444 \n",
            " Epoch 9\n",
            "[500/64] batch loss: 0.00038412914727814496 \n",
            " Epoch 10\n",
            "[500/64] batch loss: 0.0003756754449568689 \n",
            " Epoch 11\n",
            "[500/64] batch loss: 0.0003794592630583793 \n",
            " Epoch 12\n",
            "[500/64] batch loss: 0.00037940844777040184 \n",
            " Epoch 13\n",
            "[500/64] batch loss: 0.0004012858553323895 \n",
            " Epoch 14\n",
            "[500/64] batch loss: 0.0003518389130476862 \n",
            " Epoch 15\n",
            "[500/64] batch loss: 0.00035130465403199196 \n",
            " Epoch 16\n",
            "[500/64] batch loss: 0.0003913658729288727 \n",
            " Epoch 17\n",
            "[500/64] batch loss: 0.00032337859738618135 \n",
            " Epoch 18\n",
            "[500/64] batch loss: 0.0003518667072057724 \n",
            " Epoch 19\n",
            "[500/64] batch loss: 0.00030478043481707573 \n",
            " Epoch 20\n",
            "[500/64] batch loss: 0.00032310019014403224 \n",
            " Epoch 21\n",
            "[500/64] batch loss: 0.0003218437486793846 \n",
            " Epoch 22\n",
            "[500/64] batch loss: 0.00030847234302200377 \n",
            " Epoch 23\n",
            "[500/64] batch loss: 0.0003255947958678007 \n",
            " Epoch 24\n",
            "[500/64] batch loss: 0.00031039491295814514 \n",
            " Epoch 25\n",
            "[500/64] batch loss: 0.00034995749592781067 \n",
            " Epoch 26\n",
            "[500/64] batch loss: 0.000323232525261119 \n",
            " Epoch 27\n",
            "[500/64] batch loss: 0.0002655520220287144 \n",
            " Epoch 28\n",
            "[500/64] batch loss: 0.0003299889212939888 \n",
            " Epoch 29\n",
            "[500/64] batch loss: 0.00031042061164043844 \n",
            " Epoch 30\n",
            "[500/64] batch loss: 0.0002199420559918508 \n",
            " Epoch 31\n",
            "[500/64] batch loss: 0.0003427930932957679 \n",
            " Epoch 32\n",
            "[500/64] batch loss: 0.00025044320500455797 \n",
            " Epoch 33\n",
            "[500/64] batch loss: 0.0002965738531202078 \n",
            " Epoch 34\n",
            "[500/64] batch loss: 0.0002953394432552159 \n",
            " Epoch 35\n",
            "[500/64] batch loss: 0.00030390219762921333 \n",
            " Epoch 36\n",
            "[500/64] batch loss: 0.0002968900662381202 \n",
            " Epoch 37\n",
            "[500/64] batch loss: 0.0002977096592076123 \n",
            " Epoch 38\n",
            "[500/64] batch loss: 0.00025151242152787745 \n",
            " Epoch 39\n",
            "[500/64] batch loss: 0.0002935300872195512 \n",
            " Epoch 40\n",
            "[500/64] batch loss: 0.0002558730193413794 \n",
            " Epoch 41\n",
            "[500/64] batch loss: 0.00029083777917549014 \n",
            " Epoch 42\n",
            "[500/64] batch loss: 0.00033085065660998225 \n",
            " Epoch 43\n",
            "[500/64] batch loss: 0.00026606113533489406 \n",
            " Epoch 44\n",
            "[500/64] batch loss: 0.0003077403816860169 \n",
            " Epoch 45\n",
            "[500/64] batch loss: 0.0002677735756151378 \n",
            " Epoch 46\n",
            "[500/64] batch loss: 0.00028655544156208634 \n",
            " Epoch 47\n",
            "[500/64] batch loss: 0.00029663278837688267 \n",
            " Epoch 48\n",
            "[500/64] batch loss: 0.00026934771449305117 \n",
            " Epoch 49\n",
            "[500/64] batch loss: 0.00022700226691085845 \n",
            " Epoch 50\n",
            "[500/64] batch loss: 0.0002483349817339331 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 0.0002726816405545791\n",
            "Validation loss for fold 3: 0.00030150997144298285\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[500/64] batch loss: 0.0005640090093947947 \n",
            " Epoch 2\n",
            "[500/64] batch loss: 0.0005628473591059446 \n",
            " Epoch 3\n",
            "[500/64] batch loss: 0.0005010616732761264 \n",
            " Epoch 4\n",
            "[500/64] batch loss: 0.0004380657337605953 \n",
            " Epoch 5\n",
            "[500/64] batch loss: 0.0004921152722090483 \n",
            " Epoch 6\n",
            "[500/64] batch loss: 0.0004472475848160684 \n",
            " Epoch 7\n",
            "[500/64] batch loss: 0.00037254751077853143 \n",
            " Epoch 8\n",
            "[500/64] batch loss: 0.0004092770104762167 \n",
            " Epoch 9\n",
            "[500/64] batch loss: 0.00035687361378222704 \n",
            " Epoch 10\n",
            "[500/64] batch loss: 0.0003273207985330373 \n",
            " Epoch 11\n",
            "[500/64] batch loss: 0.0003378701221663505 \n",
            " Epoch 12\n",
            "[500/64] batch loss: 0.00036312738666310906 \n",
            " Epoch 13\n",
            "[500/64] batch loss: 0.00033709683339111507 \n",
            " Epoch 14\n",
            "[500/64] batch loss: 0.00036726993857882917 \n",
            " Epoch 15\n",
            "[500/64] batch loss: 0.0003075537970289588 \n",
            " Epoch 16\n",
            "[500/64] batch loss: 0.00034791522193700075 \n",
            " Epoch 17\n",
            "[500/64] batch loss: 0.0003391353820916265 \n",
            " Epoch 18\n",
            "[500/64] batch loss: 0.0003541767073329538 \n",
            " Epoch 19\n",
            "[500/64] batch loss: 0.00029508117586374283 \n",
            " Epoch 20\n",
            "[500/64] batch loss: 0.0002884201821871102 \n",
            " Epoch 21\n",
            "[500/64] batch loss: 0.00035222043516114354 \n",
            " Epoch 22\n",
            "[500/64] batch loss: 0.0003177301841787994 \n",
            " Epoch 23\n",
            "[500/64] batch loss: 0.00034740858245640993 \n",
            " Epoch 24\n",
            "[500/64] batch loss: 0.0003427836927585304 \n",
            " Epoch 25\n",
            "[500/64] batch loss: 0.0003187755646649748 \n",
            " Epoch 26\n",
            "[500/64] batch loss: 0.0002793149615172297 \n",
            " Epoch 27\n",
            "[500/64] batch loss: 0.00027110771043226123 \n",
            " Epoch 28\n",
            "[500/64] batch loss: 0.0003371287020854652 \n",
            " Epoch 29\n",
            "[500/64] batch loss: 0.00028373044915497303 \n",
            " Epoch 30\n",
            "[500/64] batch loss: 0.00030041643185541034 \n",
            " Epoch 31\n",
            "[500/64] batch loss: 0.00025145336985588074 \n",
            " Epoch 32\n",
            "[500/64] batch loss: 0.0002575349644757807 \n",
            " Epoch 33\n",
            "[500/64] batch loss: 0.0002930182672571391 \n",
            " Epoch 34\n",
            "[500/64] batch loss: 0.0003071130777243525 \n",
            " Epoch 35\n",
            "[500/64] batch loss: 0.0002932819479610771 \n",
            " Epoch 36\n",
            "[500/64] batch loss: 0.00033940942375920713 \n",
            " Epoch 37\n",
            "[500/64] batch loss: 0.00023354716540779918 \n",
            " Epoch 38\n",
            "[500/64] batch loss: 0.00034203153336420655 \n",
            " Epoch 39\n",
            "[500/64] batch loss: 0.00028923447825945914 \n",
            " Epoch 40\n",
            "[500/64] batch loss: 0.0002937972021754831 \n",
            " Epoch 41\n",
            "[500/64] batch loss: 0.0002877977676689625 \n",
            " Epoch 42\n",
            "[500/64] batch loss: 0.00032736026332713664 \n",
            " Epoch 43\n",
            "[500/64] batch loss: 0.0003165373345836997 \n",
            " Epoch 44\n",
            "[500/64] batch loss: 0.00029185498715378344 \n",
            " Epoch 45\n",
            "[500/64] batch loss: 0.0002782123629003763 \n",
            " Epoch 46\n",
            "[500/64] batch loss: 0.00028923683566972613 \n",
            " Epoch 47\n",
            "[500/64] batch loss: 0.0002677419688552618 \n",
            " Epoch 48\n",
            "[500/64] batch loss: 0.0002603876346256584 \n",
            " Epoch 49\n",
            "[500/64] batch loss: 0.00024093866522889584 \n",
            " Epoch 50\n",
            "[500/64] batch loss: 0.00022616228670813143 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 0.0002699041508289143\n",
            "Validation loss for fold 4: 0.0002996866874020267\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 0.00028598587019333047\n",
            "Training Fold 1 Loss: 0.00027589076239630065\n",
            "Training Fold 2 Loss: 0.00027264169502902674\n",
            "Training Fold 3 Loss: 0.0002726816405545791\n",
            "Training Fold 4 Loss: 0.0002699041508289143\n",
            "Trainig Average Loss: 0.0002754208238004303\n",
            "Validation Fold 0 Loss: 0.0003187963752845113 \n",
            "Validation Fold 1 Loss: 0.00031115759250622545 \n",
            "Validation Fold 2 Loss: 0.0003037763578154023 \n",
            "Validation Fold 3 Loss: 0.00030150997144298285 \n",
            "Validation Fold 4 Loss: 0.0002996866874020267 \n",
            "Vaidation Average Loss: 0.00030698539689022974\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00041865644743666053 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00034402799792587757 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003120782203041017 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.000269977783318609 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00024436184321530163 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0002477228408679366 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00022044277284294367 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.000249790697125718 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00020811711146961898 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00018844632722903043 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0002168531354982406 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00019963359227403998 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00018516169802751392 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00019613861513789743 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00021253478189464658 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00020533126371446997 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0001878122566267848 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.0001897415640996769 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00018893547530751675 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.000160886425874196 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.0001821330952225253 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00018778283265419304 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0001809199311537668 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00016677002713549882 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00017777126049622893 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0001822137419367209 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00018283848476130515 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00017556120292283595 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00017012961325235665 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00017332553397864103 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00018224050290882587 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00015006173634901643 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00018897974223364145 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00016366351337637752 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00016664493887219578 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00015519544831477106 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00016225969011429697 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00016546077677048743 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00014971828204579651 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.0001661763817537576 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.0001742379245115444 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00014778734475839883 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00015766410797368735 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00015196284221019596 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00015511884703300893 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00015313952462747693 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00014727824600413442 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00015379722754005343 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.0001641890121391043 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.0001400613400619477 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.0001490758693413845\n",
            "Validation loss for fold 0: 0.00016449344274708746\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0003964519710280001 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00031169221620075405 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0003298087976872921 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0002709454274736345 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0002834793704096228 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00023308047093451023 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0002331302675884217 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.000247066403971985 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0002289745316375047 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00021937541896477342 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00022459813044406474 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00021510063379537314 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00022449578682426363 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00021007355826441199 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00019537114712875336 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.0001815769646782428 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00019189050362911075 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00019657376105897129 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.000187247758731246 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00019132340094074607 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00018526098574511707 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00018830127373803407 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0001802909973775968 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00018234695016872138 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.0001743783213896677 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00016251986380666494 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00016832178516779095 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0001760918676154688 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00016876438166946173 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00015838783292565495 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00015426261234097183 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0001793980918591842 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00016154980403371155 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00015158334281295538 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00013271001807879657 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.0001495335454819724 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00015506010095123202 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00018034303502645344 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00016494082228746265 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.0001505635736975819 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00014367717085406184 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00016744415916036814 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00015534498379565775 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0001289552601519972 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.0001707456831354648 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00015016620454844087 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00012982967018615454 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.0001513276802143082 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00012766428699251264 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00012764619896188378 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 0.00014745041840209924\n",
            "Validation loss for fold 1: 0.00016174365473855477\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.000453502987511456 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.0003132355632260442 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.0002675263967830688 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0002615576086100191 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0002447488368488848 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.00023775982845108956 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00022060389164835215 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00023036672791931778 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.0002074513176921755 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00023256579879671335 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00020653282990679145 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00019824363698717207 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.0001779630547389388 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00020932605548296124 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00019161569071002305 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00018870409985538572 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.00020571421191561967 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00020829919958487153 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00019475938461255282 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.0001569115702295676 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00019908486865460873 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.0001994613412534818 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.0001988930453080684 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00019851226534228772 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00019665551371872425 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.0001720776635920629 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00017731187108438462 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.0001728066272335127 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00015939313743729144 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00018749821174424142 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00016497261822223663 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00016473741561640054 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.00018861741409637034 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00017135191592387855 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.0001748349459376186 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00018187356181442738 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00016510147543158382 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.00016465414955746382 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00016277481336146593 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00015971463290043175 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00016015066648833454 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00015502980386372656 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.0001659389672568068 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00017793402366805822 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00017094940994866192 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.0001513063907623291 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00014791435387451202 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00014183175517246127 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00014497220399789512 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00014790646673645824 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 0.00015284471881205511\n",
            "Validation loss for fold 2: 0.00016895102160725735\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.00038327922811731696 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00036288594128564 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.00028806814225390553 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.00026528426678851247 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.00023976940428838134 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0002418275980744511 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.00027449146728031337 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00024337800277862698 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00021508216741494834 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00024004268925637007 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.0002469492028467357 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.00021810292673762888 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.00022503036598209292 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.0001993404293898493 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00021148250380065292 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.0001989648735616356 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0002217422443209216 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00020250389934517443 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.00020469292940106243 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00019242936105001718 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00020349743135739118 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00018509748042561114 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00021786974684800953 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.0001985002454603091 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00017885123088490218 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00018228180124424398 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00017873018805403262 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00019428561790846288 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.00017978517280425876 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00018896195979323238 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.00018067900964524597 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.00017839757492765784 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.000175867389771156 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.00017620465951040387 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.00016898152534849942 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00016715902893338352 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.00015869471826590598 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0001608507736818865 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00014607900811824948 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00015841718413867056 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00017847951676230878 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00015964038902893662 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.00016432293341495097 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.0001750901574268937 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00015944422921165824 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00015171089034993201 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.0001454827142879367 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00014610109792556614 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00015084417827893049 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.0001384939532727003 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 0.0001511169903872096\n",
            "Validation loss for fold 3: 0.00016194054891875652\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[200/128] batch loss: 0.0004285366740077734 \n",
            " Epoch 2\n",
            "[200/128] batch loss: 0.00033777806675061584 \n",
            " Epoch 3\n",
            "[200/128] batch loss: 0.000274098216323182 \n",
            " Epoch 4\n",
            "[200/128] batch loss: 0.0002561199071351439 \n",
            " Epoch 5\n",
            "[200/128] batch loss: 0.0002513587533030659 \n",
            " Epoch 6\n",
            "[200/128] batch loss: 0.0002259991888422519 \n",
            " Epoch 7\n",
            "[200/128] batch loss: 0.0002231458347523585 \n",
            " Epoch 8\n",
            "[200/128] batch loss: 0.00022101058857515454 \n",
            " Epoch 9\n",
            "[200/128] batch loss: 0.00020705285714939237 \n",
            " Epoch 10\n",
            "[200/128] batch loss: 0.00021103140898048878 \n",
            " Epoch 11\n",
            "[200/128] batch loss: 0.00017672590911388397 \n",
            " Epoch 12\n",
            "[200/128] batch loss: 0.0001974594924831763 \n",
            " Epoch 13\n",
            "[200/128] batch loss: 0.000202076873392798 \n",
            " Epoch 14\n",
            "[200/128] batch loss: 0.00020006178237963468 \n",
            " Epoch 15\n",
            "[200/128] batch loss: 0.00020266251522116363 \n",
            " Epoch 16\n",
            "[200/128] batch loss: 0.00019275765225756913 \n",
            " Epoch 17\n",
            "[200/128] batch loss: 0.0001811480469768867 \n",
            " Epoch 18\n",
            "[200/128] batch loss: 0.00019835159764625132 \n",
            " Epoch 19\n",
            "[200/128] batch loss: 0.0001682767178863287 \n",
            " Epoch 20\n",
            "[200/128] batch loss: 0.00018575749709270895 \n",
            " Epoch 21\n",
            "[200/128] batch loss: 0.00016842337208800018 \n",
            " Epoch 22\n",
            "[200/128] batch loss: 0.00016421242617070675 \n",
            " Epoch 23\n",
            "[200/128] batch loss: 0.00017840172222349793 \n",
            " Epoch 24\n",
            "[200/128] batch loss: 0.00018653189181350172 \n",
            " Epoch 25\n",
            "[200/128] batch loss: 0.00016569180297665298 \n",
            " Epoch 26\n",
            "[200/128] batch loss: 0.00016310194041579962 \n",
            " Epoch 27\n",
            "[200/128] batch loss: 0.00015801540575921535 \n",
            " Epoch 28\n",
            "[200/128] batch loss: 0.00016951811267063022 \n",
            " Epoch 29\n",
            "[200/128] batch loss: 0.0001580420066602528 \n",
            " Epoch 30\n",
            "[200/128] batch loss: 0.00014916078362148255 \n",
            " Epoch 31\n",
            "[200/128] batch loss: 0.0001684028684394434 \n",
            " Epoch 32\n",
            "[200/128] batch loss: 0.0001427991664968431 \n",
            " Epoch 33\n",
            "[200/128] batch loss: 0.0001569195301271975 \n",
            " Epoch 34\n",
            "[200/128] batch loss: 0.0001556235074531287 \n",
            " Epoch 35\n",
            "[200/128] batch loss: 0.0001556880451971665 \n",
            " Epoch 36\n",
            "[200/128] batch loss: 0.00015047829947434366 \n",
            " Epoch 37\n",
            "[200/128] batch loss: 0.0001432637800462544 \n",
            " Epoch 38\n",
            "[200/128] batch loss: 0.0001502697414252907 \n",
            " Epoch 39\n",
            "[200/128] batch loss: 0.00014287522935774177 \n",
            " Epoch 40\n",
            "[200/128] batch loss: 0.00016744084132369608 \n",
            " Epoch 41\n",
            "[200/128] batch loss: 0.00016653425700496882 \n",
            " Epoch 42\n",
            "[200/128] batch loss: 0.00016332599625457078 \n",
            " Epoch 43\n",
            "[200/128] batch loss: 0.0001351942482870072 \n",
            " Epoch 44\n",
            "[200/128] batch loss: 0.00012978292943444103 \n",
            " Epoch 45\n",
            "[200/128] batch loss: 0.00014667297364212573 \n",
            " Epoch 46\n",
            "[200/128] batch loss: 0.00013627843873109668 \n",
            " Epoch 47\n",
            "[200/128] batch loss: 0.00014656048733741045 \n",
            " Epoch 48\n",
            "[200/128] batch loss: 0.00014786487736273557 \n",
            " Epoch 49\n",
            "[200/128] batch loss: 0.00013785433839075267 \n",
            " Epoch 50\n",
            "[200/128] batch loss: 0.00012508584768511355 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 0.00014307948304232083\n",
            "Validation loss for fold 4: 0.000154760340407047\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 0.0001490758693413845\n",
            "Training Fold 1 Loss: 0.00014745041840209924\n",
            "Training Fold 2 Loss: 0.00015284471881205511\n",
            "Training Fold 3 Loss: 0.0001511169903872096\n",
            "Training Fold 4 Loss: 0.00014307948304232083\n",
            "Trainig Average Loss: 0.00014871349599701386\n",
            "Validation Fold 0 Loss: 0.00016449344274708746 \n",
            "Validation Fold 1 Loss: 0.00016174365473855477 \n",
            "Validation Fold 2 Loss: 0.00016895102160725735 \n",
            "Validation Fold 3 Loss: 0.00016194054891875652 \n",
            "Validation Fold 4 Loss: 0.000154760340407047 \n",
            "Vaidation Average Loss: 0.0001623778016837406\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/512] batch loss: 0.0004883510409854352 \n",
            " Epoch 2\n",
            "[0/512] batch loss: 0.00011394838656997308 \n",
            " Epoch 3\n",
            "[0/512] batch loss: 0.00011168819037266076 \n",
            " Epoch 4\n",
            "[0/512] batch loss: 0.00010282478615408763 \n",
            " Epoch 5\n",
            "[0/512] batch loss: 9.935141133610159e-05 \n",
            " Epoch 6\n",
            "[0/512] batch loss: 9.810509800445288e-05 \n",
            " Epoch 7\n",
            "[0/512] batch loss: 8.866274583851919e-05 \n",
            " Epoch 8\n",
            "[0/512] batch loss: 8.376760524697602e-05 \n",
            " Epoch 9\n",
            "[0/512] batch loss: 7.900238415459171e-05 \n",
            " Epoch 10\n",
            "[0/512] batch loss: 7.832189294276759e-05 \n",
            " Epoch 11\n",
            "[0/512] batch loss: 7.181753608165309e-05 \n",
            " Epoch 12\n",
            "[0/512] batch loss: 7.482647197321057e-05 \n",
            " Epoch 13\n",
            "[0/512] batch loss: 7.002572237979621e-05 \n",
            " Epoch 14\n",
            "[0/512] batch loss: 6.771974585717544e-05 \n",
            " Epoch 15\n",
            "[0/512] batch loss: 6.282392132561654e-05 \n",
            " Epoch 16\n",
            "[0/512] batch loss: 6.397040851879865e-05 \n",
            " Epoch 17\n",
            "[0/512] batch loss: 6.430166831705719e-05 \n",
            " Epoch 18\n",
            "[0/512] batch loss: 6.107430817792192e-05 \n",
            " Epoch 19\n",
            "[0/512] batch loss: 6.420918361982331e-05 \n",
            " Epoch 20\n",
            "[0/512] batch loss: 5.987560871290043e-05 \n",
            " Epoch 21\n",
            "[0/512] batch loss: 6.117711745901033e-05 \n",
            " Epoch 22\n",
            "[0/512] batch loss: 5.765472451457754e-05 \n",
            " Epoch 23\n",
            "[0/512] batch loss: 6.237423804122955e-05 \n",
            " Epoch 24\n",
            "[0/512] batch loss: 5.634774061036296e-05 \n",
            " Epoch 25\n",
            "[0/512] batch loss: 6.041008236934431e-05 \n",
            " Epoch 26\n",
            "[0/512] batch loss: 5.980011337669566e-05 \n",
            " Epoch 27\n",
            "[0/512] batch loss: 5.780158971901983e-05 \n",
            " Epoch 28\n",
            "[0/512] batch loss: 5.863351179868914e-05 \n",
            " Epoch 29\n",
            "[0/512] batch loss: 5.410637822933495e-05 \n",
            " Epoch 30\n",
            "[0/512] batch loss: 5.8328350860392675e-05 \n",
            " Epoch 31\n",
            "[0/512] batch loss: 5.439721280708909e-05 \n",
            " Epoch 32\n",
            "[0/512] batch loss: 5.402484748628922e-05 \n",
            " Epoch 33\n",
            "[0/512] batch loss: 5.2612958825193346e-05 \n",
            " Epoch 34\n",
            "[0/512] batch loss: 5.210355084273033e-05 \n",
            " Epoch 35\n",
            "[0/512] batch loss: 5.295065056998283e-05 \n",
            " Epoch 36\n",
            "[0/512] batch loss: 5.49577634956222e-05 \n",
            " Epoch 37\n",
            "[0/512] batch loss: 5.549968045670539e-05 \n",
            " Epoch 38\n",
            "[0/512] batch loss: 5.367414632928558e-05 \n",
            " Epoch 39\n",
            "[0/512] batch loss: 5.0786482461262494e-05 \n",
            " Epoch 40\n",
            "[0/512] batch loss: 5.3297651902539656e-05 \n",
            " Epoch 41\n",
            "[0/512] batch loss: 5.324095764080994e-05 \n",
            " Epoch 42\n",
            "[0/512] batch loss: 4.9766022129915655e-05 \n",
            " Epoch 43\n",
            "[0/512] batch loss: 5.1004288252443075e-05 \n",
            " Epoch 44\n",
            "[0/512] batch loss: 4.971577800461091e-05 \n",
            " Epoch 45\n",
            "[0/512] batch loss: 5.4031304898671806e-05 \n",
            " Epoch 46\n",
            "[0/512] batch loss: 5.1168255595257506e-05 \n",
            " Epoch 47\n",
            "[0/512] batch loss: 4.979547156835906e-05 \n",
            " Epoch 48\n",
            "[0/512] batch loss: 5.4210646339925006e-05 \n",
            " Epoch 49\n",
            "[0/512] batch loss: 5.116324609844014e-05 \n",
            " Epoch 50\n",
            "[0/512] batch loss: 4.921598520013504e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 5.040040054146387e-05\n",
            "Validation loss for fold 0: 5.129647165286503e-05\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/512] batch loss: 0.0004889425472356379 \n",
            " Epoch 2\n",
            "[0/512] batch loss: 0.00011776204337365925 \n",
            " Epoch 3\n",
            "[0/512] batch loss: 0.00010993816249538213 \n",
            " Epoch 4\n",
            "[0/512] batch loss: 0.00010329327778890729 \n",
            " Epoch 5\n",
            "[0/512] batch loss: 0.0001047032346832566 \n",
            " Epoch 6\n",
            "[0/512] batch loss: 8.949493349064142e-05 \n",
            " Epoch 7\n",
            "[0/512] batch loss: 8.326717943418771e-05 \n",
            " Epoch 8\n",
            "[0/512] batch loss: 8.109644113574177e-05 \n",
            " Epoch 9\n",
            "[0/512] batch loss: 7.789021037751809e-05 \n",
            " Epoch 10\n",
            "[0/512] batch loss: 7.505941175622866e-05 \n",
            " Epoch 11\n",
            "[0/512] batch loss: 7.147341239033267e-05 \n",
            " Epoch 12\n",
            "[0/512] batch loss: 7.439756154781207e-05 \n",
            " Epoch 13\n",
            "[0/512] batch loss: 7.162453402997926e-05 \n",
            " Epoch 14\n",
            "[0/512] batch loss: 6.584871880477294e-05 \n",
            " Epoch 15\n",
            "[0/512] batch loss: 6.846488395240158e-05 \n",
            " Epoch 16\n",
            "[0/512] batch loss: 6.69569635647349e-05 \n",
            " Epoch 17\n",
            "[0/512] batch loss: 5.8405817981110886e-05 \n",
            " Epoch 18\n",
            "[0/512] batch loss: 6.564953946508467e-05 \n",
            " Epoch 19\n",
            "[0/512] batch loss: 6.15064927842468e-05 \n",
            " Epoch 20\n",
            "[0/512] batch loss: 6.132123962743208e-05 \n",
            " Epoch 21\n",
            "[0/512] batch loss: 5.762733781011775e-05 \n",
            " Epoch 22\n",
            "[0/512] batch loss: 6.127933011157438e-05 \n",
            " Epoch 23\n",
            "[0/512] batch loss: 5.7928689784603193e-05 \n",
            " Epoch 24\n",
            "[0/512] batch loss: 6.146412488305941e-05 \n",
            " Epoch 25\n",
            "[0/512] batch loss: 6.091866816859692e-05 \n",
            " Epoch 26\n",
            "[0/512] batch loss: 5.98119841015432e-05 \n",
            " Epoch 27\n",
            "[0/512] batch loss: 5.493923163157888e-05 \n",
            " Epoch 28\n",
            "[0/512] batch loss: 5.6566783314337954e-05 \n",
            " Epoch 29\n",
            "[0/512] batch loss: 5.702978523913771e-05 \n",
            " Epoch 30\n",
            "[0/512] batch loss: 5.824448453495279e-05 \n",
            " Epoch 31\n",
            "[0/512] batch loss: 5.527701068785973e-05 \n",
            " Epoch 32\n",
            "[0/512] batch loss: 5.516691453522071e-05 \n",
            " Epoch 33\n",
            "[0/512] batch loss: 5.4767260735388845e-05 \n",
            " Epoch 34\n",
            "[0/512] batch loss: 5.8092657127417624e-05 \n",
            " Epoch 35\n",
            "[0/512] batch loss: 5.249458263278939e-05 \n",
            " Epoch 36\n",
            "[0/512] batch loss: 5.218667502049357e-05 \n",
            " Epoch 37\n",
            "[0/512] batch loss: 5.3137260692892596e-05 \n",
            " Epoch 38\n",
            "[0/512] batch loss: 5.24265160493087e-05 \n",
            " Epoch 39\n",
            "[0/512] batch loss: 5.219380182097666e-05 \n",
            " Epoch 40\n",
            "[0/512] batch loss: 4.990827073925175e-05 \n",
            " Epoch 41\n",
            "[0/512] batch loss: 4.966148117091507e-05 \n",
            " Epoch 42\n",
            "[0/512] batch loss: 4.976113632437773e-05 \n",
            " Epoch 43\n",
            "[0/512] batch loss: 5.1143102609785274e-05 \n",
            " Epoch 44\n",
            "[0/512] batch loss: 5.251082620816305e-05 \n",
            " Epoch 45\n",
            "[0/512] batch loss: 5.134666571393609e-05 \n",
            " Epoch 46\n",
            "[0/512] batch loss: 5.222982872510329e-05 \n",
            " Epoch 47\n",
            "[0/512] batch loss: 5.401008456829004e-05 \n",
            " Epoch 48\n",
            "[0/512] batch loss: 5.000747478334233e-05 \n",
            " Epoch 49\n",
            "[0/512] batch loss: 5.167444396647625e-05 \n",
            " Epoch 50\n",
            "[0/512] batch loss: 4.8984973545884714e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 5.025683412450621e-05\n",
            "Validation loss for fold 1: 5.16513320183589e-05\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/512] batch loss: 0.0004885155940428376 \n",
            " Epoch 2\n",
            "[0/512] batch loss: 0.00010979441867675632 \n",
            " Epoch 3\n",
            "[0/512] batch loss: 0.0001083607057807967 \n",
            " Epoch 4\n",
            "[0/512] batch loss: 0.00010788848885567859 \n",
            " Epoch 5\n",
            "[0/512] batch loss: 0.00010364335321355611 \n",
            " Epoch 6\n",
            "[0/512] batch loss: 9.026624320540577e-05 \n",
            " Epoch 7\n",
            "[0/512] batch loss: 8.156501280609518e-05 \n",
            " Epoch 8\n",
            "[0/512] batch loss: 7.779640873195603e-05 \n",
            " Epoch 9\n",
            "[0/512] batch loss: 7.802849722793326e-05 \n",
            " Epoch 10\n",
            "[0/512] batch loss: 7.187003211583942e-05 \n",
            " Epoch 11\n",
            "[0/512] batch loss: 7.389160600723699e-05 \n",
            " Epoch 12\n",
            "[0/512] batch loss: 7.18422670615837e-05 \n",
            " Epoch 13\n",
            "[0/512] batch loss: 6.590930570382625e-05 \n",
            " Epoch 14\n",
            "[0/512] batch loss: 6.981044862186536e-05 \n",
            " Epoch 15\n",
            "[0/512] batch loss: 6.48833010927774e-05 \n",
            " Epoch 16\n",
            "[0/512] batch loss: 6.487571954494342e-05 \n",
            " Epoch 17\n",
            "[0/512] batch loss: 6.467993807746097e-05 \n",
            " Epoch 18\n",
            "[0/512] batch loss: 5.9475176385603845e-05 \n",
            " Epoch 19\n",
            "[0/512] batch loss: 5.831291855429299e-05 \n",
            " Epoch 20\n",
            "[0/512] batch loss: 5.7292891142424196e-05 \n",
            " Epoch 21\n",
            "[0/512] batch loss: 5.8670808357419446e-05 \n",
            " Epoch 22\n",
            "[0/512] batch loss: 5.566706749959849e-05 \n",
            " Epoch 23\n",
            "[0/512] batch loss: 5.422824324341491e-05 \n",
            " Epoch 24\n",
            "[0/512] batch loss: 5.500066254171543e-05 \n",
            " Epoch 25\n",
            "[0/512] batch loss: 5.5187665566336364e-05 \n",
            " Epoch 26\n",
            "[0/512] batch loss: 5.6247015891131014e-05 \n",
            " Epoch 27\n",
            "[0/512] batch loss: 5.1807328418362886e-05 \n",
            " Epoch 28\n",
            "[0/512] batch loss: 5.497016536537558e-05 \n",
            " Epoch 29\n",
            "[0/512] batch loss: 5.3035975724924356e-05 \n",
            " Epoch 30\n",
            "[0/512] batch loss: 5.304575461195782e-05 \n",
            " Epoch 31\n",
            "[0/512] batch loss: 5.355290704756044e-05 \n",
            " Epoch 32\n",
            "[0/512] batch loss: 5.3551411838270724e-05 \n",
            " Epoch 33\n",
            "[0/512] batch loss: 5.227728252066299e-05 \n",
            " Epoch 34\n",
            "[0/512] batch loss: 5.070700717624277e-05 \n",
            " Epoch 35\n",
            "[0/512] batch loss: 4.917492333333939e-05 \n",
            " Epoch 36\n",
            "[0/512] batch loss: 5.0136124627897516e-05 \n",
            " Epoch 37\n",
            "[0/512] batch loss: 4.8857542424229905e-05 \n",
            " Epoch 38\n",
            "[0/512] batch loss: 4.7742563765496016e-05 \n",
            " Epoch 39\n",
            "[0/512] batch loss: 5.064100696472451e-05 \n",
            " Epoch 40\n",
            "[0/512] batch loss: 4.8341957153752446e-05 \n",
            " Epoch 41\n",
            "[0/512] batch loss: 5.0099137297365814e-05 \n",
            " Epoch 42\n",
            "[0/512] batch loss: 4.702578371507116e-05 \n",
            " Epoch 43\n",
            "[0/512] batch loss: 4.971920498064719e-05 \n",
            " Epoch 44\n",
            "[0/512] batch loss: 5.059465183876455e-05 \n",
            " Epoch 45\n",
            "[0/512] batch loss: 4.870621341979131e-05 \n",
            " Epoch 46\n",
            "[0/512] batch loss: 5.0194779760204256e-05 \n",
            " Epoch 47\n",
            "[0/512] batch loss: 4.756897033075802e-05 \n",
            " Epoch 48\n",
            "[0/512] batch loss: 4.792496110894717e-05 \n",
            " Epoch 49\n",
            "[0/512] batch loss: 4.726538827526383e-05 \n",
            " Epoch 50\n",
            "[0/512] batch loss: 4.45999285147991e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 4.796038475966362e-05\n",
            "Validation loss for fold 2: 4.95754496691726e-05\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/512] batch loss: 0.0004916857578791678 \n",
            " Epoch 2\n",
            "[0/512] batch loss: 0.00011601699225138873 \n",
            " Epoch 3\n",
            "[0/512] batch loss: 0.00010634081991156563 \n",
            " Epoch 4\n",
            "[0/512] batch loss: 0.0001020805721054785 \n",
            " Epoch 5\n",
            "[0/512] batch loss: 0.00010030726116383448 \n",
            " Epoch 6\n",
            "[0/512] batch loss: 8.931521006161347e-05 \n",
            " Epoch 7\n",
            "[0/512] batch loss: 8.57794948387891e-05 \n",
            " Epoch 8\n",
            "[0/512] batch loss: 8.09877528809011e-05 \n",
            " Epoch 9\n",
            "[0/512] batch loss: 8.070951298577711e-05 \n",
            " Epoch 10\n",
            "[0/512] batch loss: 7.705964526394382e-05 \n",
            " Epoch 11\n",
            "[0/512] batch loss: 7.667156751267612e-05 \n",
            " Epoch 12\n",
            "[0/512] batch loss: 6.991839472902939e-05 \n",
            " Epoch 13\n",
            "[0/512] batch loss: 7.02092656865716e-05 \n",
            " Epoch 14\n",
            "[0/512] batch loss: 6.977564044063911e-05 \n",
            " Epoch 15\n",
            "[0/512] batch loss: 6.911637319717556e-05 \n",
            " Epoch 16\n",
            "[0/512] batch loss: 6.238690548343584e-05 \n",
            " Epoch 17\n",
            "[0/512] batch loss: 6.388274050550535e-05 \n",
            " Epoch 18\n",
            "[0/512] batch loss: 6.4143467170652e-05 \n",
            " Epoch 19\n",
            "[0/512] batch loss: 5.824119216413237e-05 \n",
            " Epoch 20\n",
            "[0/512] batch loss: 5.5171680287458e-05 \n",
            " Epoch 21\n",
            "[0/512] batch loss: 6.0124908486614004e-05 \n",
            " Epoch 22\n",
            "[0/512] batch loss: 5.923062781221233e-05 \n",
            " Epoch 23\n",
            "[0/512] batch loss: 5.610021617030725e-05 \n",
            " Epoch 24\n",
            "[0/512] batch loss: 5.811453956994228e-05 \n",
            " Epoch 25\n",
            "[0/512] batch loss: 5.9766109188785776e-05 \n",
            " Epoch 26\n",
            "[0/512] batch loss: 5.640586095978506e-05 \n",
            " Epoch 27\n",
            "[0/512] batch loss: 5.57616185687948e-05 \n",
            " Epoch 28\n",
            "[0/512] batch loss: 5.5552351113874465e-05 \n",
            " Epoch 29\n",
            "[0/512] batch loss: 5.5956916185095906e-05 \n",
            " Epoch 30\n",
            "[0/512] batch loss: 5.531991700991057e-05 \n",
            " Epoch 31\n",
            "[0/512] batch loss: 5.5293116020038724e-05 \n",
            " Epoch 32\n",
            "[0/512] batch loss: 5.463422348839231e-05 \n",
            " Epoch 33\n",
            "[0/512] batch loss: 5.432188481790945e-05 \n",
            " Epoch 34\n",
            "[0/512] batch loss: 5.12596670887433e-05 \n",
            " Epoch 35\n",
            "[0/512] batch loss: 5.412491736933589e-05 \n",
            " Epoch 36\n",
            "[0/512] batch loss: 4.95506938023027e-05 \n",
            " Epoch 37\n",
            "[0/512] batch loss: 5.2328381570987403e-05 \n",
            " Epoch 38\n",
            "[0/512] batch loss: 5.098546898807399e-05 \n",
            " Epoch 39\n",
            "[0/512] batch loss: 4.943008389091119e-05 \n",
            " Epoch 40\n",
            "[0/512] batch loss: 5.294930815580301e-05 \n",
            " Epoch 41\n",
            "[0/512] batch loss: 5.059125032857992e-05 \n",
            " Epoch 42\n",
            "[0/512] batch loss: 5.325243910192512e-05 \n",
            " Epoch 43\n",
            "[0/512] batch loss: 5.275066723697819e-05 \n",
            " Epoch 44\n",
            "[0/512] batch loss: 5.0232407375006005e-05 \n",
            " Epoch 45\n",
            "[0/512] batch loss: 5.341717405826785e-05 \n",
            " Epoch 46\n",
            "[0/512] batch loss: 5.007738946005702e-05 \n",
            " Epoch 47\n",
            "[0/512] batch loss: 5.023678022553213e-05 \n",
            " Epoch 48\n",
            "[0/512] batch loss: 4.703666490968317e-05 \n",
            " Epoch 49\n",
            "[0/512] batch loss: 5.146341572981328e-05 \n",
            " Epoch 50\n",
            "[0/512] batch loss: 4.909242852590978e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 4.923823694464783e-05\n",
            "Validation loss for fold 3: 5.003776238351685e-05\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/512] batch loss: 0.0004871945711784065 \n",
            " Epoch 2\n",
            "[0/512] batch loss: 0.00011529640323715284 \n",
            " Epoch 3\n",
            "[0/512] batch loss: 0.00010908700642175972 \n",
            " Epoch 4\n",
            "[0/512] batch loss: 0.00010445510997669771 \n",
            " Epoch 5\n",
            "[0/512] batch loss: 9.66888401308097e-05 \n",
            " Epoch 6\n",
            "[0/512] batch loss: 8.666342910146341e-05 \n",
            " Epoch 7\n",
            "[0/512] batch loss: 8.20589266368188e-05 \n",
            " Epoch 8\n",
            "[0/512] batch loss: 7.913063745945692e-05 \n",
            " Epoch 9\n",
            "[0/512] batch loss: 7.635387009941041e-05 \n",
            " Epoch 10\n",
            "[0/512] batch loss: 7.413470302708447e-05 \n",
            " Epoch 11\n",
            "[0/512] batch loss: 7.017723692115396e-05 \n",
            " Epoch 12\n",
            "[0/512] batch loss: 7.094410102581605e-05 \n",
            " Epoch 13\n",
            "[0/512] batch loss: 6.555867730639875e-05 \n",
            " Epoch 14\n",
            "[0/512] batch loss: 6.609322736039758e-05 \n",
            " Epoch 15\n",
            "[0/512] batch loss: 6.304167618509382e-05 \n",
            " Epoch 16\n",
            "[0/512] batch loss: 6.194857269292697e-05 \n",
            " Epoch 17\n",
            "[0/512] batch loss: 5.9399197198217735e-05 \n",
            " Epoch 18\n",
            "[0/512] batch loss: 5.8831301430473104e-05 \n",
            " Epoch 19\n",
            "[0/512] batch loss: 5.643180702463724e-05 \n",
            " Epoch 20\n",
            "[0/512] batch loss: 5.7366647524759173e-05 \n",
            " Epoch 21\n",
            "[0/512] batch loss: 5.5375574447680265e-05 \n",
            " Epoch 22\n",
            "[0/512] batch loss: 5.6381861213594675e-05 \n",
            " Epoch 23\n",
            "[0/512] batch loss: 5.2361516281962395e-05 \n",
            " Epoch 24\n",
            "[0/512] batch loss: 5.695178697351366e-05 \n",
            " Epoch 25\n",
            "[0/512] batch loss: 5.2767019951716065e-05 \n",
            " Epoch 26\n",
            "[0/512] batch loss: 5.579470962402411e-05 \n",
            " Epoch 27\n",
            "[0/512] batch loss: 5.268683526082896e-05 \n",
            " Epoch 28\n",
            "[0/512] batch loss: 5.296130257192999e-05 \n",
            " Epoch 29\n",
            "[0/512] batch loss: 5.1501607231330127e-05 \n",
            " Epoch 30\n",
            "[0/512] batch loss: 5.318491457728669e-05 \n",
            " Epoch 31\n",
            "[0/512] batch loss: 5.2468029025476426e-05 \n",
            " Epoch 32\n",
            "[0/512] batch loss: 5.188450086279772e-05 \n",
            " Epoch 33\n",
            "[0/512] batch loss: 5.077916648588143e-05 \n",
            " Epoch 34\n",
            "[0/512] batch loss: 5.2526123909046873e-05 \n",
            " Epoch 35\n",
            "[0/512] batch loss: 5.057645830675028e-05 \n",
            " Epoch 36\n",
            "[0/512] batch loss: 5.2463816246017814e-05 \n",
            " Epoch 37\n",
            "[0/512] batch loss: 4.5896609663031995e-05 \n",
            " Epoch 38\n",
            "[0/512] batch loss: 4.777575304615311e-05 \n",
            " Epoch 39\n",
            "[0/512] batch loss: 4.727458144770935e-05 \n",
            " Epoch 40\n",
            "[0/512] batch loss: 4.705257015302777e-05 \n",
            " Epoch 41\n",
            "[0/512] batch loss: 4.800474926014431e-05 \n",
            " Epoch 42\n",
            "[0/512] batch loss: 4.906420144834556e-05 \n",
            " Epoch 43\n",
            "[0/512] batch loss: 4.8025856813183054e-05 \n",
            " Epoch 44\n",
            "[0/512] batch loss: 4.925236135022715e-05 \n",
            " Epoch 45\n",
            "[0/512] batch loss: 4.854797225561924e-05 \n",
            " Epoch 46\n",
            "[0/512] batch loss: 4.7656802053097636e-05 \n",
            " Epoch 47\n",
            "[0/512] batch loss: 4.393952985992655e-05 \n",
            " Epoch 48\n",
            "[0/512] batch loss: 4.624886787496507e-05 \n",
            " Epoch 49\n",
            "[0/512] batch loss: 4.394637289806269e-05 \n",
            " Epoch 50\n",
            "[0/512] batch loss: 4.6370889322133735e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 4.6600928713738076e-05\n",
            "Validation loss for fold 4: 4.87816706806123e-05\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 5.040040054146387e-05\n",
            "Training Fold 1 Loss: 5.025683412450621e-05\n",
            "Training Fold 2 Loss: 4.796038475966362e-05\n",
            "Training Fold 3 Loss: 4.923823694464783e-05\n",
            "Training Fold 4 Loss: 4.6600928713738076e-05\n",
            "Trainig Average Loss: 4.889135701680392e-05\n",
            "Validation Fold 0 Loss: 5.129647165286503e-05 \n",
            "Validation Fold 1 Loss: 5.16513320183589e-05 \n",
            "Validation Fold 2 Loss: 4.95754496691726e-05 \n",
            "Validation Fold 3 Loss: 5.003776238351685e-05 \n",
            "Validation Fold 4 Loss: 4.87816706806123e-05 \n",
            "Vaidation Average Loss: 5.026853728090513e-05\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/1024] batch loss: 0.0002441744436509907 \n",
            " Epoch 2\n",
            "[0/1024] batch loss: 5.7937893870985135e-05 \n",
            " Epoch 3\n",
            "[0/1024] batch loss: 5.653097468893975e-05 \n",
            " Epoch 4\n",
            "[0/1024] batch loss: 5.575481191044673e-05 \n",
            " Epoch 5\n",
            "[0/1024] batch loss: 5.411362144513987e-05 \n",
            " Epoch 6\n",
            "[0/1024] batch loss: 5.221266837907024e-05 \n",
            " Epoch 7\n",
            "[0/1024] batch loss: 5.001080717192963e-05 \n",
            " Epoch 8\n",
            "[0/1024] batch loss: 5.002704347134568e-05 \n",
            " Epoch 9\n",
            "[0/1024] batch loss: 5.0113110773963854e-05 \n",
            " Epoch 10\n",
            "[0/1024] batch loss: 4.940224971505813e-05 \n",
            " Epoch 11\n",
            "[0/1024] batch loss: 4.966203050571494e-05 \n",
            " Epoch 12\n",
            "[0/1024] batch loss: 4.535198240773752e-05 \n",
            " Epoch 13\n",
            "[0/1024] batch loss: 4.277197513147257e-05 \n",
            " Epoch 14\n",
            "[0/1024] batch loss: 4.142479883739725e-05 \n",
            " Epoch 15\n",
            "[0/1024] batch loss: 4.140532837482169e-05 \n",
            " Epoch 16\n",
            "[0/1024] batch loss: 4.0693848859518766e-05 \n",
            " Epoch 17\n",
            "[0/1024] batch loss: 4.0265193092636764e-05 \n",
            " Epoch 18\n",
            "[0/1024] batch loss: 3.762963387998752e-05 \n",
            " Epoch 19\n",
            "[0/1024] batch loss: 3.81593854399398e-05 \n",
            " Epoch 20\n",
            "[0/1024] batch loss: 3.6282261135056615e-05 \n",
            " Epoch 21\n",
            "[0/1024] batch loss: 3.608635961427353e-05 \n",
            " Epoch 22\n",
            "[0/1024] batch loss: 3.562786150723696e-05 \n",
            " Epoch 23\n",
            "[0/1024] batch loss: 3.6975234252167866e-05 \n",
            " Epoch 24\n",
            "[0/1024] batch loss: 3.378268957021646e-05 \n",
            " Epoch 25\n",
            "[0/1024] batch loss: 3.5179706173948944e-05 \n",
            " Epoch 26\n",
            "[0/1024] batch loss: 3.4101401979569346e-05 \n",
            " Epoch 27\n",
            "[0/1024] batch loss: 3.364778604009189e-05 \n",
            " Epoch 28\n",
            "[0/1024] batch loss: 3.3682739740470424e-05 \n",
            " Epoch 29\n",
            "[0/1024] batch loss: 3.2951611501630396e-05 \n",
            " Epoch 30\n",
            "[0/1024] batch loss: 3.1929852411849424e-05 \n",
            " Epoch 31\n",
            "[0/1024] batch loss: 3.173345612594858e-05 \n",
            " Epoch 32\n",
            "[0/1024] batch loss: 3.0357890864252113e-05 \n",
            " Epoch 33\n",
            "[0/1024] batch loss: 3.219567588530481e-05 \n",
            " Epoch 34\n",
            "[0/1024] batch loss: 3.042379285034258e-05 \n",
            " Epoch 35\n",
            "[0/1024] batch loss: 2.9893841201555915e-05 \n",
            " Epoch 36\n",
            "[0/1024] batch loss: 2.9976021323818713e-05 \n",
            " Epoch 37\n",
            "[0/1024] batch loss: 3.055876004509628e-05 \n",
            " Epoch 38\n",
            "[0/1024] batch loss: 2.8659700546995737e-05 \n",
            " Epoch 39\n",
            "[0/1024] batch loss: 2.9054321203147992e-05 \n",
            " Epoch 40\n",
            "[0/1024] batch loss: 2.9563270800281316e-05 \n",
            " Epoch 41\n",
            "[0/1024] batch loss: 2.9369786716415547e-05 \n",
            " Epoch 42\n",
            "[0/1024] batch loss: 2.964508894365281e-05 \n",
            " Epoch 43\n",
            "[0/1024] batch loss: 2.9265638659126125e-05 \n",
            " Epoch 44\n",
            "[0/1024] batch loss: 2.8258751626708545e-05 \n",
            " Epoch 45\n",
            "[0/1024] batch loss: 2.9313314371393062e-05 \n",
            " Epoch 46\n",
            "[0/1024] batch loss: 2.8335183742456138e-05 \n",
            " Epoch 47\n",
            "[0/1024] batch loss: 2.724076148297172e-05 \n",
            " Epoch 48\n",
            "[0/1024] batch loss: 2.806226802931633e-05 \n",
            " Epoch 49\n",
            "[0/1024] batch loss: 2.8272386771277525e-05 \n",
            " Epoch 50\n",
            "[0/1024] batch loss: 2.708642387005966e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 2.815645668497417e-05\n",
            "Validation loss for fold 0: 2.843292956112755e-05\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/1024] batch loss: 0.00024445686722174287 \n",
            " Epoch 2\n",
            "[0/1024] batch loss: 6.00237945036497e-05 \n",
            " Epoch 3\n",
            "[0/1024] batch loss: 5.656693974742666e-05 \n",
            " Epoch 4\n",
            "[0/1024] batch loss: 5.7135741371894255e-05 \n",
            " Epoch 5\n",
            "[0/1024] batch loss: 5.349798084353097e-05 \n",
            " Epoch 6\n",
            "[0/1024] batch loss: 5.367142512113787e-05 \n",
            " Epoch 7\n",
            "[0/1024] batch loss: 5.270044130156748e-05 \n",
            " Epoch 8\n",
            "[0/1024] batch loss: 5.2186416723998263e-05 \n",
            " Epoch 9\n",
            "[0/1024] batch loss: 5.276191950542852e-05 \n",
            " Epoch 10\n",
            "[0/1024] batch loss: 5.075077569927089e-05 \n",
            " Epoch 11\n",
            "[0/1024] batch loss: 4.8679590690881014e-05 \n",
            " Epoch 12\n",
            "[0/1024] batch loss: 4.660410195356235e-05 \n",
            " Epoch 13\n",
            "[0/1024] batch loss: 4.3325620936229825e-05 \n",
            " Epoch 14\n",
            "[0/1024] batch loss: 4.089039794052951e-05 \n",
            " Epoch 15\n",
            "[0/1024] batch loss: 4.081116276211105e-05 \n",
            " Epoch 16\n",
            "[0/1024] batch loss: 3.9064125303411856e-05 \n",
            " Epoch 17\n",
            "[0/1024] batch loss: 3.8494967157021165e-05 \n",
            " Epoch 18\n",
            "[0/1024] batch loss: 3.8294583646347746e-05 \n",
            " Epoch 19\n",
            "[0/1024] batch loss: 3.593894871301018e-05 \n",
            " Epoch 20\n",
            "[0/1024] batch loss: 3.675363041111268e-05 \n",
            " Epoch 21\n",
            "[0/1024] batch loss: 3.620079951360822e-05 \n",
            " Epoch 22\n",
            "[0/1024] batch loss: 3.4452456020517275e-05 \n",
            " Epoch 23\n",
            "[0/1024] batch loss: 3.4016491554211825e-05 \n",
            " Epoch 24\n",
            "[0/1024] batch loss: 3.379447662155144e-05 \n",
            " Epoch 25\n",
            "[0/1024] batch loss: 3.266791827627458e-05 \n",
            " Epoch 26\n",
            "[0/1024] batch loss: 3.29455615428742e-05 \n",
            " Epoch 27\n",
            "[0/1024] batch loss: 3.161422864650376e-05 \n",
            " Epoch 28\n",
            "[0/1024] batch loss: 3.121823101537302e-05 \n",
            " Epoch 29\n",
            "[0/1024] batch loss: 3.114823630312458e-05 \n",
            " Epoch 30\n",
            "[0/1024] batch loss: 3.2937652576947585e-05 \n",
            " Epoch 31\n",
            "[0/1024] batch loss: 3.117387677775696e-05 \n",
            " Epoch 32\n",
            "[0/1024] batch loss: 3.014438880200032e-05 \n",
            " Epoch 33\n",
            "[0/1024] batch loss: 3.0255598176154308e-05 \n",
            " Epoch 34\n",
            "[0/1024] batch loss: 3.0281529689091258e-05 \n",
            " Epoch 35\n",
            "[0/1024] batch loss: 3.1299121474148706e-05 \n",
            " Epoch 36\n",
            "[0/1024] batch loss: 2.9757451557088643e-05 \n",
            " Epoch 37\n",
            "[0/1024] batch loss: 2.980454155476764e-05 \n",
            " Epoch 38\n",
            "[0/1024] batch loss: 2.8247100999578834e-05 \n",
            " Epoch 39\n",
            "[0/1024] batch loss: 2.8930866392329335e-05 \n",
            " Epoch 40\n",
            "[0/1024] batch loss: 2.8470216420828365e-05 \n",
            " Epoch 41\n",
            "[0/1024] batch loss: 2.81178618024569e-05 \n",
            " Epoch 42\n",
            "[0/1024] batch loss: 2.8003732950310223e-05 \n",
            " Epoch 43\n",
            "[0/1024] batch loss: 2.6694469852373004e-05 \n",
            " Epoch 44\n",
            "[0/1024] batch loss: 2.6681773306336254e-05 \n",
            " Epoch 45\n",
            "[0/1024] batch loss: 2.6723602786660194e-05 \n",
            " Epoch 46\n",
            "[0/1024] batch loss: 2.7526115445652977e-05 \n",
            " Epoch 47\n",
            "[0/1024] batch loss: 2.766636316664517e-05 \n",
            " Epoch 48\n",
            "[0/1024] batch loss: 2.688379754545167e-05 \n",
            " Epoch 49\n",
            "[0/1024] batch loss: 2.685242179722991e-05 \n",
            " Epoch 50\n",
            "[0/1024] batch loss: 2.715377377171535e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 2.7049619258600773e-05\n",
            "Validation loss for fold 1: 2.7325733150335956e-05\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/1024] batch loss: 0.00024422482238151133 \n",
            " Epoch 2\n",
            "[0/1024] batch loss: 6.023070454830304e-05 \n",
            " Epoch 3\n",
            "[0/1024] batch loss: 5.622400203719735e-05 \n",
            " Epoch 4\n",
            "[0/1024] batch loss: 5.645859710057266e-05 \n",
            " Epoch 5\n",
            "[0/1024] batch loss: 5.3992429457139224e-05 \n",
            " Epoch 6\n",
            "[0/1024] batch loss: 5.270755718811415e-05 \n",
            " Epoch 7\n",
            "[0/1024] batch loss: 5.0939703214680776e-05 \n",
            " Epoch 8\n",
            "[0/1024] batch loss: 4.927591362502426e-05 \n",
            " Epoch 9\n",
            "[0/1024] batch loss: 4.856952728005126e-05 \n",
            " Epoch 10\n",
            "[0/1024] batch loss: 4.900046769762412e-05 \n",
            " Epoch 11\n",
            "[0/1024] batch loss: 4.7335932322312146e-05 \n",
            " Epoch 12\n",
            "[0/1024] batch loss: 4.414061186253093e-05 \n",
            " Epoch 13\n",
            "[0/1024] batch loss: 4.151265966356732e-05 \n",
            " Epoch 14\n",
            "[0/1024] batch loss: 4.0011698729358613e-05 \n",
            " Epoch 15\n",
            "[0/1024] batch loss: 3.839960845652968e-05 \n",
            " Epoch 16\n",
            "[0/1024] batch loss: 3.8699163269484416e-05 \n",
            " Epoch 17\n",
            "[0/1024] batch loss: 3.824189116130583e-05 \n",
            " Epoch 18\n",
            "[0/1024] batch loss: 3.677171116578393e-05 \n",
            " Epoch 19\n",
            "[0/1024] batch loss: 3.736400685738772e-05 \n",
            " Epoch 20\n",
            "[0/1024] batch loss: 3.804568768828176e-05 \n",
            " Epoch 21\n",
            "[0/1024] batch loss: 3.4970842534676194e-05 \n",
            " Epoch 22\n",
            "[0/1024] batch loss: 3.546471270965412e-05 \n",
            " Epoch 23\n",
            "[0/1024] batch loss: 3.462106906226836e-05 \n",
            " Epoch 24\n",
            "[0/1024] batch loss: 3.635903340182267e-05 \n",
            " Epoch 25\n",
            "[0/1024] batch loss: 3.597101385821588e-05 \n",
            " Epoch 26\n",
            "[0/1024] batch loss: 3.340506009408273e-05 \n",
            " Epoch 27\n",
            "[0/1024] batch loss: 3.462498352746479e-05 \n",
            " Epoch 28\n",
            "[0/1024] batch loss: 3.3074411476263776e-05 \n",
            " Epoch 29\n",
            "[0/1024] batch loss: 3.1146948458626866e-05 \n",
            " Epoch 30\n",
            "[0/1024] batch loss: 3.298854426247999e-05 \n",
            " Epoch 31\n",
            "[0/1024] batch loss: 3.1120180210564286e-05 \n",
            " Epoch 32\n",
            "[0/1024] batch loss: 3.1315721571445465e-05 \n",
            " Epoch 33\n",
            "[0/1024] batch loss: 3.092024417128414e-05 \n",
            " Epoch 34\n",
            "[0/1024] batch loss: 3.084817944909446e-05 \n",
            " Epoch 35\n",
            "[0/1024] batch loss: 2.8966134777874686e-05 \n",
            " Epoch 36\n",
            "[0/1024] batch loss: 3.108483360847458e-05 \n",
            " Epoch 37\n",
            "[0/1024] batch loss: 2.985978608194273e-05 \n",
            " Epoch 38\n",
            "[0/1024] batch loss: 3.041282980120741e-05 \n",
            " Epoch 39\n",
            "[0/1024] batch loss: 2.9807910323143005e-05 \n",
            " Epoch 40\n",
            "[0/1024] batch loss: 2.993000998685602e-05 \n",
            " Epoch 41\n",
            "[0/1024] batch loss: 2.9953716875752434e-05 \n",
            " Epoch 42\n",
            "[0/1024] batch loss: 2.9902063033659942e-05 \n",
            " Epoch 43\n",
            "[0/1024] batch loss: 3.0221948691178113e-05 \n",
            " Epoch 44\n",
            "[0/1024] batch loss: 2.9122027626726776e-05 \n",
            " Epoch 45\n",
            "[0/1024] batch loss: 2.8693588319583796e-05 \n",
            " Epoch 46\n",
            "[0/1024] batch loss: 2.8506521630333737e-05 \n",
            " Epoch 47\n",
            "[0/1024] batch loss: 2.8950662454008125e-05 \n",
            " Epoch 48\n",
            "[0/1024] batch loss: 2.9146198357921094e-05 \n",
            " Epoch 49\n",
            "[0/1024] batch loss: 2.8086156817153096e-05 \n",
            " Epoch 50\n",
            "[0/1024] batch loss: 2.8362615921651013e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 2.866089263477841e-05\n",
            "Validation loss for fold 2: 2.9031636273018485e-05\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/1024] batch loss: 0.0002458113885950297 \n",
            " Epoch 2\n",
            "[0/1024] batch loss: 5.931247869739309e-05 \n",
            " Epoch 3\n",
            "[0/1024] batch loss: 5.6255681556649506e-05 \n",
            " Epoch 4\n",
            "[0/1024] batch loss: 5.542666622204706e-05 \n",
            " Epoch 5\n",
            "[0/1024] batch loss: 5.257545126369223e-05 \n",
            " Epoch 6\n",
            "[0/1024] batch loss: 5.1205232011852786e-05 \n",
            " Epoch 7\n",
            "[0/1024] batch loss: 5.12713122589048e-05 \n",
            " Epoch 8\n",
            "[0/1024] batch loss: 5.011991743231192e-05 \n",
            " Epoch 9\n",
            "[0/1024] batch loss: 4.904933666693978e-05 \n",
            " Epoch 10\n",
            "[0/1024] batch loss: 4.904631350655109e-05 \n",
            " Epoch 11\n",
            "[0/1024] batch loss: 4.66921555926092e-05 \n",
            " Epoch 12\n",
            "[0/1024] batch loss: 4.491133222472854e-05 \n",
            " Epoch 13\n",
            "[0/1024] batch loss: 4.308221832616255e-05 \n",
            " Epoch 14\n",
            "[0/1024] batch loss: 4.163067933404818e-05 \n",
            " Epoch 15\n",
            "[0/1024] batch loss: 4.106128108105622e-05 \n",
            " Epoch 16\n",
            "[0/1024] batch loss: 3.9831167669035494e-05 \n",
            " Epoch 17\n",
            "[0/1024] batch loss: 3.8961716199992225e-05 \n",
            " Epoch 18\n",
            "[0/1024] batch loss: 3.8777208828832954e-05 \n",
            " Epoch 19\n",
            "[0/1024] batch loss: 3.76223906641826e-05 \n",
            " Epoch 20\n",
            "[0/1024] batch loss: 3.8122048863442615e-05 \n",
            " Epoch 21\n",
            "[0/1024] batch loss: 3.5823944926960394e-05 \n",
            " Epoch 22\n",
            "[0/1024] batch loss: 3.582745557650924e-05 \n",
            " Epoch 23\n",
            "[0/1024] batch loss: 3.3652806450845674e-05 \n",
            " Epoch 24\n",
            "[0/1024] batch loss: 3.325259967823513e-05 \n",
            " Epoch 25\n",
            "[0/1024] batch loss: 3.45117696269881e-05 \n",
            " Epoch 26\n",
            "[0/1024] batch loss: 3.3574997360119596e-05 \n",
            " Epoch 27\n",
            "[0/1024] batch loss: 3.332715277792886e-05 \n",
            " Epoch 28\n",
            "[0/1024] batch loss: 3.258828292018734e-05 \n",
            " Epoch 29\n",
            "[0/1024] batch loss: 3.289509913884103e-05 \n",
            " Epoch 30\n",
            "[0/1024] batch loss: 3.303922858322039e-05 \n",
            " Epoch 31\n",
            "[0/1024] batch loss: 3.267653664806858e-05 \n",
            " Epoch 32\n",
            "[0/1024] batch loss: 3.083472256548703e-05 \n",
            " Epoch 33\n",
            "[0/1024] batch loss: 3.256005220464431e-05 \n",
            " Epoch 34\n",
            "[0/1024] batch loss: 3.0410195904551074e-05 \n",
            " Epoch 35\n",
            "[0/1024] batch loss: 2.991196197399404e-05 \n",
            " Epoch 36\n",
            "[0/1024] batch loss: 3.194698365405202e-05 \n",
            " Epoch 37\n",
            "[0/1024] batch loss: 3.013985406141728e-05 \n",
            " Epoch 38\n",
            "[0/1024] batch loss: 3.071452374570072e-05 \n",
            " Epoch 39\n",
            "[0/1024] batch loss: 3.04141594824614e-05 \n",
            " Epoch 40\n",
            "[0/1024] batch loss: 2.951486749225296e-05 \n",
            " Epoch 41\n",
            "[0/1024] batch loss: 3.0335568226291798e-05 \n",
            " Epoch 42\n",
            "[0/1024] batch loss: 3.0270372008089907e-05 \n",
            " Epoch 43\n",
            "[0/1024] batch loss: 3.0058787160669453e-05 \n",
            " Epoch 44\n",
            "[0/1024] batch loss: 2.9662560336873867e-05 \n",
            " Epoch 45\n",
            "[0/1024] batch loss: 2.8949893021490425e-05 \n",
            " Epoch 46\n",
            "[0/1024] batch loss: 2.926677916548215e-05 \n",
            " Epoch 47\n",
            "[0/1024] batch loss: 2.871884498745203e-05 \n",
            " Epoch 48\n",
            "[0/1024] batch loss: 2.905916335294023e-05 \n",
            " Epoch 49\n",
            "[0/1024] batch loss: 2.905091423599515e-05 \n",
            " Epoch 50\n",
            "[0/1024] batch loss: 2.906900226662401e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 2.909671633169918e-05\n",
            "Validation loss for fold 3: 2.915883855169309e-05\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/1024] batch loss: 0.00024361966643482447 \n",
            " Epoch 2\n",
            "[0/1024] batch loss: 5.92707256146241e-05 \n",
            " Epoch 3\n",
            "[0/1024] batch loss: 5.5910895753186196e-05 \n",
            " Epoch 4\n",
            "[0/1024] batch loss: 5.588744897977449e-05 \n",
            " Epoch 5\n",
            "[0/1024] batch loss: 5.477767263073474e-05 \n",
            " Epoch 6\n",
            "[0/1024] batch loss: 5.329949999577366e-05 \n",
            " Epoch 7\n",
            "[0/1024] batch loss: 5.0617545639397576e-05 \n",
            " Epoch 8\n",
            "[0/1024] batch loss: 5.062081618234515e-05 \n",
            " Epoch 9\n",
            "[0/1024] batch loss: 5.000525925424881e-05 \n",
            " Epoch 10\n",
            "[0/1024] batch loss: 4.797476503881626e-05 \n",
            " Epoch 11\n",
            "[0/1024] batch loss: 4.555679333861917e-05 \n",
            " Epoch 12\n",
            "[0/1024] batch loss: 4.5439443056238815e-05 \n",
            " Epoch 13\n",
            "[0/1024] batch loss: 4.1660670831333846e-05 \n",
            " Epoch 14\n",
            "[0/1024] batch loss: 3.94827984564472e-05 \n",
            " Epoch 15\n",
            "[0/1024] batch loss: 3.966083386330865e-05 \n",
            " Epoch 16\n",
            "[0/1024] batch loss: 3.847761399811134e-05 \n",
            " Epoch 17\n",
            "[0/1024] batch loss: 3.7455443816725165e-05 \n",
            " Epoch 18\n",
            "[0/1024] batch loss: 3.550133260432631e-05 \n",
            " Epoch 19\n",
            "[0/1024] batch loss: 3.5483524698065594e-05 \n",
            " Epoch 20\n",
            "[0/1024] batch loss: 3.5409731935942546e-05 \n",
            " Epoch 21\n",
            "[0/1024] batch loss: 3.4779252018779516e-05 \n",
            " Epoch 22\n",
            "[0/1024] batch loss: 3.415978790144436e-05 \n",
            " Epoch 23\n",
            "[0/1024] batch loss: 3.454358011367731e-05 \n",
            " Epoch 24\n",
            "[0/1024] batch loss: 3.397775799385272e-05 \n",
            " Epoch 25\n",
            "[0/1024] batch loss: 3.243630635552108e-05 \n",
            " Epoch 26\n",
            "[0/1024] batch loss: 3.1925585062708706e-05 \n",
            " Epoch 27\n",
            "[0/1024] batch loss: 3.1099403713596985e-05 \n",
            " Epoch 28\n",
            "[0/1024] batch loss: 3.100662797805853e-05 \n",
            " Epoch 29\n",
            "[0/1024] batch loss: 3.0156075808918104e-05 \n",
            " Epoch 30\n",
            "[0/1024] batch loss: 2.996540570165962e-05 \n",
            " Epoch 31\n",
            "[0/1024] batch loss: 2.946190033981111e-05 \n",
            " Epoch 32\n",
            "[0/1024] batch loss: 2.8950862542842515e-05 \n",
            " Epoch 33\n",
            "[0/1024] batch loss: 2.791744191199541e-05 \n",
            " Epoch 34\n",
            "[0/1024] batch loss: 2.7903168302145787e-05 \n",
            " Epoch 35\n",
            "[0/1024] batch loss: 2.832224708981812e-05 \n",
            " Epoch 36\n",
            "[0/1024] batch loss: 2.775366920104716e-05 \n",
            " Epoch 37\n",
            "[0/1024] batch loss: 2.751885403995402e-05 \n",
            " Epoch 38\n",
            "[0/1024] batch loss: 2.7535190383787267e-05 \n",
            " Epoch 39\n",
            "[0/1024] batch loss: 2.7906917239306495e-05 \n",
            " Epoch 40\n",
            "[0/1024] batch loss: 2.737544127739966e-05 \n",
            " Epoch 41\n",
            "[0/1024] batch loss: 2.7909465643460862e-05 \n",
            " Epoch 42\n",
            "[0/1024] batch loss: 2.646865505084861e-05 \n",
            " Epoch 43\n",
            "[0/1024] batch loss: 2.7769005100708455e-05 \n",
            " Epoch 44\n",
            "[0/1024] batch loss: 2.721593773458153e-05 \n",
            " Epoch 45\n",
            "[0/1024] batch loss: 2.69256215688074e-05 \n",
            " Epoch 46\n",
            "[0/1024] batch loss: 2.7418756872066297e-05 \n",
            " Epoch 47\n",
            "[0/1024] batch loss: 2.6470175725989975e-05 \n",
            " Epoch 48\n",
            "[0/1024] batch loss: 2.7118665457237512e-05 \n",
            " Epoch 49\n",
            "[0/1024] batch loss: 2.6869765861192718e-05 \n",
            " Epoch 50\n",
            "[0/1024] batch loss: 2.719004260143265e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 2.678399803156039e-05\n",
            "Validation loss for fold 4: 2.7261752426720043e-05\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 2.815645668497417e-05\n",
            "Training Fold 1 Loss: 2.7049619258600773e-05\n",
            "Training Fold 2 Loss: 2.866089263477841e-05\n",
            "Training Fold 3 Loss: 2.909671633169918e-05\n",
            "Training Fold 4 Loss: 2.678399803156039e-05\n",
            "Trainig Average Loss: 2.7949536588322582e-05\n",
            "Validation Fold 0 Loss: 2.843292956112755e-05 \n",
            "Validation Fold 1 Loss: 2.7325733150335956e-05 \n",
            "Validation Fold 2 Loss: 2.9031636273018485e-05 \n",
            "Validation Fold 3 Loss: 2.915883855169309e-05 \n",
            "Validation Fold 4 Loss: 2.7261752426720043e-05 \n",
            "Vaidation Average Loss: 2.8242177992579028e-05\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/4096] batch loss: 6.104484054958448e-05 \n",
            " Epoch 2\n",
            "[0/4096] batch loss: 5.310271808411926e-05 \n",
            " Epoch 3\n",
            "[0/4096] batch loss: 2.6099098249687813e-05 \n",
            " Epoch 4\n",
            "[0/4096] batch loss: 1.5146133591770194e-05 \n",
            " Epoch 5\n",
            "[0/4096] batch loss: 1.4714250937686302e-05 \n",
            " Epoch 6\n",
            "[0/4096] batch loss: 1.4184024621499702e-05 \n",
            " Epoch 7\n",
            "[0/4096] batch loss: 1.437113132851664e-05 \n",
            " Epoch 8\n",
            "[0/4096] batch loss: 1.4303856914921198e-05 \n",
            " Epoch 9\n",
            "[0/4096] batch loss: 1.4205835213942919e-05 \n",
            " Epoch 10\n",
            "[0/4096] batch loss: 1.4063822163734585e-05 \n",
            " Epoch 11\n",
            "[0/4096] batch loss: 1.425417485734215e-05 \n",
            " Epoch 12\n",
            "[0/4096] batch loss: 1.4069557437323965e-05 \n",
            " Epoch 13\n",
            "[0/4096] batch loss: 1.390790112054674e-05 \n",
            " Epoch 14\n",
            "[0/4096] batch loss: 1.3792940990242641e-05 \n",
            " Epoch 15\n",
            "[0/4096] batch loss: 1.3541032785724383e-05 \n",
            " Epoch 16\n",
            "[0/4096] batch loss: 1.3648063941218425e-05 \n",
            " Epoch 17\n",
            "[0/4096] batch loss: 1.3566679626819678e-05 \n",
            " Epoch 18\n",
            "[0/4096] batch loss: 1.3351646884984802e-05 \n",
            " Epoch 19\n",
            "[0/4096] batch loss: 1.3102231605444103e-05 \n",
            " Epoch 20\n",
            "[0/4096] batch loss: 1.3183516784920357e-05 \n",
            " Epoch 21\n",
            "[0/4096] batch loss: 1.3183368537283968e-05 \n",
            " Epoch 22\n",
            "[0/4096] batch loss: 1.305110527027864e-05 \n",
            " Epoch 23\n",
            "[0/4096] batch loss: 1.2714416698145214e-05 \n",
            " Epoch 24\n",
            "[0/4096] batch loss: 1.2689668437815271e-05 \n",
            " Epoch 25\n",
            "[0/4096] batch loss: 1.2600087757164147e-05 \n",
            " Epoch 26\n",
            "[0/4096] batch loss: 1.293462537432788e-05 \n",
            " Epoch 27\n",
            "[0/4096] batch loss: 1.2849926861235872e-05 \n",
            " Epoch 28\n",
            "[0/4096] batch loss: 1.2825588783016428e-05 \n",
            " Epoch 29\n",
            "[0/4096] batch loss: 1.2841994248447008e-05 \n",
            " Epoch 30\n",
            "[0/4096] batch loss: 1.2577714187500533e-05 \n",
            " Epoch 31\n",
            "[0/4096] batch loss: 1.2710170267382637e-05 \n",
            " Epoch 32\n",
            "[0/4096] batch loss: 1.289548981731059e-05 \n",
            " Epoch 33\n",
            "[0/4096] batch loss: 1.2680929103225935e-05 \n",
            " Epoch 34\n",
            "[0/4096] batch loss: 1.2903428796562366e-05 \n",
            " Epoch 35\n",
            "[0/4096] batch loss: 1.2698148566414602e-05 \n",
            " Epoch 36\n",
            "[0/4096] batch loss: 1.259281998500228e-05 \n",
            " Epoch 37\n",
            "[0/4096] batch loss: 1.2677144695771858e-05 \n",
            " Epoch 38\n",
            "[0/4096] batch loss: 1.2662254448514432e-05 \n",
            " Epoch 39\n",
            "[0/4096] batch loss: 1.2870107639173511e-05 \n",
            " Epoch 40\n",
            "[0/4096] batch loss: 1.2258501556061674e-05 \n",
            " Epoch 41\n",
            "[0/4096] batch loss: 1.254909329873044e-05 \n",
            " Epoch 42\n",
            "[0/4096] batch loss: 1.234147566719912e-05 \n",
            " Epoch 43\n",
            "[0/4096] batch loss: 1.2253866771061439e-05 \n",
            " Epoch 44\n",
            "[0/4096] batch loss: 1.2300729395064991e-05 \n",
            " Epoch 45\n",
            "[0/4096] batch loss: 1.2202684047224466e-05 \n",
            " Epoch 46\n",
            "[0/4096] batch loss: 1.2120724932174198e-05 \n",
            " Epoch 47\n",
            "[0/4096] batch loss: 1.2029468962282408e-05 \n",
            " Epoch 48\n",
            "[0/4096] batch loss: 1.1774976883316413e-05 \n",
            " Epoch 49\n",
            "[0/4096] batch loss: 1.1688486665661912e-05 \n",
            " Epoch 50\n",
            "[0/4096] batch loss: 1.1522487511683721e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 1.1603028157502989e-05\n",
            "Validation loss for fold 0: 1.1522501011174021e-05\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/4096] batch loss: 6.111810216680169e-05 \n",
            " Epoch 2\n",
            "[0/4096] batch loss: 5.427034921012819e-05 \n",
            " Epoch 3\n",
            "[0/4096] batch loss: 2.9282571631483734e-05 \n",
            " Epoch 4\n",
            "[0/4096] batch loss: 1.5307690773624927e-05 \n",
            " Epoch 5\n",
            "[0/4096] batch loss: 1.486371002101805e-05 \n",
            " Epoch 6\n",
            "[0/4096] batch loss: 1.4340905181597918e-05 \n",
            " Epoch 7\n",
            "[0/4096] batch loss: 1.4345899217005353e-05 \n",
            " Epoch 8\n",
            "[0/4096] batch loss: 1.4247103536035866e-05 \n",
            " Epoch 9\n",
            "[0/4096] batch loss: 1.4241049029806163e-05 \n",
            " Epoch 10\n",
            "[0/4096] batch loss: 1.4190198271535337e-05 \n",
            " Epoch 11\n",
            "[0/4096] batch loss: 1.422806872142246e-05 \n",
            " Epoch 12\n",
            "[0/4096] batch loss: 1.4162025763653219e-05 \n",
            " Epoch 13\n",
            "[0/4096] batch loss: 1.4123936125542969e-05 \n",
            " Epoch 14\n",
            "[0/4096] batch loss: 1.3886147826269735e-05 \n",
            " Epoch 15\n",
            "[0/4096] batch loss: 1.3628206033899914e-05 \n",
            " Epoch 16\n",
            "[0/4096] batch loss: 1.3348965694603976e-05 \n",
            " Epoch 17\n",
            "[0/4096] batch loss: 1.3175907042750623e-05 \n",
            " Epoch 18\n",
            "[0/4096] batch loss: 1.3095398571749683e-05 \n",
            " Epoch 19\n",
            "[0/4096] batch loss: 1.3150365703040734e-05 \n",
            " Epoch 20\n",
            "[0/4096] batch loss: 1.3118241440679412e-05 \n",
            " Epoch 21\n",
            "[0/4096] batch loss: 1.2972122931387275e-05 \n",
            " Epoch 22\n",
            "[0/4096] batch loss: 1.2877656445198227e-05 \n",
            " Epoch 23\n",
            "[0/4096] batch loss: 1.3114145076542627e-05 \n",
            " Epoch 24\n",
            "[0/4096] batch loss: 1.2998186321055982e-05 \n",
            " Epoch 25\n",
            "[0/4096] batch loss: 1.3017477613175288e-05 \n",
            " Epoch 26\n",
            "[0/4096] batch loss: 1.2883190720458515e-05 \n",
            " Epoch 27\n",
            "[0/4096] batch loss: 1.2863235497206915e-05 \n",
            " Epoch 28\n",
            "[0/4096] batch loss: 1.269063250219915e-05 \n",
            " Epoch 29\n",
            "[0/4096] batch loss: 1.2700293154921383e-05 \n",
            " Epoch 30\n",
            "[0/4096] batch loss: 1.2940668057126459e-05 \n",
            " Epoch 31\n",
            "[0/4096] batch loss: 1.2889057870779652e-05 \n",
            " Epoch 32\n",
            "[0/4096] batch loss: 1.2772547052009031e-05 \n",
            " Epoch 33\n",
            "[0/4096] batch loss: 1.2710198461718392e-05 \n",
            " Epoch 34\n",
            "[0/4096] batch loss: 1.2741712453134824e-05 \n",
            " Epoch 35\n",
            "[0/4096] batch loss: 1.2864466043538414e-05 \n",
            " Epoch 36\n",
            "[0/4096] batch loss: 1.285170583287254e-05 \n",
            " Epoch 37\n",
            "[0/4096] batch loss: 1.2863088159065228e-05 \n",
            " Epoch 38\n",
            "[0/4096] batch loss: 1.2525289093900938e-05 \n",
            " Epoch 39\n",
            "[0/4096] batch loss: 1.2462870472518262e-05 \n",
            " Epoch 40\n",
            "[0/4096] batch loss: 1.2494658221839927e-05 \n",
            " Epoch 41\n",
            "[0/4096] batch loss: 1.2674704521487001e-05 \n",
            " Epoch 42\n",
            "[0/4096] batch loss: 1.2484167200454976e-05 \n",
            " Epoch 43\n",
            "[0/4096] batch loss: 1.2252672604518011e-05 \n",
            " Epoch 44\n",
            "[0/4096] batch loss: 1.2302366485528182e-05 \n",
            " Epoch 45\n",
            "[0/4096] batch loss: 1.2041182344546542e-05 \n",
            " Epoch 46\n",
            "[0/4096] batch loss: 1.1843055290228222e-05 \n",
            " Epoch 47\n",
            "[0/4096] batch loss: 1.1395000910852104e-05 \n",
            " Epoch 48\n",
            "[0/4096] batch loss: 1.1115796041849535e-05 \n",
            " Epoch 49\n",
            "[0/4096] batch loss: 1.0943783308903221e-05 \n",
            " Epoch 50\n",
            "[0/4096] batch loss: 1.0444921827001963e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 1.0640138824934204e-05\n",
            "Validation loss for fold 1: 1.0718767558140962e-05\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/4096] batch loss: 6.105237844167277e-05 \n",
            " Epoch 2\n",
            "[0/4096] batch loss: 5.4379030189011246e-05 \n",
            " Epoch 3\n",
            "[0/4096] batch loss: 3.219517020625062e-05 \n",
            " Epoch 4\n",
            "[0/4096] batch loss: 1.5431756764883175e-05 \n",
            " Epoch 5\n",
            "[0/4096] batch loss: 1.4850426850898657e-05 \n",
            " Epoch 6\n",
            "[0/4096] batch loss: 1.4708482012792956e-05 \n",
            " Epoch 7\n",
            "[0/4096] batch loss: 1.4381696928467136e-05 \n",
            " Epoch 8\n",
            "[0/4096] batch loss: 1.4073533748160116e-05 \n",
            " Epoch 9\n",
            "[0/4096] batch loss: 1.4356179235619493e-05 \n",
            " Epoch 10\n",
            "[0/4096] batch loss: 1.4030991223989986e-05 \n",
            " Epoch 11\n",
            "[0/4096] batch loss: 1.4143499356578104e-05 \n",
            " Epoch 12\n",
            "[0/4096] batch loss: 1.3916664101998322e-05 \n",
            " Epoch 13\n",
            "[0/4096] batch loss: 1.3937878975411877e-05 \n",
            " Epoch 14\n",
            "[0/4096] batch loss: 1.3905058040108997e-05 \n",
            " Epoch 15\n",
            "[0/4096] batch loss: 1.3748609489994124e-05 \n",
            " Epoch 16\n",
            "[0/4096] batch loss: 1.3645432773046196e-05 \n",
            " Epoch 17\n",
            "[0/4096] batch loss: 1.3441904229694046e-05 \n",
            " Epoch 18\n",
            "[0/4096] batch loss: 1.3173247680242639e-05 \n",
            " Epoch 19\n",
            "[0/4096] batch loss: 1.3138609574525617e-05 \n",
            " Epoch 20\n",
            "[0/4096] batch loss: 1.2898228305857629e-05 \n",
            " Epoch 21\n",
            "[0/4096] batch loss: 1.2971887372259516e-05 \n",
            " Epoch 22\n",
            "[0/4096] batch loss: 1.2828596481995191e-05 \n",
            " Epoch 23\n",
            "[0/4096] batch loss: 1.2987305126443971e-05 \n",
            " Epoch 24\n",
            "[0/4096] batch loss: 1.2947240975336172e-05 \n",
            " Epoch 25\n",
            "[0/4096] batch loss: 1.2819953553844243e-05 \n",
            " Epoch 26\n",
            "[0/4096] batch loss: 1.2886775948572904e-05 \n",
            " Epoch 27\n",
            "[0/4096] batch loss: 1.2995500583201647e-05 \n",
            " Epoch 28\n",
            "[0/4096] batch loss: 1.2733396033581812e-05 \n",
            " Epoch 29\n",
            "[0/4096] batch loss: 1.2451289876480587e-05 \n",
            " Epoch 30\n",
            "[0/4096] batch loss: 1.2563393283926416e-05 \n",
            " Epoch 31\n",
            "[0/4096] batch loss: 1.2369020623737015e-05 \n",
            " Epoch 32\n",
            "[0/4096] batch loss: 1.2592114217113703e-05 \n",
            " Epoch 33\n",
            "[0/4096] batch loss: 1.2540172974695452e-05 \n",
            " Epoch 34\n",
            "[0/4096] batch loss: 1.249138676939765e-05 \n",
            " Epoch 35\n",
            "[0/4096] batch loss: 1.2590356163855176e-05 \n",
            " Epoch 36\n",
            "[0/4096] batch loss: 1.243304996023653e-05 \n",
            " Epoch 37\n",
            "[0/4096] batch loss: 1.2229657841089647e-05 \n",
            " Epoch 38\n",
            "[0/4096] batch loss: 1.2204515769553836e-05 \n",
            " Epoch 39\n",
            "[0/4096] batch loss: 1.2034352948830929e-05 \n",
            " Epoch 40\n",
            "[0/4096] batch loss: 1.1734850886568893e-05 \n",
            " Epoch 41\n",
            "[0/4096] batch loss: 1.1674646884785034e-05 \n",
            " Epoch 42\n",
            "[0/4096] batch loss: 1.156534199253656e-05 \n",
            " Epoch 43\n",
            "[0/4096] batch loss: 1.1279072168690618e-05 \n",
            " Epoch 44\n",
            "[0/4096] batch loss: 1.0939561434497591e-05 \n",
            " Epoch 45\n",
            "[0/4096] batch loss: 1.077364413504256e-05 \n",
            " Epoch 46\n",
            "[0/4096] batch loss: 1.0546094017627183e-05 \n",
            " Epoch 47\n",
            "[0/4096] batch loss: 1.046877423505066e-05 \n",
            " Epoch 48\n",
            "[0/4096] batch loss: 1.0399214261269663e-05 \n",
            " Epoch 49\n",
            "[0/4096] batch loss: 1.0200776159763336e-05 \n",
            " Epoch 50\n",
            "[0/4096] batch loss: 1.0064862181025092e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 1.0269492519408118e-05\n",
            "Validation loss for fold 2: 1.0215826212903262e-05\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/4096] batch loss: 6.145153020042926e-05 \n",
            " Epoch 2\n",
            "[0/4096] batch loss: 5.4596486734226346e-05 \n",
            " Epoch 3\n",
            "[0/4096] batch loss: 2.99030161841074e-05 \n",
            " Epoch 4\n",
            "[0/4096] batch loss: 1.549565240566153e-05 \n",
            " Epoch 5\n",
            "[0/4096] batch loss: 1.4706602996739093e-05 \n",
            " Epoch 6\n",
            "[0/4096] batch loss: 1.449023238819791e-05 \n",
            " Epoch 7\n",
            "[0/4096] batch loss: 1.4481187463388778e-05 \n",
            " Epoch 8\n",
            "[0/4096] batch loss: 1.4147001820674632e-05 \n",
            " Epoch 9\n",
            "[0/4096] batch loss: 1.4141955944069196e-05 \n",
            " Epoch 10\n",
            "[0/4096] batch loss: 1.4066940821066964e-05 \n",
            " Epoch 11\n",
            "[0/4096] batch loss: 1.4104311048868112e-05 \n",
            " Epoch 12\n",
            "[0/4096] batch loss: 1.3814324120176025e-05 \n",
            " Epoch 13\n",
            "[0/4096] batch loss: 1.3828645023750141e-05 \n",
            " Epoch 14\n",
            "[0/4096] batch loss: 1.3902787941333372e-05 \n",
            " Epoch 15\n",
            "[0/4096] batch loss: 1.3865171240468044e-05 \n",
            " Epoch 16\n",
            "[0/4096] batch loss: 1.3634720744448714e-05 \n",
            " Epoch 17\n",
            "[0/4096] batch loss: 1.3258186299935915e-05 \n",
            " Epoch 18\n",
            "[0/4096] batch loss: 1.3319695426616818e-05 \n",
            " Epoch 19\n",
            "[0/4096] batch loss: 1.2882596820418257e-05 \n",
            " Epoch 20\n",
            "[0/4096] batch loss: 1.3015818694839254e-05 \n",
            " Epoch 21\n",
            "[0/4096] batch loss: 1.2743693332595285e-05 \n",
            " Epoch 22\n",
            "[0/4096] batch loss: 1.2942369721713476e-05 \n",
            " Epoch 23\n",
            "[0/4096] batch loss: 1.3033260074735153e-05 \n",
            " Epoch 24\n",
            "[0/4096] batch loss: 1.2769521163136233e-05 \n",
            " Epoch 25\n",
            "[0/4096] batch loss: 1.2932467143400572e-05 \n",
            " Epoch 26\n",
            "[0/4096] batch loss: 1.291059743380174e-05 \n",
            " Epoch 27\n",
            "[0/4096] batch loss: 1.2834010703954846e-05 \n",
            " Epoch 28\n",
            "[0/4096] batch loss: 1.2586016964633018e-05 \n",
            " Epoch 29\n",
            "[0/4096] batch loss: 1.2846107892983127e-05 \n",
            " Epoch 30\n",
            "[0/4096] batch loss: 1.2562972187879495e-05 \n",
            " Epoch 31\n",
            "[0/4096] batch loss: 1.2551349755085539e-05 \n",
            " Epoch 32\n",
            "[0/4096] batch loss: 1.2445603715605102e-05 \n",
            " Epoch 33\n",
            "[0/4096] batch loss: 1.2315275853325147e-05 \n",
            " Epoch 34\n",
            "[0/4096] batch loss: 1.2517669347289484e-05 \n",
            " Epoch 35\n",
            "[0/4096] batch loss: 1.23429526865948e-05 \n",
            " Epoch 36\n",
            "[0/4096] batch loss: 1.2237376722623594e-05 \n",
            " Epoch 37\n",
            "[0/4096] batch loss: 1.2043810784234665e-05 \n",
            " Epoch 38\n",
            "[0/4096] batch loss: 1.2274541404622141e-05 \n",
            " Epoch 39\n",
            "[0/4096] batch loss: 1.1900938261533156e-05 \n",
            " Epoch 40\n",
            "[0/4096] batch loss: 1.194968444906408e-05 \n",
            " Epoch 41\n",
            "[0/4096] batch loss: 1.1718024325091392e-05 \n",
            " Epoch 42\n",
            "[0/4096] batch loss: 1.1481845831440296e-05 \n",
            " Epoch 43\n",
            "[0/4096] batch loss: 1.142559813160915e-05 \n",
            " Epoch 44\n",
            "[0/4096] batch loss: 1.1375275789760053e-05 \n",
            " Epoch 45\n",
            "[0/4096] batch loss: 1.1320095836708788e-05 \n",
            " Epoch 46\n",
            "[0/4096] batch loss: 1.1198505490028765e-05 \n",
            " Epoch 47\n",
            "[0/4096] batch loss: 1.0965473848045804e-05 \n",
            " Epoch 48\n",
            "[0/4096] batch loss: 1.09029897430446e-05 \n",
            " Epoch 49\n",
            "[0/4096] batch loss: 1.0815871064551175e-05 \n",
            " Epoch 50\n",
            "[0/4096] batch loss: 1.0789677617140114e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 1.080180675826938e-05\n",
            "Validation loss for fold 3: 1.0704772635716287e-05\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/4096] batch loss: 6.089874659664929e-05 \n",
            " Epoch 2\n",
            "[0/4096] batch loss: 5.2079376473557204e-05 \n",
            " Epoch 3\n",
            "[0/4096] batch loss: 2.3390042770188302e-05 \n",
            " Epoch 4\n",
            "[0/4096] batch loss: 1.5188762517936993e-05 \n",
            " Epoch 5\n",
            "[0/4096] batch loss: 1.4644804650743026e-05 \n",
            " Epoch 6\n",
            "[0/4096] batch loss: 1.4461517821473535e-05 \n",
            " Epoch 7\n",
            "[0/4096] batch loss: 1.442566735931905e-05 \n",
            " Epoch 8\n",
            "[0/4096] batch loss: 1.4367177755048033e-05 \n",
            " Epoch 9\n",
            "[0/4096] batch loss: 1.4286471923696809e-05 \n",
            " Epoch 10\n",
            "[0/4096] batch loss: 1.4188985915097874e-05 \n",
            " Epoch 11\n",
            "[0/4096] batch loss: 1.399285247316584e-05 \n",
            " Epoch 12\n",
            "[0/4096] batch loss: 1.3910561392549425e-05 \n",
            " Epoch 13\n",
            "[0/4096] batch loss: 1.3878162462788168e-05 \n",
            " Epoch 14\n",
            "[0/4096] batch loss: 1.3742474948230665e-05 \n",
            " Epoch 15\n",
            "[0/4096] batch loss: 1.3517471415980253e-05 \n",
            " Epoch 16\n",
            "[0/4096] batch loss: 1.3706261597690172e-05 \n",
            " Epoch 17\n",
            "[0/4096] batch loss: 1.3652444977196865e-05 \n",
            " Epoch 18\n",
            "[0/4096] batch loss: 1.3324543033377267e-05 \n",
            " Epoch 19\n",
            "[0/4096] batch loss: 1.318949944106862e-05 \n",
            " Epoch 20\n",
            "[0/4096] batch loss: 1.3027981367486063e-05 \n",
            " Epoch 21\n",
            "[0/4096] batch loss: 1.3470308658725116e-05 \n",
            " Epoch 22\n",
            "[0/4096] batch loss: 1.2959699233761057e-05 \n",
            " Epoch 23\n",
            "[0/4096] batch loss: 1.2984253771719523e-05 \n",
            " Epoch 24\n",
            "[0/4096] batch loss: 1.2996853911317885e-05 \n",
            " Epoch 25\n",
            "[0/4096] batch loss: 1.2849619452026673e-05 \n",
            " Epoch 26\n",
            "[0/4096] batch loss: 1.2875761967734434e-05 \n",
            " Epoch 27\n",
            "[0/4096] batch loss: 1.2881047041446436e-05 \n",
            " Epoch 28\n",
            "[0/4096] batch loss: 1.2907536984130275e-05 \n",
            " Epoch 29\n",
            "[0/4096] batch loss: 1.2493805115809664e-05 \n",
            " Epoch 30\n",
            "[0/4096] batch loss: 1.2608077668119222e-05 \n",
            " Epoch 31\n",
            "[0/4096] batch loss: 1.2490245353546925e-05 \n",
            " Epoch 32\n",
            "[0/4096] batch loss: 1.2496912859205622e-05 \n",
            " Epoch 33\n",
            "[0/4096] batch loss: 1.2478437383833807e-05 \n",
            " Epoch 34\n",
            "[0/4096] batch loss: 1.2318169865466189e-05 \n",
            " Epoch 35\n",
            "[0/4096] batch loss: 1.2258599781489465e-05 \n",
            " Epoch 36\n",
            "[0/4096] batch loss: 1.2005592907371465e-05 \n",
            " Epoch 37\n",
            "[0/4096] batch loss: 1.1811183867393993e-05 \n",
            " Epoch 38\n",
            "[0/4096] batch loss: 1.1805134818132501e-05 \n",
            " Epoch 39\n",
            "[0/4096] batch loss: 1.1772751349781174e-05 \n",
            " Epoch 40\n",
            "[0/4096] batch loss: 1.168729522760259e-05 \n",
            " Epoch 41\n",
            "[0/4096] batch loss: 1.1506780538184103e-05 \n",
            " Epoch 42\n",
            "[0/4096] batch loss: 1.1565236491151154e-05 \n",
            " Epoch 43\n",
            "[0/4096] batch loss: 1.1265197827015072e-05 \n",
            " Epoch 44\n",
            "[0/4096] batch loss: 1.1223503861401696e-05 \n",
            " Epoch 45\n",
            "[0/4096] batch loss: 1.1322013961034827e-05 \n",
            " Epoch 46\n",
            "[0/4096] batch loss: 1.1256046491325833e-05 \n",
            " Epoch 47\n",
            "[0/4096] batch loss: 1.129784868680872e-05 \n",
            " Epoch 48\n",
            "[0/4096] batch loss: 1.117594638344599e-05 \n",
            " Epoch 49\n",
            "[0/4096] batch loss: 1.07446294350666e-05 \n",
            " Epoch 50\n",
            "[0/4096] batch loss: 1.06457255242276e-05 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 1.0688859722681743e-05\n",
            "Validation loss for fold 4: 1.0490462590866531e-05\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 1.1603028157502989e-05\n",
            "Training Fold 1 Loss: 1.0640138824934204e-05\n",
            "Training Fold 2 Loss: 1.0269492519408118e-05\n",
            "Training Fold 3 Loss: 1.080180675826938e-05\n",
            "Training Fold 4 Loss: 1.0688859722681743e-05\n",
            "Trainig Average Loss: 1.0800665196559287e-05\n",
            "Validation Fold 0 Loss: 1.1522501011174021e-05 \n",
            "Validation Fold 1 Loss: 1.0718767558140962e-05 \n",
            "Validation Fold 2 Loss: 1.0215826212903262e-05 \n",
            "Validation Fold 3 Loss: 1.0704772635716287e-05 \n",
            "Validation Fold 4 Loss: 1.0490462590866531e-05 \n",
            "Vaidation Average Loss: 1.0730466001760214e-05\n",
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.052194369956851e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9100912797730416e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.654596210049931e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.1383631974458694e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.3062148354947567e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.222125870815944e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.591412213514559e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.445098617608892e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.4698086791613605e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.306863608391723e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.2361981438007206e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.270678906934336e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.234946679091081e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.179910880950047e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.1949943958316e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.129930509108817e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.203789664345095e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.141996775317239e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.1227036642085295e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.026238108664984e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.060218194965273e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 6.991272584855324e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.007945441728225e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.022968929959461e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 6.918157396285096e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 6.8938561526010744e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 6.877368832647335e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 6.9004727265564725e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 6.7951527853438165e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 6.866052899567876e-06 \n",
            " Epoch 31\n",
            "[0/8192] batch loss: 6.805964858358493e-06 \n",
            " Epoch 32\n",
            "[0/8192] batch loss: 6.721259524056222e-06 \n",
            " Epoch 33\n",
            "[0/8192] batch loss: 6.733766895195004e-06 \n",
            " Epoch 34\n",
            "[0/8192] batch loss: 6.641494110226631e-06 \n",
            " Epoch 35\n",
            "[0/8192] batch loss: 6.614396170334658e-06 \n",
            " Epoch 36\n",
            "[0/8192] batch loss: 6.561921509273816e-06 \n",
            " Epoch 37\n",
            "[0/8192] batch loss: 6.5630729295662604e-06 \n",
            " Epoch 38\n",
            "[0/8192] batch loss: 6.412511083908612e-06 \n",
            " Epoch 39\n",
            "[0/8192] batch loss: 6.589621989405714e-06 \n",
            " Epoch 40\n",
            "[0/8192] batch loss: 6.540218691952759e-06 \n",
            " Epoch 41\n",
            "[0/8192] batch loss: 6.527866844407981e-06 \n",
            " Epoch 42\n",
            "[0/8192] batch loss: 6.402397048077546e-06 \n",
            " Epoch 43\n",
            "[0/8192] batch loss: 6.384253993019229e-06 \n",
            " Epoch 44\n",
            "[0/8192] batch loss: 6.440860943257576e-06 \n",
            " Epoch 45\n",
            "[0/8192] batch loss: 6.419992132578045e-06 \n",
            " Epoch 46\n",
            "[0/8192] batch loss: 6.484353434643708e-06 \n",
            " Epoch 47\n",
            "[0/8192] batch loss: 6.415486950572813e-06 \n",
            " Epoch 48\n",
            "[0/8192] batch loss: 6.396921889972873e-06 \n",
            " Epoch 49\n",
            "[0/8192] batch loss: 6.429861514334334e-06 \n",
            " Epoch 50\n",
            "[0/8192] batch loss: 6.3766970015421975e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 6.494278383070898e-06\n",
            "Validation loss for fold 0: 6.4778504126861945e-06\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.055780689464882e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.928835601778701e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.7130079615744762e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.265202783746645e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.4654357983090449e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.59203555592103e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.670499144296627e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.517833182646427e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.3778383011813276e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.311419722100254e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.3058067755482625e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.193068540800596e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.2114416980184615e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.221532996481983e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.088806341926102e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.197325430752244e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.143616585381096e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.079994247760624e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.146169537008973e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.139035460568266e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.095214186847443e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.081183866830543e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.090461622283328e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.04290823705378e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 7.1343233685183804e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.05339380147052e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 6.959994607314002e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 6.936465979379136e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 6.938335900485981e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 6.858731921965955e-06 \n",
            " Epoch 31\n",
            "[0/8192] batch loss: 6.810671948187519e-06 \n",
            " Epoch 32\n",
            "[0/8192] batch loss: 6.75189403409604e-06 \n",
            " Epoch 33\n",
            "[0/8192] batch loss: 6.700908215862e-06 \n",
            " Epoch 34\n",
            "[0/8192] batch loss: 6.66424966766499e-06 \n",
            " Epoch 35\n",
            "[0/8192] batch loss: 6.606128863495542e-06 \n",
            " Epoch 36\n",
            "[0/8192] batch loss: 6.643372216785792e-06 \n",
            " Epoch 37\n",
            "[0/8192] batch loss: 6.54156292512198e-06 \n",
            " Epoch 38\n",
            "[0/8192] batch loss: 6.596121238544583e-06 \n",
            " Epoch 39\n",
            "[0/8192] batch loss: 6.5282906689390074e-06 \n",
            " Epoch 40\n",
            "[0/8192] batch loss: 6.5388162511226255e-06 \n",
            " Epoch 41\n",
            "[0/8192] batch loss: 6.516542271128856e-06 \n",
            " Epoch 42\n",
            "[0/8192] batch loss: 6.490127361757914e-06 \n",
            " Epoch 43\n",
            "[0/8192] batch loss: 6.545594260387588e-06 \n",
            " Epoch 44\n",
            "[0/8192] batch loss: 6.5364533838874195e-06 \n",
            " Epoch 45\n",
            "[0/8192] batch loss: 6.520357601402793e-06 \n",
            " Epoch 46\n",
            "[0/8192] batch loss: 6.504713383037597e-06 \n",
            " Epoch 47\n",
            "[0/8192] batch loss: 6.496223249996547e-06 \n",
            " Epoch 48\n",
            "[0/8192] batch loss: 6.5214971982641146e-06 \n",
            " Epoch 49\n",
            "[0/8192] batch loss: 6.530989139719168e-06 \n",
            " Epoch 50\n",
            "[0/8192] batch loss: 6.524525815621018e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 6.54248461760149e-06\n",
            "Validation loss for fold 1: 6.477994011719197e-06\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.0526403861586004e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9215505492174998e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.720105658227112e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.3271688405657187e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.6054804291343316e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 9.313836017099675e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.797411853971425e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.523841304646339e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.40906534701935e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.400202321150573e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.230180017359089e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.17739885658375e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.2546426963526756e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.178757641668199e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.148119948396925e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.146606549213175e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.085356628522277e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.0744358708907384e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.071742857078789e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.0705141297366936e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.128929610189516e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.119442216207972e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.055608421069337e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.055504738673335e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 6.988351287873229e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 7.013008598732995e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 6.922542524989694e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 6.879087322886335e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 6.870370270917192e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 6.920304713275982e-06 \n",
            " Epoch 31\n",
            "[0/8192] batch loss: 6.799631592002697e-06 \n",
            " Epoch 32\n",
            "[0/8192] batch loss: 6.770265827071853e-06 \n",
            " Epoch 33\n",
            "[0/8192] batch loss: 6.7678884079214185e-06 \n",
            " Epoch 34\n",
            "[0/8192] batch loss: 6.739005584677216e-06 \n",
            " Epoch 35\n",
            "[0/8192] batch loss: 6.663859039690578e-06 \n",
            " Epoch 36\n",
            "[0/8192] batch loss: 6.645310804742621e-06 \n",
            " Epoch 37\n",
            "[0/8192] batch loss: 6.512007530545816e-06 \n",
            " Epoch 38\n",
            "[0/8192] batch loss: 6.511427727673436e-06 \n",
            " Epoch 39\n",
            "[0/8192] batch loss: 6.51954542263411e-06 \n",
            " Epoch 40\n",
            "[0/8192] batch loss: 6.547253178723622e-06 \n",
            " Epoch 41\n",
            "[0/8192] batch loss: 6.453011792473262e-06 \n",
            " Epoch 42\n",
            "[0/8192] batch loss: 6.48400055069942e-06 \n",
            " Epoch 43\n",
            "[0/8192] batch loss: 6.456443770730402e-06 \n",
            " Epoch 44\n",
            "[0/8192] batch loss: 6.499937626358587e-06 \n",
            " Epoch 45\n",
            "[0/8192] batch loss: 6.450706678151619e-06 \n",
            " Epoch 46\n",
            "[0/8192] batch loss: 6.47583874524571e-06 \n",
            " Epoch 47\n",
            "[0/8192] batch loss: 6.475727332144743e-06 \n",
            " Epoch 48\n",
            "[0/8192] batch loss: 6.513825155707309e-06 \n",
            " Epoch 49\n",
            "[0/8192] batch loss: 6.410190962924389e-06 \n",
            " Epoch 50\n",
            "[0/8192] batch loss: 6.379566457326291e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 2: 6.516834483643757e-06\n",
            "Validation loss for fold 2: 6.4841218243005805e-06\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.072688923566602e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.9467055355780758e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.7299351131659932e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.285540904267691e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.495070773671614e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 8.68376355356304e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.649127837794367e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.441240995831322e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.438855845975922e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.228122740343679e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.299079697986599e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.222469776024809e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.141005426092306e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.1149934228742495e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.165695024013985e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.105032636900432e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.101905339368386e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.06777291270555e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.01379076417652e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.030736924207304e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 6.977403700147988e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 6.982038030400872e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 7.050528893159935e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 7.028674190223683e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 6.933645181561587e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 6.950997885724064e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 6.984187166381162e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 6.91108016326325e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 6.840840342192678e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 6.877665327920113e-06 \n",
            " Epoch 31\n",
            "[0/8192] batch loss: 6.768947969248984e-06 \n",
            " Epoch 32\n",
            "[0/8192] batch loss: 6.714194114465499e-06 \n",
            " Epoch 33\n",
            "[0/8192] batch loss: 6.627582934015663e-06 \n",
            " Epoch 34\n",
            "[0/8192] batch loss: 6.618791303480975e-06 \n",
            " Epoch 35\n",
            "[0/8192] batch loss: 6.53125698590884e-06 \n",
            " Epoch 36\n",
            "[0/8192] batch loss: 6.515941549878335e-06 \n",
            " Epoch 37\n",
            "[0/8192] batch loss: 6.435619980038609e-06 \n",
            " Epoch 38\n",
            "[0/8192] batch loss: 6.481540822278475e-06 \n",
            " Epoch 39\n",
            "[0/8192] batch loss: 6.518274403788382e-06 \n",
            " Epoch 40\n",
            "[0/8192] batch loss: 6.460713848355226e-06 \n",
            " Epoch 41\n",
            "[0/8192] batch loss: 6.463519639510196e-06 \n",
            " Epoch 42\n",
            "[0/8192] batch loss: 6.388107522070641e-06 \n",
            " Epoch 43\n",
            "[0/8192] batch loss: 6.413003575289622e-06 \n",
            " Epoch 44\n",
            "[0/8192] batch loss: 6.425080755434465e-06 \n",
            " Epoch 45\n",
            "[0/8192] batch loss: 6.445646249630954e-06 \n",
            " Epoch 46\n",
            "[0/8192] batch loss: 6.330233645712724e-06 \n",
            " Epoch 47\n",
            "[0/8192] batch loss: 6.343558197841048e-06 \n",
            " Epoch 48\n",
            "[0/8192] batch loss: 6.461540124291787e-06 \n",
            " Epoch 49\n",
            "[0/8192] batch loss: 6.423602371796733e-06 \n",
            " Epoch 50\n",
            "[0/8192] batch loss: 6.340659638226498e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 3: 6.47024079639479e-06\n",
            "Validation loss for fold 3: 6.4604022096698005e-06\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=15, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=15, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=196, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=196, out_features=204, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[0/8192] batch loss: 3.0449004043475725e-05 \n",
            " Epoch 2\n",
            "[0/8192] batch loss: 2.8879347155452706e-05 \n",
            " Epoch 3\n",
            "[0/8192] batch loss: 2.6038644136860967e-05 \n",
            " Epoch 4\n",
            "[0/8192] batch loss: 2.0194895114400424e-05 \n",
            " Epoch 5\n",
            "[0/8192] batch loss: 1.1656436981866136e-05 \n",
            " Epoch 6\n",
            "[0/8192] batch loss: 7.94049719843315e-06 \n",
            " Epoch 7\n",
            "[0/8192] batch loss: 7.5701777859649155e-06 \n",
            " Epoch 8\n",
            "[0/8192] batch loss: 7.40040104574291e-06 \n",
            " Epoch 9\n",
            "[0/8192] batch loss: 7.350487521762261e-06 \n",
            " Epoch 10\n",
            "[0/8192] batch loss: 7.3170658652088605e-06 \n",
            " Epoch 11\n",
            "[0/8192] batch loss: 7.214840024971636e-06 \n",
            " Epoch 12\n",
            "[0/8192] batch loss: 7.1821668825577945e-06 \n",
            " Epoch 13\n",
            "[0/8192] batch loss: 7.106499197107041e-06 \n",
            " Epoch 14\n",
            "[0/8192] batch loss: 7.121627277228981e-06 \n",
            " Epoch 15\n",
            "[0/8192] batch loss: 7.060643383738352e-06 \n",
            " Epoch 16\n",
            "[0/8192] batch loss: 7.136162366805365e-06 \n",
            " Epoch 17\n",
            "[0/8192] batch loss: 7.137591182981851e-06 \n",
            " Epoch 18\n",
            "[0/8192] batch loss: 7.0554183366766665e-06 \n",
            " Epoch 19\n",
            "[0/8192] batch loss: 7.073996584949782e-06 \n",
            " Epoch 20\n",
            "[0/8192] batch loss: 7.028563686617417e-06 \n",
            " Epoch 21\n",
            "[0/8192] batch loss: 7.023049874987919e-06 \n",
            " Epoch 22\n",
            "[0/8192] batch loss: 7.107780675141839e-06 \n",
            " Epoch 23\n",
            "[0/8192] batch loss: 6.918060535099357e-06 \n",
            " Epoch 24\n",
            "[0/8192] batch loss: 6.971037237235578e-06 \n",
            " Epoch 25\n",
            "[0/8192] batch loss: 6.960313839954324e-06 \n",
            " Epoch 26\n",
            "[0/8192] batch loss: 6.940114872122649e-06 \n",
            " Epoch 27\n",
            "[0/8192] batch loss: 6.907075203343993e-06 \n",
            " Epoch 28\n",
            "[0/8192] batch loss: 6.902678705955623e-06 \n",
            " Epoch 29\n",
            "[0/8192] batch loss: 6.879283318994567e-06 \n",
            " Epoch 30\n",
            "[0/8192] batch loss: 6.90974138706224e-06 \n",
            " Epoch 31\n",
            "[0/8192] batch loss: 6.793709417252103e-06 \n",
            " Epoch 32\n",
            "[0/8192] batch loss: 6.709308308927575e-06 \n",
            " Epoch 33\n",
            "[0/8192] batch loss: 6.755631147825625e-06 \n",
            " Epoch 34\n",
            "[0/8192] batch loss: 6.748784926458029e-06 \n",
            " Epoch 35\n",
            "[0/8192] batch loss: 6.666861281701131e-06 \n",
            " Epoch 36\n",
            "[0/8192] batch loss: 6.628754817938898e-06 \n",
            " Epoch 37\n",
            "[0/8192] batch loss: 6.635264071519487e-06 \n",
            " Epoch 38\n",
            "[0/8192] batch loss: 6.617125563934678e-06 \n",
            " Epoch 39\n",
            "[0/8192] batch loss: 6.533979558298597e-06 \n",
            " Epoch 40\n",
            "[0/8192] batch loss: 6.519580438180128e-06 \n",
            " Epoch 41\n",
            "[0/8192] batch loss: 6.5200788412767e-06 \n",
            " Epoch 42\n",
            "[0/8192] batch loss: 6.522801868413808e-06 \n",
            " Epoch 43\n",
            "[0/8192] batch loss: 6.501841198769398e-06 \n",
            " Epoch 44\n",
            "[0/8192] batch loss: 6.3858892644930165e-06 \n",
            " Epoch 45\n",
            "[0/8192] batch loss: 6.373103133228142e-06 \n",
            " Epoch 46\n",
            "[0/8192] batch loss: 6.388042038452113e-06 \n",
            " Epoch 47\n",
            "[0/8192] batch loss: 6.437793672375847e-06 \n",
            " Epoch 48\n",
            "[0/8192] batch loss: 6.391586339304922e-06 \n",
            " Epoch 49\n",
            "[0/8192] batch loss: 6.33541276329197e-06 \n",
            " Epoch 50\n",
            "[0/8192] batch loss: 6.345498604787281e-06 Training process has finished. Saving trained model.\n",
            "Training loss for fold 4: 6.4463795569831424e-06\n",
            "Validation loss for fold 4: 6.560561194579533e-06\n",
            "--------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "--------------------------------\n",
            "Training Fold 0 Loss: 6.494278383070898e-06\n",
            "Training Fold 1 Loss: 6.54248461760149e-06\n",
            "Training Fold 2 Loss: 6.516834483643757e-06\n",
            "Training Fold 3 Loss: 6.47024079639479e-06\n",
            "Training Fold 4 Loss: 6.4463795569831424e-06\n",
            "Trainig Average Loss: 6.494043567538816e-06\n",
            "Validation Fold 0 Loss: 6.4778504126861945e-06 \n",
            "Validation Fold 1 Loss: 6.477994011719197e-06 \n",
            "Validation Fold 2 Loss: 6.4841218243005805e-06 \n",
            "Validation Fold 3 Loss: 6.4604022096698005e-06 \n",
            "Validation Fold 4 Loss: 6.560561194579533e-06 \n",
            "Vaidation Average Loss: 6.492185930591062e-06\n"
          ]
        }
      ],
      "source": [
        "AE_trained_models = []\n",
        "for batch_size in batch_sizes:\n",
        "  globals()['AE' '{0}' '_full_outputs'.format(batch_size)]= train_AE_model(net=AEnet15,\n",
        "                                          dataset=dataset,\n",
        "                                          k=k, \n",
        "                                          epochs=epochs,\n",
        "                                          batch_size=batch_size)\n",
        "  AE_trained_models.append(globals()['AE' '{0}' '_full_outputs'.format(batch_size)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8y05zsF3Fk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYuwn3FID6ce"
      },
      "outputs": [],
      "source": [
        "AE_avg_train_loss = []\n",
        "AE_avg_val_loss = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_iJBBCrD1oB"
      },
      "outputs": [],
      "source": [
        "for model in AE_trained_models:\n",
        "  AE_avg_train_loss.append(model[0])\n",
        "  AE_avg_val_loss.append(model[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "NLqoXK9vEKum",
        "outputId": "2e0a85eb-bde6-4988-f69f-e8adbcc20bb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                               64.0     128.0     512.0    1024.0    4096.0  \\\n",
              "Loss                                                                           \n",
              "AE Average training loss    0.000275  0.000149  0.000049  0.000028  0.000011   \n",
              "AE Average validation loss  0.000307  0.000162  0.000050  0.000028  0.000011   \n",
              "\n",
              "0                             8192.0  \n",
              "Loss                                  \n",
              "AE Average training loss    0.000006  \n",
              "AE Average validation loss  0.000006  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d8dde50-8549-421e-8c31-bfdb49e6cacd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>64.0</th>\n",
              "      <th>128.0</th>\n",
              "      <th>512.0</th>\n",
              "      <th>1024.0</th>\n",
              "      <th>4096.0</th>\n",
              "      <th>8192.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Loss</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AE Average training loss</th>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AE Average validation loss</th>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d8dde50-8549-421e-8c31-bfdb49e6cacd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d8dde50-8549-421e-8c31-bfdb49e6cacd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d8dde50-8549-421e-8c31-bfdb49e6cacd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df = pd.DataFrame(data=[batch_sizes,\n",
        "                        AE_avg_train_loss,\n",
        "                        AE_avg_val_loss])\n",
        "\n",
        "df.columns = df.iloc[0]\n",
        "df = df[1:]\n",
        "\n",
        "df['Loss'] = ['AE Average training loss',\n",
        "              'AE Average validation loss']\n",
        "df = df.set_index('Loss')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "l8DVomhgEwTs",
        "outputId": "bfbddd4d-eb7d-4f9e-9696-48c20d313501"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# transpose and plot\n",
        "ax = df.T.plot(figsize=(7, 6))\n",
        "ax.set_ylabel('MSE loss', fontsize=12)\n",
        "ax.set_xlabel('Latent Dimensions', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aps4xa3PddFz",
        "outputId": "930dee1c-2bf6-470b-a1c1-2ed71852bad3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD8CAYAAAC4uSVNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcVZn/8c93ZjK5hyTEhJDEJEIIoGiiMQsqIleRZQneuKgrLCDrBVfQVUH2J64uvxd4WXR/+kMRIqjIRS4aFQ0BQdxVLgFCSCAhISSQEBJIQoAEMsz0s39UJXYm09N1TtX0TDfP+/Wq13RX11PnTE/PmZpT5zlHZoZzzrn60tTbFXDOORfOG2/nnKtD3ng751wd8sbbOefqkDfezjlXh7zxds65OuSNt3POZSBptqT1khZVeF2S/kvSckkLJb217LVTJC1Lt1OKqI833s45l82VwNHdvP4+YEq6nQlcCiBpJHAB8HfATOACSSPyVsYbb+ecy8DM7gI2dnPILOCnlrgbGC5pLPBeYJ6ZbTSzTcA8uv8jkElL3hOEaJ1+WnA659bLPxhczqtPPRYcA9D6lkOCY2Sl4Ji2PfYLjgH4yrwngmPOPnhSVFl3r34hOOb414d/nFo2PhkcA/CJe5uDYy6duCKqrKYJ+4YHrVsZHPLpx8eFlwNcvOHa4Jg/H3J2VFm/Xrg2OObC9+0TVdYeuw1WVGCZrG3Oqwt+8s8kV8vbXWZmlwUWNw54quz56nRfpf251LTxds65vihtqEMb616Vq9tE0tGSlqYd9OcWVSnnnCuCmpozbQVZA0woez4+3Vdpfy7RjbekZuAHJJ30+wMnS9o/b4Wcc64oTS2tmbaCzAE+no46ORDYbGZrgbnAUZJGpDcqj0r35ZKn22QmsNzMVgBIupakw/6RvJVyzrkiFHhVjaRrgPcAoyStJhlB0g/AzH4I3AIcAywHtgL/lL62UdI3gPvSU33dzLq78ZlJnsa7q074v8tXHeecK46ai2u8zezkKq8b8JkKr80GZhdWGWowVFDSmZLmS5pfem5pTxfnnHM7NDU1Z9rqUZ7GO1MnvJldZmYzzGxG06ipOYpzzrkwNb5hWVN5uk3uA6ZImkzSaJ8EfKSQWjnnXAHqtWHOIrrxNrN2SWeR3DVtBmab2eLCauacczk1tfTr7Sr0mFxJOmZ2C8kdVuec63Ma+cpbtVyAuP3+3wUXNuiMG4PLuf+WS4JjAIb0C78FsPqFtuCYqbsPCI4B2P2pu8ODhuweVdbaYXsHx2xpD58qoIm4DOi9Xgq/+f0/TI4qa+Lw/sEx7aXw36tJL8Td0L+3+Q3BMTM74qYKIGI6iPbdJ0UV1Tp8dO70+N2PuTDTD2LDLefnLqvWPD3eOdewihwq2NfkTY/vdn5b55zrTY082iTvOO8rKWBqQ+ec6wnNLa2ZtnqU94blXZImFVMV55wrVr1eVWfhfd7OuYbVyI13TdPjf3zTH3q6OOec26GR+7x7/Mq7fJLzmKGCzjkXq14b5iy828Q517C88a6gq/ltzeyKIirmnHN5NfWrz5EkWeQdbdLt/LbOOdeb/Mq7IDGrusekur/tmHOCYwCWzvtecEy/5vCs2uG8HBwDoAGDg2Pa14S/5wBjBg0PjnncRgbHtLZEZiWX2oNDJo4MT3MHaOsIv1UzsCV8LEDHs6uDYwDWDx0fHGOlzVFlta1cEhzTb/rQqLIYPjourkwjN9551rCcIOkOSY9IWizpc0VWzDnn8mpqUqatHuUZKtgOfMHM9gcOBD7jCxA75/oSNSnTlulc0tGSlkpaLuncLl6/RNKCdHtM0vNlr3WUvTaniO8tz3zea4G16eMXJT1Ksq6lL0DsnOsTmpuLSWWR1Az8ADiSZL3e+yTNMbMd7Z2ZnVN2/GeB6WWneNnMphVSmVQh31maIj8duKeI8znnXBEKvPKeCSw3sxVm1gZcC8zq5viTgWsK+BYqyt14SxoC3AicbWYv5K+Sc84Vo8DGexzwVNnz1em+XcuUJgKTgT+W7R6QZprfLen42O+nXN4pYfuRNNxXm9lNFY7ZkR5/xa1/zVOcc84FaZIybeXtVLqdmaPYk4AbzKyjbN9EM5tBss7vdyXtlesbI0eftyQBVwCPmtl/VjquPD3+5V9d4unxzrmayXozsrydqmANMKHs+fh0X1dOAj7T6fxr0q8rJN1J0s38eKbKVZDnyvudwD8Ch5XdRT0mT2Wcc65IBXab3AdMkTRZUitJA73LqBFJ+wIjgL+W7RshqX/6eBRJ25l7YEee0Sb/DZELEDrnXA00xyaBdWJm7ZLOAuYCzcBsM1ss6evAfDPb3pCfBFxrOy8OvB/wI0klkgvmi8pHqcSqaYZl61sOCY6JWRQ4JlMSYOqR4XlGMWWteRWe2xqeITht6JjgmOa2V4JjANa0jAqOUcQCxK90lBjSLzwLrmPYHsExq57fFhwD8IYR4QtGb97WUf2gTkZPPCA4BmCyDQyOMV4fVVbMTCEdg0ZElVWEpHe3GGZ2C3BLp31f7fT8a13E/QWI++F2w2cV7AUxDXejimm4ncuqXrMns8hzw3IAcBfQPz3PDWZ2QVEVc865vLLesKxHea68twGHmdlL6ZDB/5b0ezO7u6C6OedcLt54dyHtkH8pfdov3XwooHOuz2gqsM+7r8m7GEMzcD+wN/ADM/P0eOdcn9EUMTVvvcj1nZlZRzrZynhgpqQ3dT5mpwWIr+kyCdM553pEI08JW8hoEzN7XtIdwNHAok6v7chc6njiAe9Wcc7VTJFDBfuaPIsxvE7S8PTxQJKpEsOX2XDOuR6ipmxbPcpz5T0WuCrt924Crjez3xZTLeecy69eu0SyyDPaZCE7TzbunHN9SlNBizH0RTXNsJSFp0+vfqEtOCZmUWCIS3WPSal/5Na49H1t2xocYy9viSpr/MZF1Q/qZO3u4RnAqzbHpe+PaQn/vrZ1hC/gDBAxQwP9Iq74rP9u4QUBTz8Tnva//4DwzxJA24rFwTFNk2ZElVUEv/J2zrk65Ek63Uj7vOcDa8zs2PxVcs65YjR7492tzwGPAsMKOJdzzhWmkRvvvMugjQf+Hri8mOo451xxmpuUaatHea+8vwt8CRhaQF2cc65QrZ4evytJxwLrzez+KsftSI+/zNPjnXM11NKkTFs9ynPl/U7guHTdygHAMEk/N7OPlR9Unh5fWjHf0+OdczVTr10iWURfeZvZeWY23swmkazb9sfODbdzzvWmRu7zbtwOIefca15zU1OmLQtJR0taKmm5pHO7eP1USc9KWpBuZ5S9doqkZel2ShHfW1GzCt4J3FnEuZxzrihFXVWn+Sw/IJmAbzVwn6Q5XawCf52ZndUpdiRwATCDZMGa+9PYTXnqVNMMy7Y99guOmfpqeDnDeTk8CHhoc/jCwDGp7vsfFZ5SD7D+z98Pjhn60saoshYPDv9Z9WsLXzF9SGvcR9D6hQ9w2ndQ+CrrABtfCf++Vj0fnva/2x5x6fvT94gI2hR3+6nloOODYzqaei+Ru8DRJjOB5Wa2AkDStcAsoHPj3ZX3AvPMbGMaO49k+uxr8lTIu02ccw2rWcq0lY+KS7czO51qHPBU2fPV6b7OPihpoaQbJE0IjA2Sdxm0lcCLQAfQbma9NwONc851krXbpHxUXA6/Aa4xs22S/hm4Cjgs5zkrKuL/mUPN7LkCzuOcc4UqcCTJGmBC2fPx6b4dzGxD2dPLgW+Wxb6nU+ydeSvk3SbOuYZVYJLOfcAUSZMltZIMj55TfoCksWVPjyOZ8wlgLnCUpBGSRgBHpfvyfW854w24VZIBP0r/9XDOuT6hqBuWZtYu6SySRrcZmG1miyV9HZhvZnOAf5F0HNAObAROTWM3SvoGyR8AgK9vv3mZR97G+11mtkbSaGCepCVmdlf5AWnH/5kA3/9//8Xpp52Ws0jnnMumyAQcM7sFuKXTvq+WPT4POK9C7GxgdmGVIWfjbWZr0q/rJd1MMpzmrk7H7LgR8MrWLZ4e75yrmXrNnswiz8RUgyUN3f6YpB8nfO0s55zrIY2cHp/nynsMcLOk7ef5hZn9oZBaOedcAeq1Yc4iz+rxK4C3FFgX55wrlDfeBfnKvCeCY76117rgGA2ISzOeNnRMeFkRK7rHpLkDjD74rOoHdfLeT30iqqxffKw1OGbgppXBMdr0dHAMwBcfD/9Z/Uf79VFlvW7E6OCYyaXwlPoHB8wKjgGYvuzXwTEX6Z1RZb1r0qDgmD2tFFXWlPCidtHIizH46vHOuYbVyFfeedewHJ7m8C+R9Kikg4qqmHPO5ZV1bpN6lPfK+3vAH8zsQ2nWUQH/6DjnXDGa6rRhziK68Za0G/Bu/pZF1Aa0FVMt55zLr7lx2+5c3SaTgWeBn0h6UNLl6Xhv55zrE5qalGmrR3ka7xbgrcClZjYd2AJ0tTTQjnlyF879ZY7inHMuTL+mpkxbPcpT69XAajO7J31+A0ljvhMzu8zMZpjZjDe/98M5inPOuTDNyrbVozyrxz8DPCVparrrcLItCeScczXRyN0meUebfBa4Oh1psgL4p/xVcs65YvhokwrMbAHJisjOOdfn1GuXSBY1zbA8++BJ4UEvhK/o3r7msfBygOa28BW/7eUtwTGxK7rHpLrPvfTHUWX1P+Gi4Bh7ellwTMeLzwfHABz7xjcHxzStGBFVVsvo8LViSxGfi/lPbw6OAZi6bGlwzMjph0aVtWxD+Pc1bY/eG4TWr7k+b0Zm4enxzrmG1cjdJnnm854qaUHZ9oKks4usnHPO5eGjTbpgZkvNbJqZTQPeBmwFbi6sZs45l1OTlGnLQtLRkpZKWi6pq5yWz0t6RNJCSbdLmlj2WkfZhe6czrExiuo2ORx43MxWFXQ+55zLrahZBSU1Az8AjiTJcblP0hwzKx8e/SAww8y2SvoU8E3gxPS1l9ML3cIU1Zt/EnBNQedyzrlCNCnblsFMYLmZrUjncboW2GkCdjO7w8y2T/B/NzC+yO+ls9yNdzrG+zigy9z38vT4X1xV6OLJzjnXrazp8eXtVLqd2elU44Cnyp6vTvdVcjrw+7LnA9Lz3i3p+CK+tyK6Td4HPGBmXS55U756/JMbX/LV451zNZN1pGB5O5WXpI+R5L8cUrZ7opmtkfQG4I+SHjazx/OUU0TjfTLeZeKc64MKHCq4BphQ9nx8um8nko4AzgcOMbNt2/eb2Zr06wpJdwLTgVyNd96VdAaTdODflOc8zjnXEwpcSec+YIqkyWlX8UnATqNGJE0HfgQcZ2bry/aPkNQ/fTwKeCcFzAOVNz1+C7B73ko451xPKOrK28zaJZ0FzAWagdlmtljS14H5ZjYH+BYwBPilknKfNLPjgP2AH0kqkVwwX9RplEoUmdWuG/r6hU8HF/aO8cOCyxnT/lxwDMDallHBMeM3LgqOWTx4v+AYgEm7ha/o3r/txaiyhhy6yzDWqv7wy4uDYw4aPzQ4BmDtS68Gx0ywTVFlbRkY/rmIUYr8Vdzt+fD/vjsGxk0VcN2T4TF/P2VkVFkjhw7K3fIufHpzpnf1zXvuVnepOp4e75xrWA2cHZ+7z/scSYslLZJ0jaQBRVXMOefyakKZtnqUZ26TccC/kGQUvYmkH+ikoirmnHN5Sdm2epS326QFGCjpVWAQ8HT+KjnnXDHqdJGcTPJMTLUG+DbwJLAW2GxmtxZVMeecy6uRr7zzdJuMIMntnwzsCQxOM4s6H7cj7fS2G34eX1PnnAtU4DjvPidPt8kRwBNm9iyApJuAdwA7tdDlaacxQwWdcy6Wd5t07UngQEmDlIxIPxx4tJhqOedcfsq41aPoK28zu0fSDcADQDvJXLaFTOzinHNFaORl0PKmx18AXFBQXZxzrlAN3HbXNj2+7fn1wYU90Rae9xP7LcX8oIf0aw6OebGtI7wgYO/2XSYxqypmRXeAP484KDjm6A9/OTjm4bnfDY4B2KvtqeoHdfLskElRZQ1pDe9dfG5re3DMnq3hKf8Am61/cMzIrXGjekv9w1eCt5a43L3+w0bmbnpXZ5yGevzIIXXXzOfNsPxcml252Bcfds71Nc1NyrTVozxDBd8EfIJkeaC3AMdK2ruoijnnXF4+zrtr+wH3mNlWM2sH/gR8oJhqOedcfk0Zt3qUp96LgIMl7S5pEHAMO6804ZxzvUpSpq0e5Rkq+Kiki4FbgS3AAiDuTpxzzvWAOu3OziTXfwxmdoWZvc3M3g1sAh7rfEx5evzlV/40T3HOORekWdm2epRrnLek0Wa2XtLrSfq7D+x8THl6fMxQQeeci1WvXSJZ5O2rv1HSI8BvgM+Y2fMF1Mk55wrRpGxbFpKOlrRU0nJJu6wTKKm/pOvS1++RNKnstfPS/UslvbeI7y1vhuXBRVTCOed6QlHX3ZKagR8ARwKrgfskzem0kPDpwCYz21vSScDFwImS9idZqOaNJDOw3iZpHzPLdY+wXkfJOOdcVU1Spi2DmcByM1thZm3AtSRTYpebBVyVPr4BODydtG8WcK2ZbTOzJ4Dl6flyqekCxC0bw5eebhoyNTimtSXu721LxK3pVZtfCY4Z0hr3tmtTeEpzx4txPVkHHRC+qntMqvsB741LzN06+4TgmD9tGB5V1qhBrcEx61/aFhxz4sgNwTEASzvGB8cctGmXsQWZlNaF/w732z98qgUAhsWtOl8ua5e3pDOBM8t2XZber9tuHFA+J8Nq4O86nWbHMWbWLmkzsHu6/+5OseOy1ayyqq2IpNnAscD6dK1KJI0ErgMmASuBE8xsU97KOOdckVTK1jNRPrCiXmTpNrkSOLrTvnOB281sCnB7+tw55/oUWSnTlsEadk5CHJ/u6/IYSS3AbsCGjLHBqjbeZnYXsLHT7vK+nauA4/NWxDnnCmelbFt19wFTJE2W1EpyA3JOp2PmAKekjz8E/NGSaVvnACelo1EmA1OAe/N+a7F93mPMbG36+BlgTN6KOOdc4Qqa8jrtwz4LmAs0A7PNbLGkrwPzzWwOcAXwM0nLSS54T0pjF0u6HniEZOGaz+QdaQIF3LA0M5PkyTfOub4n21V1tlOZ3QLc0mnfV8sevwJ8uELshcCFhVWG+KGC6ySNBUi/rq90YHl6/GXX3BRZnHPOhSuwz7vPib3y3t63c1H69deVDiy/i1taMd+v0J1ztVMKX9GoXmQZKngN8B5glKTVJGtWXgRcL+l0YBUQPujWOed6Wp1eVWdRtfE2s5MrvHR4wXVxzrlilV7DjXeRPnFv+GK9P565NLygyH+VOobtERwzpmVLcIz1C89eBPji4+GDeo5945ujypr0UvhiuDGLAsdkSgIMOu364JgXv1Px1ky3bE34z7i05cXgmOv2/khwDMD711wbHHPA7XHrpux/wFuDY44YGZfZ+qlJUWE7qdf+7Cxq2ng751xNNXDjXXW0iaTZktZLWlS278PpivElSTN6torOORep1JFtq0Ox6fGLSBZfuKvoCjnnXFFe00MFzeyu8knF032PQmOvUuGcawB12jBn4X3ezrnG1cCNd48vxlCeYbnk9ht7ujjnnPub4iam6nN6vPE2s8vMbIaZzdj38A/2dHHOObfDa7rP2znn6lZHfY4kySLLUMFrgL8CUyWtlnS6pPenqfIHAb+TNLenK+qcc8EauNskT3r8zQXXxTnnClWvXSJZ1LTb5NKJK4Jj/of3BMdMHNk/OAZg1fPhi8Zu6xgcHLPvoIHBMQD/0R6eEt60YkRUWc3TjwyOeXbIpOCY2EWBY1Ldh37hzqiyrrji68Ex/ZrDh9Ge2G9ZcAzAVXv8Q3DMg0f+Lqqsl1f9JThmyF6fjiqrEN54O+dcHWrgxjs2Pf5bkpZIWijpZklxl0/OOdeTPD1+l/T4ecCbzOzNwGPAeQXXyznncrP2VzNteUkaKWmepGXp1136KyVNk/TXdF6ohZJOLHvtSklPSFqQbtOqlRm1eryZ3Wpm2+ddvZtkKXvnnOtbanflfS5wu5lNAW5Pn3e2Ffi4mb2R5IL4u516Lb5oZtPSbUG1AotI0jkN+H0B53HOuUJZR0emrQCzgKvSx1cBx+9SF7PHzGxZ+vhpkrV/XxdbYK7GW9L5JEvZX93NMTvS4y//1bw8xTnnXJhSKdNW3k6l25mBJY0xs7Xp42eAbldOkTQTaAUeL9t9YdqdcomkqkPmokebSDoVOBY43MwqLixcvgBx219v9AWInXO1k7FLpLydqkTSbUBXy22d3+lcJqliWydpLPAz4BSzHcNhziNp9FvTenwZ6HaMalTjLelo4EvAIWa2NeYczjnX04q4GbnjXGZHVHpN0jpJY81sbdo4d5mIIGkY8DvgfDO7u+zc26/at0n6CfCv1eoTlR4PfB8YCsxL74z+sNp5nHOu1qzUkWkrwBzglPTxKcCvOx8gqZUkM/2nZnZDp9fGpl9F0l++qHN8Z7Hp8VdUi3POuV5XuzHcFwHXpxe3q4ATANJlIj9pZmek+94N7J52OwOcmo4suVrS6wABC4BPViuwphmWTRP2DY6ZOCg81b2tI65r/Q0jBgTH9Iu45bvxlbgP1OtGjA6OaRk9LqqsFweOCo4Z0hSeEj5qUGtwDMSt6B6T5g5w+ulfDY756ZXfCI6x4WODYwAmtYZPt6Dnwz/rAAPG7Rkc07R1U1RZ8PrIuDKl2mRYmtkG4PAu9s8Hzkgf/xz4eYX4w0LL9PR451zDKmgYYJ8Umx7/jXRIywJJt0oK/3PsnHM9zdPjd0mP/5aZvdnMpgG/BcL/r3TOuR5Wq/T43hC7evwLZU8HAz5+2znX99TpVXUWeZJ0LgQ+DmwGDi2sRs45V5QGbryj0+PN7Hwzm0CSGn9WpePK005/fPUvY4tzzrlgVipl2upREaNNrgZuAS7o6sXytNP21Yu9e8U5VzsNfOUdmx4/ZfvsWCSzaS0prkrOOVcMe7Wtt6vQY6o23ml6/HuAUemK8RcAx0iaCpRIsomqZgM551zN1WmXSBaeHu+ca1zebVKQdSuDQ9onhqfIDmyJuw+7eVv4D7pfREr4qudfCY4BmBzxQSy9HJ5GHuu5re3VD+pk/UvbosoqbXkxOCZmRXeIS3X/+Kn/JzjmA/Pi0vc3bwx/39ufXRNVlloipjOoPGN0jyto0qk+ydPjnXMNq15HkmQRlR5f9toXJJmk8FmMnHOuh1lHKdNWj2LT45E0ATgKeLLgOjnnXCFKr7Zn2upR1OrxqUtIVtPxsdvOuT6pka+8Y8d5zwLWmNlDycIPzjnX99Rrw5xF8LAMSYOAr5BxJsGd0uNv+kNocc45F63U0ZFpq0cxV957AZOB7Vfd44EHJM00s2c6H7xTevz9v/MuFudczTTyaJPgxtvMHgZ2rMclaSUww8yeK7BezjmXW626TSSNBK4DJgErgRPMbJf13yR1AA+nT580s+PS/ZOBa4HdgfuBfzSzbnP7Y1ePd865Pq+Go03OBW43synA7enzrrxsZtPS7biy/RcDl5jZ3sAmoGo7G5seX/76pGrncM653lCq3Q3LWSRzQAFcBdwJfDlLoJL+58OAj5TFfw24tLu4mmZYfvrx8JXMfzhiaXBMx7Org2MARk88IDjG+u8WHLPbHoODYwAeHDArOGb+05ujyvpIxN2JPVvDl5M6ceSG8IKA6/b+SPWDOpfVb1n1g7oQs6p7TKr7kCPjVhPcctnxwTF/mRk3l9zIQf2CY155Na4BfXtU1M6ydptIOhM4s2zXZen9uqzGmNna9PEzwJgKxw2QNB9oBy4ys1+RdJU8b2bb/wVYDVRtLD093jnXsLI23uUDKyqRdBuwRxcvnd/pXCap0uXPRDNbI+kNwB8lPUyyGlmwLFPCzgaOBdab2ZvSfV8DPgE8mx72FTO7JaYCzjnXU4ocbWJmR1R6TdI6SWPNbK2kscD6CudYk35dIelOYDpwIzBcUkt69T0eqDpzWHR6PEnn+vaOd2+4nXN9TqmtPdNWgDnAKenjU4Bfdz5A0ghJ/dPHo4B3Ao+YmQF3AB/qLr6zPOnxzjnXp5VKpUxbAS4CjpS0DDgifY6kGZIuT4/ZD5gv6SGSxvoiM3skfe3LwOclLSfpA6+6ZkKePu+zJH0cmA98oasxjc4515tqNc7bzDYAh3exfz5wRvr4L0CXoyLMbAUwM6TM2NXjLyXJtJwGrAW+U+nA8vT4R2+7MbI455wLZx0dmbZ6FNV4m9k6M+swsxLwY7r5i2Fml5nZDDObsd8RH4ytp3POBbNSKdNWj2JnFRxbNqbx/cAuCzU451xva+RZBWNXj3+PpGkkc3mvBP65B+vonHNROooZSdIn+erxzrmGVa9dIlnUNMPy4g3XBsfc2/yl4Jj1Q8cHxwBMtoHBMU8/E776+fSucrSyxC2rOvRzF1OXhU8vADBs/EeDYzYMe0NwzNKOuJ/V+9eEf5au2uMfosqa1Br+uYhZ0T0mzR1g8Jm/Co556f+HT2UAsObG8LKm/lvF8Qw97jXdbeKcc/XKOhp3CYHo1eMlfVbSEkmLJX2z56ronHNxSh2lTFs9ynLlfSXwfeCn23dIOpRkCsS3mNk2SaMrxDrnXK+xUuNeeWe5YXmXpEmddn+KJLVzW3pMl5OwOOdcb+poq88EnCxiMyz3AQ6WdI+kP0kqYupd55wrlHVYpq0exTbeLcBI4EDgi8D16WoQuyhPj7/yvxdEFuecc+FKHZZpq0exo01WAzelUxneK6kEjOJv83vvUD7J+aZLz63Pd8k5V5caeahg7JX3r4BDASTtA7QCvnq8c65PKZUs01aPYtPjZwOz0+GDbcAp6VW4c871GY18wzLP6vEfK7guzjlXqHq9GZlFTTMs/3zI2cExx3asCI6xUtyK6cbrg2P2H7A1vKBNcR+oi/TO4JiR0w+NKuv0geEfjZFbnw6OOWjTY8ExAAfcPiE45sEjfxdVlp4fEBzT/mzVJQh3Ebuie0yq+5BPx70X8264JDhmz379o8oqwmu68a6wAPF1wNT0kOEky9ZP67FaOudchHrNnswiagFiMztx++LDJCsf39QDdXPOuVysZJm2vCSNlDRP0rL064gujjlU0oKy7RVJx6evXSnpibLXql4M5xTglrYAAAxHSURBVFqAOB3bfQJwTdXvzjnnaqyG47zPBW43synA7enznZjZHWUXvYcBW4Fbyw754vbXzaxqUkzePu+DgXVmtizneZxzrnCl2o02mUUyKg/gKuBOkhXhK/kQ8Hszi7hplogd573dyfhVt3Ouj6rhlfeYsqUhnwHGVDn+JHZtOy+UtFDSJZKq3uWNbrwltQAfAK6rctyO9Pi51/8stjjnnAuWdQHi8nYq3c7sfC5Jt0la1MU2a6cyk5yXin8RJI0FDgDmlu0+D9gXeDvJ1CPdXbUD+bpNjgCWmNnq7g4qT4+f88gzjTtuxznX52S9qi5vp7o55ohKr0lat31h9rRx7m6m1ROAm81sxxjPsqv2bZJ+AvxrtTpnWYzhGuCvwFRJqyWdnr7U1WW/c871GTWcVXAOcEr6+BSguzULd+luThv87YNAjgcWdRG3k+gMSzM7tVqsc871phpOTHURyeyqpwOrSK6ukTQD+KSZnZE+nwRMAP7UKf5qSa8DBCwAqmZs+RqWzrmG1dFWm8bbzDYAh3exfz5wRtnzlcC4Lo47LLTMmjbev164tvpBnRz79vA3v23lkuAYSKZGDC5rxeLgmJaD4lYJf9ekQcExyzZsiSrruifDY06cNDg4prQuoiBg/wPeGhzz8qq/RJU1YNyewTFqCf80jRzULzgG4lZ0j0lzBzjyQ18Kjrnh6v8bVdZx+w+NiitXauD58qIWIJY0TdLdaSbQfEkze7aazjkXrsMs01aPotLjgW8C/55mCn01fe6cc31Kh2Xb6lHsAsQGDEsf7waETyfnnHM9rF6vqrOI7fM+G5gr6dskV+/vKK5KzjlXjLY6XSUni9gMy08B55jZBOAc4IpKB5ZnLi25/cbI4pxzLlwjd5vENt6n8LdpYH8JVLxhaWaXmdkMM5ux7+EfjCzOOefCvdZvWHblaeCQ9PFhgM8q6Jzrcxr5yjt2AeJPAN9LJ6d6BdhlEhfnnOtt9dowZ5FnAeK3FVwX55wrVL12iWQhq+E398zmLcGFjbTwDMGmLRuCYwBKg3ZZuagq6zcwPKYpbpDPUy+FZ5uOHhxXVkfEXfrB9kpwTMvGuAzLy54ZHhxzxl5xvYRNWzeFB0X8Xj3QNDG8HOAtw8IXII753ALMXRX++/ihj34lqqy2B2crKrDMfw7dJ9MP4vMvPpa7rFrzuU2ccw2rkbtNYtPj3yLpr5IelvQbScO6O4dzzvWG1/pokyvZNT3+cuBcMzsAuBn4YsH1cs653Bp5tEns6vH7AHelj+cBPoDbOdfnvNavvLuymGS1ZIAPk0wu7pxzfUop41aPYhvv04BPS7ofGAq0VTqwPD3+Z1fOjizOOefCtZUs01aPokabmNkS4CgASfsAf9/NsTsW9owZKuicc7HqtUski6jGW9JoM1svqQn4N+CHxVbLOefyq9ebkVnErh5/sqTHgCUk85z8pGer6Zxz4Wp1w1LShyUtllRKFx2udNzRkpZKWi7p3LL9kyXdk+6/TlLVdfTypMd/r1qsc871phpeeS8CPgD8qNIBkpqBHwBHAquB+yTNMbNHgIuBS8zsWkk/BE4HLu2uwNgbls451+fV6oalmT1qZkurHDYTWG5mK8ysDbgWmCVJJLOz3pAedxVQfZVyM+sTG3BmX41p1LL6ev38vfD3olYbycyo88u22PfgTmBGhdc+BFxe9vwfge8Do9JGffv+CcCiamX1pSvvmGllaxXTqGX19frVsqy+Xr9altXX61c4K1s0Jt0u63yMpNskLepim9XVOXuaT0zlnHMZmNkROU+xhp0TGsen+zYAwyW1mFl72f5u9aUrb+eca2T3AVPSkSWtwEnAHEv6Su4g6VaBZJnJX1c7WV9qvHf5N6UPxTRqWX29frUsq6/Xr5Zl9fX69TmS3p+uNHYQ8DtJc9P9e0q6BSC9qj4LmAs8ClxvZovTU3wZ+Lyk5cDudLOo+44y0w5y55xzdaQvXXk755zLyBtv55yrQ73eeFdKF60Ss8vqPhliJki6Q9IjaRrr5zLEDJB0r6SH0ph/z1peGt8s6UFJv814/Mp0daIFkuYHlDNc0g2Slkh6VNJBVY6fmpaxfXtB0tkZyjknfR8WSbpG0oCM9ftcGrO4UjkVVmwaKWmepGXp110WGa0Q122qcoWYb6Xv30JJN0vaZZHMCnHfSGMWSLpV0p7VYspe+4IkkzQqQzlfk7Sm7Gd2TJb6pfs/m35viyV9M0NZ15WVs1LSgozvxTRJd2///EqamSHGV+WK1csD45uBx4E3AK3AQ8D+GeLeDbyVDAPZy2LGAm9NHw8FHqtWFiBgSPq4H3APcGBAmZ8HfgH8NuPxK4FREe/jVcAZ6eNWYHjgz+AZYGKV48YBTwAD0+fXA6dmOP+bSFKHB5EMTb0N2DvLzxT4JsmKTQDnAhdnjNsPmEqFhIkKMUcBLenjiwPKGlb2+F+AH2b5rJIMGZsLrOr8M69QzteAfw39vQAOTd/z/unz0VnqV/b6d4CvZizrVuB96eNjgDszxNwHHJI+Pg34Rujn/7W69faVd5fpotWCrOvVfarFrDWzB9LHL5Lc7R1XJcbM7KX0ab90y3SHV9J4kqlyLw+pZyhJu5H8UlwBYGZtZvZ8wCkOBx43s1UZjm0BBkpqIWmMn84Qsx9wj5ltteRu+59I5oDYSYWf6SySP0xQIWW4qzirkqpcIebWtH4Ad5OMtc0S90LZ08F0+nx081m9BPhS5+OrxHSrQtyngIvMbFt6zPqsZUkScAJwTcayDNh+5bwbnT4fFWJ8Va5Ivd14jwOeKnu+mioNahEkTQKmk1xJVzu2Of23cT0wz8yqxqS+S/LLGbJQhwG3SrpfUtbMs8nAs8BP0i6ayyUNDijzJLr45dylYmZrgG8DTwJrgc1mdmuG8y8CDpa0u6RBJFdkWVdeGmNma9PHzwBjMsbldRrw+6wHS7pQ0lPAR4GvZjh+FrDGzB4KrNdZaRfN7K66kCrYh+T9v0fSnyS9PaC8g4F1ZrYs4/FnA99K34tvA+dliPFVuSL1duNdc5KGADcCZ3e6auqSmXWY2TSSK7GZkt6UoYxjgfVmdn9g9d5lZm8F3gd8RtK7M8S0kPwreqmZTQe2kHQxVKUkUeA44JcZjh1B8ks2GdgTGCzpY9XizOxRkm6IW4E/AAuAjiz163QeI+N/PXlIOh9oB67OGmNm55vZhDTmrCrnHwR8hQyNfCeXAnsB00j+eH4nY1wLMBI4kGSh8OvTK+osTibDH/YynwLOSd+Lc8gwVpmAVbncznq78a6ULtojJPUjabivNrObQmLTrog7gKMzHP5O4DhJK0m6gg6T9PMMZaxJv64HbibpVqpmNbC67D+CG0ga8yzeBzxgZusyHHsE8ISZPWtmrwI3Ae/IUoiZXWFmbzOzdwObSO43ZLFO0liA9Ov6KsfnIulU4Fjgo+kfi1BXU/3f/r1I/gA+lH4+xgMPSNqjuyAzW5deSJSAH5PtswHJ5+OmtAvwXpL/BEdViSHtGvsAcF3GciDJDNz+e/XLLHU0syVmdpSZvY3kD8XjAeW9pvV2491lumhPFJRebVwBPGpm/5kx5nXbRx1IGkgyD++SanFmdp6ZjTezSSTf0x/NrNurVEmDJQ3d/pjkBlrV0TRm9gzwlKSp6a7DgUeqxaVCrqyeBA6UNCh9Lw8nuW9QlaTR6dfXkzQIv8hY5hySBgEypgzHknQ0STfXcWa2NSBuStnTWVT5fJjZw2Y22swmpZ+P1SQ30p+pUs7YsqfvJ8NnI/UrkpuW25csbAWeyxB3BLDEzFZnLAeSPu5D0seHAVW7W8o+G74qV6jevmNK0gf6GMlf3PMzxlxD8q/jqyQf/tMzxLyL5N/uhST/ui8AjqkS82bgwTRmEV3cdc9Q7nvIMNqEZMTNQ+m2OOt7kcZOI5nGciHJL+uIDDGDSSbE2S2gnH8naZwWAT8jHcGQIe7PJH9QHgIOz/ozJUkTvp2kEbgNGJkx7v3p423AOmBuhpjlJPdftn82fpixrBvT92Mh8BtgXMhnlS5GGFUo52fAw2k5c4CxGevXCvw8reMDwGFZ6gdcCXwy5HeQ5Hfs/vTnfA/wtgwxnyP5/X8MuIg069u36punxzvnXB3q7W4T55xzEbzxds65OuSNt3PO1SFvvJ1zrg554+2cc3XIG2/nnKtD3ng751wd+l9hF7YpfxNy7AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD/CAYAAADVGuzgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZxdVZXvv7+qSiWVuZJACAkkASKDDwwYEMWWUQhtP4ItIPB5TUAwrf0ApW0EP/ikGwWDNtI4NjQGEJpJaDG2zHMrEhIgjAETAoGMhMwhY1Wt98c5hSeXunXWrbpVyb21vp/P/tQ5e//O3vvee+66u/bZay+ZGUEQBMGOT8327kAQBEHgIwx2EARBhRAGOwiCoEIIgx0EQVAhhMEOgiCoEMJgB0EQVAhhsIMgCIogaZqkdyW9XKRckn4saZ6kFyUdlCmbLGlumiaXoz9hsIMgCIpzIzCxnfLjgXFpmgL8AkDSEOBS4BPAIcClkho725kw2EEQBEUwsyeBle1IJgG/soSngcGSRgDHAQ+Z2UozWwU8RPuG30UY7CAIgo4zEngnc74wzSuW3ynqOltBHvUHfsnl+/7Dn37bXecnRg12a/ds7OPWPrdkvUt36B9+7K6zz6kXubV1K99y6R7btIu7ziO3vuLWqk9/t3b+gL1dusffbG9wsi2Tx8qtfWXLQJduxsI17jqPHzfUrZ23cqNbu+uA3i7d2H7+bSLe2+r/6q7e1OzSvbFqg7vOsY0Nbu1eM6a5tQ0nXuC/CYrgtTkAW2ff8PckUxmtXGdm13W2D11FlxvsIAiC7kQ1tW5tapw7Y6AXAbtlzkeleYuAIwryH+9EO0BMiQRBUGWoptadysB04Ix0tcihwBozWwI8ABwrqTF92HhsmtcpckfYkvYhmVhvnX9ZBEw3szmdbTwIgqDclMkQJ3VJt5GMlIdJWkiy8qMXgJn9O3Av8NfAPGADcFZatlLSd4GZaVWXmZl/frAI7RpsSRcBpwG3A8+k2aOA2yTdbmZTi1w3hXReqHbUp6gZ5pvvDIIg6Cw1verLVpeZnZZTbsD/LVI2DfBP4DvIG2GfDXzUzLZmMyX9CHgFaNNgZ+eFSnkAEARB0FlqyjjC3tHIM9gtwK7AgoL8EWlZEATBDkU5p0R2NPIM9teBRyTN5S9rCncH9gLO7cqOBUEQdIQea7DN7H5JHyFxrcw+dJxpZq7Fnd711Ree+z2XDuDx3/zQrd2w1f+PwKpNW/NFwNb3/WtwGzb61wFr62aXbsk6nw6gafXb/vZ7+9esv9U82q31UrNsrlv70Apf+/V1/oVQDSVoB9T7V8R612zv0cdvaIbV+Wcalzu/ArsP8n/+pbB6zjy3tuHEzrenmupd/JZ715lZC/B0N/QlCIKg09TUle+h445GOM4EQVBV9NgpkSAIgkpDtWGwgyAIKoIYYQdBEFQIYbCDIAgqhJ7sOBMEQVBRxCqRIAiCCiGmRDqBN9hAKc4wR3z+Qrd2+R9+5tZ+fIRvU/zegwe461RLk1vb0sdX7yGjfP0EqDX/pvwaNsqt7dXk22d+t0H+je5t4O5u7SkjfEEc1mz2bd4PsHyD/7NqbPB/dfr08jlyLG32G5rhtf7XtbXZ5zmzbov/9e9ZQgCDwfvu5daWgzDYQRAEFUIY7CAIggqhR6/DTgMYjARmmNn6TP5EM7u/KzsXBEFQKtX80LHdyTVJ5wO/Bc4DXpY0KVN8RTvXTZE0S9Kse269qTw9DYIgcNDNIcK6lbwR9peBj5vZekljgLskjTGza4CiT52yAQyeXrAyAhgEQdBt1NR0OvD6Dkuewa5pnQYxs7ckHUFitEfTjsEOgiDYXqiMBlvSROAaoBa4vjAsoqSrgSPT077AzmY2OC1rBl5Ky942sxM62588g71M0ngzmw2QjrT/hiRO2f6dbTwIgqDcSOUx2JJqgZ8BnwUWAjMlTTezV1s1ZnZBRn8ecGCmio1mNr4snUnJM9hnANsszjSzJpKw7td6Gtiz0bcpeimBBkpZW73Tp9uMj9kmbz76E5eu75FfcNd5/Zv+m2e/nYa5dC22xV3nmHGfdGtLWTM+ptn3ufau87/+UgIYzNjUz6U7ZqzPDwBg4TpfAAuA97f410EP79/Lpdt18xJ3ncu0q1u7cz/frOQBQ/wb/y/e5J/p7DV6X7e2HJRxSuQQYJ6ZzQeQdDswCXi1iP40kqjqXUZexJmF7ZT9sfzdCYIg6Bw1JUQOymEkfwmNCMko+xNtCdNp4rHAo5nsPpJmkQx6p5rZPZ3tUKzDDoKgqqgpYUpE0hRgSibrunTRRKmcCtxVEDpxtJktkrQH8Kikl8zsjQ7U/QFhsIMgqCpKeeiYXdHWBouA3TLno9K8tjgV2Gb+1cwWpX/nS3qcZH67Uwa7eqNVBkHQI1GN3CmHmcA4SWMl1ZMY5ekfai9xLmwE/pTJa5TUOz0eBhxG8blvNzHCDoKgqijXQ0cza5J0LvAAybK+aWb2iqTLgFlm1mq8TwVuN7Psk9h9gWsltZAMjKdmV5d0lDDYQRBUFSrjvIGZ3QvcW5D3nYLzf27juqfogqXPYbCDIKgqamurd6Y3DHYQBFVFOT0ddzS63GA/t2R9vghYtcnvtOANNAB+ZxiAsUed59JtuPFUd52T99/DrTXnZjSPvLnGXWfd4jlubfPOe7q167f4HJ1WbfI7RA0ZvrdbO2CJz3Fl5mLf/Qdw0Ij+bu0bqza5tes2+96Dtf1HuuscWOs3SkvX+xyilm0pZTMkv+NM8wq/Q1A5DFIY7CAIggqhlHXYlUYY7CAIqopqHmGXPDsv6Vdd0ZEgCIJyUFtX406VRrsjbEmFi8QFHClpMEA5tgsMgiAoJ+XarW9HJO8nZhSwFvgRcFWa1mWO2yQbcebeO2JAHgRB96Eaf6o08uawJwBfAy4BLjSz2ZI2mtkT7V2U9c9/4PV3I+JMEATdRo+NOGNmLcDVkn6d/l2Wd00QBMH2pJofOrqMb7ov9smSPkcyRRIEQbBDUs1z2CWNls3s98DvS7nm0D/82KXb+v5Gd529Bw9wa0uJDuN1iOl75u3uOhc98Wm3ds1mn/PQ0W/c5a5z7sF/59b+4okFbu1U3eHSrXzhNXedtZO/6tYe27Lapdv6lt9xqHb1CLf2oBZ/xJm1M59y6frutZe7zppPft6t3f3lB33CEl6TGnwRfwBOe3s/t/Zut7I4lbj6w0tMbwRBUFXU9vQpkSAIgkohDHYQBEGFEAY7CIKgQgiDHQRBUCGEwQ6CIKgQescqkSAIgsqgmkfY2jZuZPnZ/P46VwPa6N+UXy2+DdkBpr3p//Am7z/MpVvX5K9z5OHnurU3TrvMpfvM6MHuOktxIijlXhjS4Putr3v/PXedNZv898Drtbu5dPtsnu+u84kmfwCBMYP7uLW79vWN+Gqe97s4vDb2OLd21IBeLl3/lg3uOlfT4NYOqvEHJ+k9YHCnre2Ztz7nvpFvPP2gdtuTNBG4hiQI7/VmNrWg/Ezgh8CiNOunZnZ9WjYZ+Haa/z0zu8nbr2LECDsIgqqiXCNsSbXAz4DPAguBmZKmtxH9/A4zO7fg2iHApST7MRnwbHrtqs70qXone4Ig6JHU1sidcjgEmGdm881sC3A7MMnZjeOAh8xsZWqkHwImdvhFpYTBDoKgqqivrXGn7FbQaZqSqWok8E7mfGGaV8gXJL0o6S5JrXN13mtLIi+AwSeAOWa2VlIDcDFwEPAqcIWZ+ScdgyAIuoFSpkSyW0F3kN8Bt5nZZkl/D9wEHNWJ+tolb4Q9DWh9EnENMAi4Ms27odhF2V+t66cVlQVBEJSdMk6JLAKyT7dH8ZeHiwCY2Qoz25yeXg983HttR8h76FhjZq1LMiaY2UHp8R8kzS52UfZXy7tKJAiCoBzUlW9Z30xgnKSxJMb2VOD0rEDSCDNbkp6eALRuD/kAcIWkxvT8WOBbne1QnsF+WdJZZnYD8IKkCWY2S9JHAP9anSAIgm6iXKtEzKxJ0rkkxrcWmGZmr0i6DJhlZtOB8yWdADQBK4Ez02tXSvouidEHuMzMVna2T3kG+xzgGknfBt4D/iTpHZLJ9HM623gQBEG5KafjjJndC9xbkPedzPG3KDJyNrNpJNPKZSMvRNga4ExJA4GxqX6hmS1zN7DyLZdOWzfni1Ja+vgDGOy3k88ZBsBqal06b6AB8DvDAJz5pe/ki4ClT/7UXWdTi39GSiVEJd3Y1OLSDZg7w10nYw5wSwf38X1WtnSpu869x4xza4fV+u/XmrW+pbc1u+/rrrO2BIeoLc57oOb9Fe46G+t6u7Xv9trJre30Mgqgvqe7ppvZWuCFLu5LEARBp6lm1/TwdAyCoKoIgx0EQVAhhMEOgiCoEMJgB0EQVAhhsIMgCCqE+toevkokCIKgUqgpYcljpREGOwiCqqK2eu1110eceXjuclcDS9b5HREOGTXQrV26botbu25Ls0t39Bt3uetc8anJbm3fXr5/5Xb5jD+Kzaa7vuLWNg3Z3a19c2tfl+7d9X4no09pgVv7WJMv4sx7G/yf/7gh/dzaVZv8r8v7uR7c7313ne83+B3CXn1vo0s3toQoOqs2+b4rAHu+fLdb2/uYszptbqfNettt1L40YfeKMu8xwg6CoKooxQu00giDHQRBVdGrp64SkVRPsqXgYjN7WNLpwKdIthC8zsxix74gCHYoanqqwSYJUlAH9E0jAPcH/gs4miTemX+CNgiCoBvoyatE9jezAyTVkWzgvauZNUu6hXY2g0rjok0B+Ppl/8rnTj2jbB0OgiBoj2peJZIbcSadFukH9CUJEbYS6A30KnZRNuKMd5VIEARBOejJI+xfAq+RRFu4BPi1pPnAoSQh34MgCHYoqtk1PXcdtqRdAcxssaTBwDHA22b2jKeB5lcfd42wm5a+7ZEBUDtoqFvbPO6Tbm3d4ldcurmNH3PXOaDet9E+gHO5LsPe8/UToM9J/+7Wfv5r/jXbt05Y59JZs3+97sF3+rXPnu3bFH/LPP827vV77u/WloT5gj1ctci/tvr8g/zaXstec2u9WJ1/zfZht7zn1j797WM6bW3ve22Z+7/64/cZXlHWPddEmNliM1ucHq82s7u8xjoIgqC7qZHcKQ9JEyW9LmmepIvbKP9HSa9KelHSI5JGZ8qaJc1O0/RyvLZYhx0EQVVRrikRSbXAz4DPAguBmZKmm9mrGdnzwAQz2yDpq8APgC+mZRvNbHxZOpNSvdtaBUHQI6mRP+VwCDDPzOab2RaS53aTsgIze8zMNqSnTwOjyv16soTBDoKgqqiV3EnSFEmzMmlKpqqRwDuZ84W0Hyf4bOC+zHmftM6nJZ1YjtcWUyJBEFQVvUpYiJ1dgtwZJP0fYAJweCZ7tJktkrQH8Kikl8zsjc60EwY7CIKqoozrsBcB2W0hR6V52yDpGJJlz4eb2QfbjprZovTvfEmPAwcCnTLYMSUSBEFVUcqUSA4zgXGSxmb2VdpmtYekA4FrgRPM7N1MfqOk3unxMOAwIPuwskPECDsIgqqiXCNsM2uSdC7wAInz4DQze0XSZcAsM5sO/JBkj6VfK2n3bTM7AdgXuFZSC8nAeGrB6pIO0eUBDFrmz/I5ziyZ766zdtTebm3zoBFuLS1NLtmFT6xwV3nh4WPd2t51vn94BjWvddf5d7/1OyT95hq/k82mX0/JFwEt7/v7umaPT7u1/W2TS9fr3T+762weMNyttd7+YAd1K32fwTO1e7jrPNj8n2tzo2/hQq2znwDUFd2Z4kM8vMH/vh63986dtrazF612G7XxIwdXlONMjLCDIKgqevJeIkEQBBVFNUecafd/cEmDJE2V9JqklZJWSJqT5g3urk4GQRB4kfyp0sibNL0TWAUcYWZDzGwocGSad2dXdy4IgqBUyujpuMORZ7DHmNmVZra0NcPMlprZlcDoYhdlvYeuu+2/ytXXIAiCXKp5hJ03h71A0jeBm8xsGYCk4cCZbOuyuQ1Z7yHvKpEgCIJyUEMFWmIneSPsLwJDgSfSOeyVwOPAEODkLu5bEARByfTYEbaZrQIuStM2SDqLJEhvEATBDkM1x3TssOOMpLfNbPc83bzl61wNvLXa5wgB0KuEpwVjBvsjY6zf4osMsseMX7rrtOO+6tZubPK1/95Gn4MPwF7vznRr1cfvDNLnZN9+Oa88cI27zj1WznZrXxl4gEvXv96/+8Kqjf6INw3e8EBArVM6pp//vl7T5G9/8fqtbq2Xxj7+FcE7/3GaW9twwvmdNrcLVqx3G7XRQ/tXlHlv912X9GKxIsDvvhQEQdBNVOJUh5e8n8nhwHEky/iyCHiqS3oUBEHQCap5R7s8g/3fQH8z+9D/qul2gUEQBDsUquIhdt5Dx7PbKTu9/N0JgiDoHN5nBpVI7CUSBEFVUcX2Ogx2EATVRY+dEgmCIKg0KnGPEC9dbrAff3Nl2evcbVCDW9u7zv/prdrkWwe98oXX3HXu9On33NoBc2f4dMCM4Ue4tHs2+9cWWwnBBrzrqz963Nfcda5/bKpb27+EdcgvLF3v0o0a6F+zv/z9LW6tl2ENfd3aQTX+9lv6+4MNPL/E917161XrrnPpk35fgLEnuKVFqWJ7HSPsSsRrrAO/sQ78xnpHp5pH2NU8Px8EQQ+ktkbulIekiZJelzRP0sVtlPeWdEdaPkPSmEzZt9L81yUdV47XlhfAYKCk70u6WdLpBWU/L0cHgiAIyolKSO3WI9UCPwOOB/YDTpO0X4HsbGCVme0FXA1cmV67H0mU9Y8CE4Gfp/V1irwR9g0kr+tu4FRJd7eGbgcO7WzjQRAE5aZGcqccDgHmmdl8M9sC3A5MKtBMAm5Kj+8CjlayTGUScLuZbTazN4F5aX2de2055Xua2cVmdk8auv054FFJQ9u7KBvA4Mnf3NrZPgZBELgpZXvVrK1K05RMVSPZdt//hWkebWnMrAlYQ7Iltefaksl76NhbUo2ZtaQdulzSIuBJoH+xi7IBDK5/ZkEEMAiCoNtQCTuQZm1VJZA3wv4dcFQ2w8xuBL4BlH9dUxAEQWexFn9qn0XAbpnzUWlemxpJdcAgYIXz2pJp12Cb2TfN7OE28u8Hruhs40EQBOVGLU3ulMNMYJyksZLqSR4iTi/QTAcmp8cnAY9aEmRgOslzv96SxgLjgGc6/dq6OoDB1uVvuxqoWTbX3bYNzW32L/Wue9etbRq+t0tXu2qhu06r8zstUFvvkmnTOneVE65f5tY+fPHhbu3gRc+6dM27+N5TgP5HfmjVVFE2TDvFpbMR49x11m4o3EW4OM392n2Ms22965e7dBe86HcI+9dPD3JrTb7Vuy19G9111pTwXh13y3y39rGvf6bTq6g3r1vtNmq9Bwxutz1Jfw38G1ALTEunhS8DZpnZdEl9gJuBA4GVwKlmNj+99hLgS0AT8HUzu69DLyhDBDAIgqC6yJ/q8Fdldi9wb0HedzLHmygS39bMLgcuL1tniAAGQRBUGSqjwd7RiAAGQRBUFz3VYEcAgyAIKo78h4kVS2z+FARBddHSQ0fYQRAElUZPnsMOgiCoLMJgB0EQVAgd9C2pBLrcYL+yZaBL99CK0e46Txmxi1s7Y1M/t3bAEl90lmNbVrvrfL3xILd2cB/f7otz1m9w1/ns2f7Rxlbb5Na+MvAAl66UyDBeZxiAvl+606V7/Dc/dNc5doj/vlqwZrNbO6Cvz8nmqkP9Tk7NDX7HmedX+O7r8f38n9XyWr+TzYOH+b8vZSFG2EEQBJWBw+W8YinZYEva2cz8/t5BEATdSU8dYUsaUpgFPCPpQJJ9SMofYTcIgqAz9FSDDbwHLCjIG0kSyMCAPbqiU0EQBB2lmpf15T1luBB4HTjBzMaa2VhgYXpc1FhnozjcfcuNZexuEARBDi0t/lRh5LmmXyXpDuBqSe8Al5KMrNslG8Vh9iL/VodBEASdpicv6zOzhcDJkk4AHgL6dnmvgiAIOkg1rxJxL7w0s+nAkcAxAJLO6qpOBUEQdJjyhQjb4ShpWZ+ZbQReTk//Bbgh75oZC9e46q6v8y/aX7PZ5wgAcMzYwW7tzMXrXbqtb81x17lPX3/7tnSpS/c/Nfu769yy7AW3tr6EkUn/wR916V5Y6ntPAXYrITqM1yHmiM9f6K5zwWM/cWtr5Q+MsmGr7359q8HvuDN6oz/iS2OD7x6cu9ofpvX9Lf7v4NDl/lCGJcRnKk4FGmIvEXEmCILqosX/Y1JpRMSZIAiqCuum1R+pn8odwBjgLeAUM1tVoBkP/AIYCDQDl5vZHWnZjcDhQOs0xJltBYvJEhFngiCoLpr8Uzud5GLgETObKuni9PyiAs0G4AwzmytpV+BZSQ+YWesGKxea2V3eBiPiTBAEVYU1d9uUyCTgiPT4JuBxCgy2mf05c7xY0rvATkCHdsTyP+kLgiCoBLrPcWa4mS1Jj5eS81xP0iFAPfBGJvtySS9KulpS77wGY7e+IAiqixIeOkqaAkzJZF2XOv61lj8MtLV855LsiZmZpKIeO5JGADcDk80+WMbyLRJDX0/iaHgRcFl7/Q2DHQRBVWElGOysV3aR8mOKlUlaJmmEmS1JDXKbu5hKGgj8HrjEzJ7O1N06Ot8s6Qbgn/L62+UG+/hxvs3bG0pYh718g3+98MJ1W93ag0b0d+lqV49w1/lE00i3du8xvnXI49b7X1N9f/+a7eb+O7m1qzb6vhSjBvZx11m7wb9e1xtsoJS11aOPPM+tnX3fv7m1azf77tfda9e561xshRtpFseafP/6l7K2uk8J31fq6v3actB9e4RMByYDU9O/vy0USKoHfgP8qvDhYsbYCziRv/i4FCVG2EEQVBXWfatEpgJ3SjqbZFfTUwAkTQC+YmbnpHmfAYZKOjO9rnX53n9K2olkmfRs4Ct5DXYkgMFQM1tR6nVBEATdQjeNsFM7eHQb+bOAc9LjW4Bbilx/VKlttvt/jaSpkoalxxMkzQdmSFog6fBSGwuCIOhqrLnZnSqNvImoz5nZe+nxD4EvmtlewGeBq7q0Z0EQBB2hpdmfKow8g10nqXXapMHMZsIHi8GLrhnMBjC49aZpZepqEASBgyo22Hlz2D8H7pU0Fbhf0jXAfwFHkUySt0l2qczbK9dX727iQRDscHTXXiLbgzzX9J9Iegn4KvCRVD8OuAf4btd3LwiCoES6b5VIt+OJOPM4iY/8NqQBDHL3ww6CIOhOeuwIOwdXAIN5Kze6KhtQ7+9KY4NfW4ozwBurNrl0B5Uw9zVmsN9xZFjtZpduzqYS5t5K8G+w3v3c2gbzVbz8ff9op7m/z8kKYMEa33tVSqCBUpxhxh//dbf22Xuvdums3v/+D6mpdWvfWeszYP3q/XXW1/rf161L33Fr/d+WdqjAuWkvEcAgCILqoqcabCKAQRAEFUYlrq/2EgEMgiCoLpr8e+1UGhHAIAiCqqKU3foqjdj8KQiCqiJWiQRBEFQI1hwGOwiCoCIIgx0EQVAhxJRIJ9h1QG5cScDvYAPQp5ffG2R4/15u7brNvg967Uz/isZdPzbRra1ZW7h6sm369hrsrhNntBGAupVvu7W1/T/i74O3zvXL3doBfX1ONhu2+h9AeSPDgN8ZBuDjf32BS7dh2inuOpvHfcatbezjc4h5dsl6d51bSxjFjqzt3ljfLVv8n2OlESPsIAiqipYqXoedF8BggqTHJN0iaTdJD0laI2mmpAO7q5NBEARerKXFnSqNvP9Vfg78gCTi71PAtWY2CLg4LQuCINihsOYWd+oMkoakg9i56d/GIrpmSbPTND2TP1bSDEnzJN2RBuxtlzyD3cvM7jOz2wBrjfprZo9Qpn1agiAIykl3GWySgesjZjYOeCQ9b4uNZjY+TSdk8q8Erk6jeK0CijoqtpJnsDdJOlbSyYBJOhEgjedYdKIoG3HmzptjB9YgCLqP5q1N7tRJJgE3pcc3ASd6L5QkkkAwd5Vyfd5Dx6+QTIm0kGwC9VVJNwKLgC8Xuygbcea1ZWsj4kwQBN1GN67DHm5mS9LjpRTfwbSPpFlAEzDVzO4BhgKrzaz1V2MhMDKvwby9RF4gMdStfC1NrQEMYse+IAh2KEox2JKmAFMyWdelA87W8oeBXdq49JJt2jQzScUGp6PNbJGkPYBH0yhea9ydzNDlAQzG9vMNsPdwrhUFWNrs1+66eUm+KGVt/9wfOAD67rWXu86a53/v1+6+r0t3cD//2vKrXh3m1v7VmCFu7YH9fBvYD2vo667zgocb3NqrDl3m0r3V0NZ3rW12r13n1pYSbMC7vrrvl+5017n+x6vd2j4f/98u3XFDNrjrtF4+/wqAlj2+4daWg1JWf2RnA4qUH1OsTNIySSPMbImkEcC7RepYlP6dn+5yeiBwNzBYUl06yh5FMnPRLhHAIAiCqqIbp0SmA5OBqenf3xYK0pUjG8xss6RhwGHAD9IR+WPAScDtxa4vJAIYBEFQVXSjwZ4K3CnpbGABcAok/ivAV8zsHGBf4FpJLSSLPKaa2avp9RcBt0v6HvA88Mu8BiOAQRAEVUUZVn+4MLMVwNFt5M8CzkmPnwL2L3L9fOCQUtqMAAZBEFQVPTlEWBAEQUVRiS7nXsJgB0FQVcR+2EEQBBVCGOwgCIIKoaWKDbbMutZzfNGq910NDKsrITS9fE4bAO9t9f8mDezt22i992a/k9JrG/3OILXO1zVygP81NdgWt7ZuxVtu7YrGcS7doBr/51qzaa1b29IwyFfnRv9ntbjG7zg0pMHvvNXc4vuO9X1+er4opf/5D7q1q5/ybay5alPXPKwr5bvde8Bg/5e7CHPOOsFt1Pa9YXqn2+tOYoQdBEFVYc3Vu31RnqfjIOBbJLtI7QwYifvlb0kWgPv9Y4MgCLqBap4SyZsDuJPEy/EIMxtiZkOBI9M8/8YHQRAE3YS1mDtVGnlTImPM7MpshpktBa6U9KWu61YQBEHHaKniKZG8EfYCSd+U9MFGT5KGS7oIeKfYRdkABrfcOK1cfQ2CIMilZUuzO1UaeSPsL5KEvXkiNdoGLCPZparonpHZLQu9q0SCIAjKQTWPsPP2Elkl6QbgIeBpM1vfWiZpIpkMwwIAAA7jSURBVHB/F/cvCIKgJKrZcabdKRFJ55OsCDkXeFnSpEzxFV3ZsSAIgo7Q0mLuVGnkTYl8Gfi4ma2XNAa4S9IYM7uGZE/sXFY7F+MvL+FHcWsJv6A7OyPeACxd79uWcfeX/U4Low4+ya3d4ryBXn1vo7vOQ5vmurVNO/kj6Sxe73OGaOnvj44zRD7HJYDnV/juq8aGwe46rcl/X72z1q9tdEZT8kaGAVj9lDveK4M/9Q8u3aInfuqus5f/o6LX0tf84gGH+rVF6LHrsIGa1mkQM3tL0hEkRns0ToMdBEHQnfTkddjLJI1vPUmN998AwyiyKXcQBMH2pGVLiztVGnkj7DNIQrN/QBow8gxJ13ZZr4IgCDpIjx1hm9nC1FGmrbI/dk2XgiAIOk53eTpKGiLpIUlz07+NbWiOlDQ7kzZJOjEtu1HSm5my8R9uZVtKeHQQBEGw49PSbO7USS4GHjGzccAj6fk2mNljZjbezMYDRwEbgOyqhQtby9uKnVtIGOwgCKoKa25xp04yCbgpPb6JZJO89jgJuM/MNnS0wTDYQRBUFdZs7tRJhpvZkvR4KTC8PTFwKnBbQd7lkl6UdLWk3nkNdvl+2G+s8v+Y7D6oj0u3bos/jP0BQ/y/Scu2ODelb/HvQdC/xf/6a95f4dINq4flfUf5Kn3P3Ty1K9/2i+v3cMmeX7I+X5Ry5O4fmgIsyvh+vs917mp/AIf3S9hbol+9P4DBs8734Lgh/ntlRf3Obq13ffXIw8911/mn6T9yaw9Y0eZjsDap8bsCFKV5q/9zlDQFmJLJui7dWqO1/GFglzYuvSR7YmYmqegvgKQRJCvrHshkf4vE0NeTbOVxEXBZe/3dYQIYeI11UIKxDoIeSClz09l9j4qUH1OsTNIySSPMbElqkN9tp6lTgN+Y2QceZ5nR+eZ0C5B/yutvnmv6QEnfl3SzpNMLynxxh4IgCLqRbpwSmQ5MTo8nk2zjUYzTKJgOSY08kkQy//1yXoN5/1feQOLReDdwqqS7M/MsnfchDYIgKDPd+NBxKvBZSXOBY9JzJE2QdH2rKN3WYzfgiYLr/1PSS8BLJM6I38trMG9KZE8z+0J6fI+kS4BHJZ2Q/1qCIAi6n+7aXtXMVgBHt5E/Czgnc/4WMLIN3VGltpk3wu4t/WVHHjO7HPgP4ElgaLGLsgEMHrjz5lL7FARB0GGam1rcqdLIG2H/jmSx98OtGWZ2o6SlwE+KXZSdyJ/+6tLq3TorCIIdjmarXpOT55r+TWChpKMl9c/k3w+c39WdC4IgKJVm86dKI2+VyHkkTz7P48MBDC7vyo4FQRB0hGYzd6o08qZEptDJAAZjGxs618M22LOEOhdvKuVD8WnV0M9d42r8fW2sy3V0AmCVMygEwM51Jaxvr/U7gzT28S3h79fLX2fNhlVu7fJan5NNKc4wfer8Tlb1tf7t4L0BN6yX7/MvFW+wgVKcYT55wj+6tRuu/0K+qIxU4sjZSwQwCIKgqqjEkbOXCGAQBEFVsaXF3KnSiAAGQRBUFT12SsTMFrZTFgEMgiDY4eixBjsIgqDSqOY57DDYQRBUFTHCDoIgqBBihB0EQVAhVOLqDy9dbrD3mjHNpVs9Z567zsH7+sNS9Bq9r1vbvGJJvgg47e393HXeesDWfFHKu712cun2fPZOd52HPb2bW/svJx/g1n7med/nuvTJme46jxv1Zbf2wcNWu3RDly9y10ldvVu6dek7bu3IWp/nSsse33DXOazZf1/1WvqaS1dKZJhSnGH6nnO3W7vl+c+5tcWIKZEMknY2s/YiKwRBEGw3euyUiKQhhVnAM5IOBGRmK7usZ0EQBB2g8jZN9ZM3wn4PWFCQNxJ4jmTjDV8k1iAIgm6ix46wgQuBzwIXmtlLAJLeNLOxXd6zIAiCDlDNDx3z9sO+iiTUzXck/UjSABxb2mUjzvzywT+VqatBEAT59Nj9sCFxTzezk4HHgYeAvo5rrjOzCWY24exjP9n5XgZBEDjprv2wJZ0s6RVJLZImtKObKOl1SfMkXZzJHytpRpp/h6TcZUq5BlvSPpKOBh4FjiSJDoykia5XFQRB0I104wj7ZeBvSWLctomkWuBnwPHAfsBpklrXBV8JXG1mewGrgLPzGsyLOHM+mYgzwLFm9nJafEVe5UEQBN1Nd42wzWyOmb2eIzsEmGdm881sC3A7MEmSSOLl3pXqbgJO9DRaNAEvAf3T4zHALOBr6fnz7V2bl4Ap5dR1lXZ7t19Jfd3e7VdSX7d3+5XW165KJFG1ZmVSyX0imS6eUKTsJOD6zPnfAT8liSkwL5O/G/Bybls5HXml4Lw/cD/wI2B2J9+oWeXUdZV2e7dfSX3d3u1XUl+3d/uV1tftlYCHSWYXCtOkjKbbDHbesr5lksab2WxIIs5I+htgGhFxJgiCKsfMjulkFYtIjHEro9K8FcBgSXWWBIVpzW+XvIeOZwDbbDBgZk1mdgbwmVJ6HQRB0AOZCYxLV4TUA6cC0y0ZVj9GMgIHmEzyvLBd8tZhLzSzNneEsc5HnLmuzLqu0m7v9kvR9vT2S9H29PZL0W7v9ndIJH1e0kLgk8DvJT2Q5u8q6V74IKTiucADwBzgTjN7Ja3iIuAfJc0DhgK/zG0znT8JgiAIdnB8+z4GQRAE250w2EEQBBVCGOwgCIIKoVtChEnaB5hEsjUrJMtXppvZnDLUOxKYYWbrM/kTzez+zPkhgJnZzNQtdCLwmpnd62jjV+mqmDzdp0m8ml42swcLyj4BzDGztZIagIuBg4BXgSvMbE2qOx/4jZnlhjPJPHFebGYPSzod+BTJg43rzGxrgX4PEjfa3YBm4M/ArWa2Nq+tICg3EQilY3T5CFvSRSTumAKeSZOA27IboTjqOavgfBu3eUmTMsVXZHSXAj8GfiHp+ySL1vsBF0u6pKDO6QXpd8Dftp4XaJ/JHH85rXcAcGkbr2sasCE9vgYYRLKPwAbghozuu8AMSf8j6R8ktRcz7Abgc8DXJN0MnAzMAA4Grm/jvfp3oE9a3pvEcD8t6Yh22qg4JO3cBXUOLXed5UDSIElTJb0maaWkFZLmpHmDnXXcV3A+UNL3Jd2cDgKyZT8vON9F0i8k/UzSUEn/LOklSXdKGpHRDSlIQ0kCoTS2ESQlaI9u8BT6M9Crjfx6YG4J9bxdcO5ym091tSS7DK4FBqb5DcCLBXU+B9wCHAEcnv5dkh4fXqDNtjET2Ck97ge8VKCdk22joGx2tk6SH9FjSZb4LCfxLJ0MDCi47sX0bx2wDKhNz9XG63opU94XeDw93p2CLQZIfkymAq8BK0kW+M9J8waX8HndlzkeCHwfuBk4vUD384LzXYBfkGyYMxT457T/dwIjCrRDCtJQ4C2gERiS0U0seH2/BF4EbgWGF9Q5FRiWHk8A5gPzSAJ5FN4DzwHfBvZ0vB8TSNbd3kLyY/kQsCa9dw7M6PoDlwGvpOXLgaeBM9uo8wGSpWG7FLx/FwEPZvIOKpI+DiwpqPPu9D04EZienvcucu/eTzJgujh9Py9KX9t5wG8zuhbgzYK0Nf07vyN2paemrm8g+eKPbiN/NPB6Qd6LRdJLwOYCrcttnm0Na6Fxml1wXgNckH6Zxqd5bd5QwAupYRhKgYttG+38GjgrPb6B1I0V+AgwM6Mr/EL0Ak4AbgOWF5S9TPKj1wisIzVQJKPoOQXalzJfusZsfylwh/UagTTfZQi6wgikWpchyLZB8t/H99L77wLgnsL3KnP8GHBw5rMq/JzfBP4VeJvkP8cLgF2L3C/PkOzYdhrwDnBSmn808KeM7rfAmSSeb/8I/D9gHMnmQFcU1Pl6W20VlpFMgT2avp7CtDHnO3EJ8EeS+7zws8p+twoHVNnv4DfSz3X/7HvXnt2IVORz7fIGkvniecB9JAvlr0s/vHlkRj6pdhkwPv0yZdMYkrnarPZRUqOayasDfgU0Z/JmAH3T45pM/qDCGzBTNorEyP608EbMaN4iGX29mf4dkeb3b+OmHwTcCLyR9mdres0TwMcyuqIbarW+hsz5BWkdC4DzgUeA/yAxzpcWaL9GYvz+g+QHtPXHYyfgyQKtywik5y5D0BVGID13GQK2NdiFdRSezwHq0uOnC8oK/3PK1vtXwM9JPIMfo2AToZzXlS17oaBsZuu9S/LcJVv2IPBNMv8lAMNJfuQezuS9DIwr8pm+08brrynIO5NkxL+gIP+FzPH3ct6r1u/Uj0imDmNk3YHUPY0kN9uhwBfSdCjpv+gFul8Cny5Sx61t3AC7FNEeljnuXUQzLPtFL6L5HAWjGsdr7QuMLVI2EPgYyQh0eBvlHymxrV1JR3TAYBI310OKaD+alu+TU6fLCKT5LkPQVUYgcx+0awiAhSSj1W+Q/MgpU1Y4fXRe+h4cRTIdcw3JlNi/ADcXaD/0g08y/TYRuKEg/08kU10nk/zInpjmH862//E81fodIPnv6oFMWeEPZiPJs5DXSPZTXpm+11ey7ZTQScDeRT6nEwvOfwAc04ZuIgVTmCRTN/3b0O4F3FWkvRNIpniWlnKvR0rfv+3dgUg7ViowAisLjEBjgdZlCLraCKTlRQ0BcGlBan3esAvwqzb0RwB3kDxTeAm4l2QbzroC3e0lvK8fI5luug/YJ/0hWE3yo/WpjO4AkumTVcAfSH/ESf4bOr+NevchCSrSvyC/8L/XfUimX9rV5WiPL0FbtH2S50f/q1j7kdq5j7Z3ByJVTiKdSimntpx1FhiCbm+/u98rkqmw14F7SKboslt+PleqLj0/r9zaUtqPlHMPbO8ORKqcRJH5/M5ou6LOSmq/M32ltJVSrkAkXaEtpc5I7aducZwJKgdJLxYrIpnLLlnbFXVWUvtd1VeS5wLrAczsrXRN/V2SRqf6UnVdpS2lzqAdwmAHhQwHjiOZQ80ikgdiHdF2RZ2V1H5X9dUbYKSUQCRdoY1AKGUiDHZQyH+T/Ps6u7BA0uMd1HZFnZXUflf19QygKZthyf7LZ0i6tgO6rtKWUmfQDrEfdhAEQYUQu/UFQRBUCGGwgyAIKoQw2EEQBBVCGOwgCIIKIQx2EARBhfD/AUhACa9C2GRJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for model in AE_trained_models:\n",
        "  val_latents_df = pd.DataFrame(model[2])\n",
        "  plt.figure()\n",
        "  sb.heatmap(val_latents_df.corr(), cmap=\"RdBu\", vmin=-1, vmax=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AE batch size.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}