{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Multi-morbidity Data VAE_Max_lb 50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSW3pk27+ysIC7wiG9H6xi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacPowell/QMUL-Projects/blob/main/Final_Multi_morbidity_Data_VAE_Max_lb_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9-z-70m-t0p",
        "outputId": "e6d999df-f756-4f06-dace-8a88ad06ad6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Setting up google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from IPython import display\n",
        "import math\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n"
      ],
      "metadata": {
        "id": "yEuR_eE0VH1Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()"
      ],
      "metadata": {
        "id": "ywqF18h5n7D8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, test_data = torch.utils.data.random_split(full_data_tens, [30000, 10469])\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Data/MM_train_data'\n",
        "train_data = torch.load(train_path)\n",
        "test_path = '/content/drive/MyDrive/Data/MM_test_data'\n",
        "test_data = torch.load(test_path)\n"
      ],
      "metadata": {
        "id": "bqTXmz5SPhbo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://dfdazac.github.io/01-vae.html\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "class BernoulliVAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, latent_size):\n",
        "        super(BernoulliVAE, self).__init__()\n",
        "        # Encoder parameters\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, latent_size)\n",
        "        self.lrelu = nn.LeakyReLU(0.1)\n",
        "        self.enc_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.enc_logvar = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "        # Distribution to sample for the reparameterization trick\n",
        "        self.normal_dist = MultivariateNormal(torch.zeros(latent_size),\n",
        "                                              torch.eye(latent_size))\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.fc5 = nn.Linear(latent_size, hidden_size3)\n",
        "        self.fc6 = nn.Linear(hidden_size3, hidden_size2)\n",
        "        self.fc7 = nn.Linear(hidden_size2, hidden_size1)\n",
        "        self.fc8 = nn.Linear(hidden_size1, input_size)\n",
        "        self.dec_mu = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "        # Reconstruction loss: binary cross-entropy\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Obtain the parameters of the latent variable distribution\n",
        "        h = self.lrelu(self.fc1(x))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.lrelu(self.fc3(h))\n",
        "        h = self.lrelu(self.fc4(h))\n",
        "        mu_e = self.enc_mu(h)\n",
        "        logvar_e = self.enc_logvar(h)\n",
        "\n",
        "        # Get a latent variable sample with the reparameterization trick\n",
        "        epsilon = self.normal_dist.sample((x.shape[0],))\n",
        "        z = mu_e + torch.sqrt(torch.exp(logvar_e)) * epsilon\n",
        "\n",
        "        return z, mu_e, logvar_e\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Obtain the parameters of the observation distribution\n",
        "        h = self.lrelu(self.fc5(z))\n",
        "        h = self.lrelu(self.fc6(h))\n",
        "        h = self.lrelu(self.fc7(h))\n",
        "        output = torch.sigmoid(self.fc8(h))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Calculate the negative lower bound for the given input \"\"\"\n",
        "        z, mu_e, logvar_e = self.encode(x)\n",
        "        output = self.decode(z)\n",
        "        MSE_Loss = self.criterion(output, x)\n",
        "        kl_div = -0.5* (1 + logvar_e - mu_e**2 - torch.exp(logvar_e)).sum()\n",
        "\n",
        "        # Since the optimizer minimizes, we return the negative\n",
        "        # of the lower bound that we need to maximize\n",
        "        return MSE_Loss + kl_div, z, output"
      ],
      "metadata": {
        "id": "9-jwVuT_kAfG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/christianversloot/machine-learning-articles/blob/main/\n",
        "#how-to-use-k-fold-cross-validation-with-pytorch.md\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Configuration options\n",
        "  k_folds = 5\n",
        "  epochs = 30\n",
        "  #loss_function = nn.CrossEntropyLoss()\n",
        "  loss_function = nn.MSELoss()\n",
        "  \n",
        "  # For fold results\n",
        "  train_results = {}\n",
        "  val_results = {}\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare dataset by concatenating Train/Test part; we split later.\n",
        "\n",
        "  dataset = ConcatDataset([test_data, train_data])\n",
        "  \n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "    \n",
        "  # Start print\n",
        "  print('--------------------------------')\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "    \n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "    \n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      dataset, \n",
        "                      batch_size=100, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      dataset,\n",
        "                      batch_size=100, sampler=test_subsampler)\n",
        "    \n",
        "    net = BernoulliVAE(input_size=204, \n",
        "          hidden_size1=192, \n",
        "          hidden_size2=128, \n",
        "          hidden_size3=64, \n",
        "          latent_size=50)\n",
        "    net.apply(reset_weights)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f'\\n Epoch {epoch}')\n",
        "        loss = 0\n",
        "        training_loss = 0\n",
        "        test_loss = 0\n",
        "        val_latents = []\n",
        "        val_outputs = []\n",
        "        train_outputs = []\n",
        "        for i, (batch_features) in enumerate(trainloader):\n",
        "            optimizer.zero_grad()\n",
        "            # Reshape data so each image is an array with 204 elements\n",
        "            batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "            criterion, z, output = net(batch_features)\n",
        "            criterion.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            \n",
        "            train_outputs.append(output.detach().numpy())\n",
        "            \n",
        "            train_loss = loss_function(output, batch_features)\n",
        "            \n",
        "            # Print statistics\n",
        "            loss += train_loss.item()\n",
        "            training_loss += train_loss.item()\n",
        "            \n",
        "            \n",
        "\n",
        "            if i % 100 == 0:\n",
        "                # Print average loss per sample in batch\n",
        "                batch_loss = loss/len(batch_features)\n",
        "                print(f'\\r[{i:d}/{len(batch_features):d}] batch loss: {batch_loss} ',\n",
        "                      end='', flush=True)\n",
        "            loss = 0\n",
        "          \n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    #print('Starting testing')\n",
        "\n",
        "    # Evaluation for this fold\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, (batch_features) in enumerate(testloader):\n",
        "\n",
        "          # Reshape data so each image is an array with 784 elements\n",
        "          batch_features = batch_features.view(-1, 204)\n",
        "\n",
        "          #test_loss = net(batch_features)\n",
        "          criterion, z, output = net(batch_features)\n",
        "          \n",
        "          val_outputs.append(output.detach().numpy())\n",
        "          val_latents.append(z.detach().numpy())\n",
        "          val_loss = loss_function(output, batch_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          test_loss += val_loss.item()\n",
        "\n",
        "\n",
        "      train_outputs = np.concatenate( train_outputs, axis=0 )\n",
        "      val_outputs = np.concatenate( val_outputs, axis=0 )\n",
        "      val_latents = np.concatenate( val_latents, axis=0 )\n",
        "      model_train_loss = training_loss / len(train_outputs\n",
        "                        )\n",
        "      test_loss = test_loss / len(val_outputs\n",
        "                        )\n",
        "      #animator.add(epoch, (loss, test_loss))\n",
        "      #print(train_iter[0][:5])\n",
        "      #print(outputs[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Print fold loss\n",
        "      print(f'Training loss for fold {fold}: {model_train_loss}')\n",
        "      print(f'Validation loss for fold {fold}: {test_loss}')\n",
        "      print('--------------------------------')\n",
        "      train_results[fold] = model_train_loss\n",
        "      val_results[fold] = test_loss\n",
        "  # Print fold results\n",
        "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "  print('--------------------------------')\n",
        "  train_sum = 0.0\n",
        "  val_sum = 0.0\n",
        "  for key, value in train_results.items():\n",
        "    print(f'Training Fold {key} Loss: {value}')\n",
        "    train_sum += value\n",
        "  print(f'Trainig Average Loss: {train_sum/len(train_results.items())}')\n",
        "  for key, value in val_results.items():\n",
        "    print(f'Validation Fold {key} Loss: {value} ')\n",
        "    val_sum += value\n",
        "  print(f'Vaidation Average Loss: {val_sum/len(val_results.items())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyWJJgMcmEwi",
        "outputId": "50136e9c-d93f-4bf2-bf17-d8060d356c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=204, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[300/100] batch loss: 0.0005112626776099205 \n",
            " Epoch 2\n",
            "[300/100] batch loss: 0.0005736715346574783 \n",
            " Epoch 3\n",
            "[300/100] batch loss: 0.0006285979598760605 \n",
            " Epoch 4\n",
            "[300/100] batch loss: 0.000511576347053051 \n",
            " Epoch 5\n",
            "[300/100] batch loss: 0.0005280552431941032 \n",
            " Epoch 6\n",
            "[300/100] batch loss: 0.000546879842877388 \n",
            " Epoch 7\n",
            "[300/100] batch loss: 0.0005297841504216194 \n",
            " Epoch 8\n",
            "[300/100] batch loss: 0.0005111865326762199 \n",
            " Epoch 9\n",
            "[300/100] batch loss: 0.0006034326180815696 \n",
            " Epoch 10\n",
            "[300/100] batch loss: 0.0005957712233066559 \n",
            " Epoch 11\n",
            "[300/100] batch loss: 0.0005628101900219917 \n",
            " Epoch 12\n",
            "[300/100] batch loss: 0.0005412783846259117 \n",
            " Epoch 13\n",
            "[300/100] batch loss: 0.0005099799111485481 \n",
            " Epoch 14\n",
            "[300/100] batch loss: 0.0005513664707541466 \n",
            " Epoch 15\n",
            "[300/100] batch loss: 0.0005243222787976265 \n",
            " Epoch 16\n",
            "[300/100] batch loss: 0.000538891889154911 \n",
            " Epoch 17\n",
            "[300/100] batch loss: 0.0005291115120053292 \n",
            " Epoch 18\n",
            "[300/100] batch loss: 0.0005483266711235047 \n",
            " Epoch 19\n",
            "[300/100] batch loss: 0.0005014241114258767 \n",
            " Epoch 20\n",
            "[300/100] batch loss: 0.0005161970481276512 \n",
            " Epoch 21\n",
            "[300/100] batch loss: 0.0005307255685329437 \n",
            " Epoch 22\n",
            "[300/100] batch loss: 0.0005233635380864143 \n",
            " Epoch 23\n",
            "[300/100] batch loss: 0.0005383281037211418 \n",
            " Epoch 24\n",
            "[300/100] batch loss: 0.0005240954831242561 \n",
            " Epoch 25\n",
            "[300/100] batch loss: 0.0004990082606673241 \n",
            " Epoch 26\n",
            "[300/100] batch loss: 0.0005293813720345498 \n",
            " Epoch 27\n",
            "[300/100] batch loss: 0.0005525104701519012 \n",
            " Epoch 28\n",
            "[300/100] batch loss: 0.0004908816888928413 \n",
            " Epoch 29\n",
            "[300/100] batch loss: 0.0005779965966939926 \n",
            " Epoch 30\n",
            "[300/100] batch loss: 0.0004974858462810516 Training process has finished. Saving trained model.\n",
            "Training loss for fold 0: 0.0005294353012642805\n",
            "Validation loss for fold 0: 0.0005389064184958652\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=204, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[300/100] batch loss: 0.0005380486696958542 \n",
            " Epoch 2\n",
            "[300/100] batch loss: 0.0004914722591638565 \n",
            " Epoch 3\n",
            "[300/100] batch loss: 0.0005000224336981773 \n",
            " Epoch 4\n",
            "[300/100] batch loss: 0.0004931369796395302 \n",
            " Epoch 5\n",
            "[300/100] batch loss: 0.0005590993911027909 \n",
            " Epoch 6\n",
            "[300/100] batch loss: 0.0005391272529959678 \n",
            " Epoch 7\n",
            "[300/100] batch loss: 0.000520176962018013 \n",
            " Epoch 8\n",
            "[300/100] batch loss: 0.00048769436776638033 \n",
            " Epoch 9\n",
            "[300/100] batch loss: 0.0005050726234912872 \n",
            " Epoch 10\n",
            "[300/100] batch loss: 0.0005102675035595894 \n",
            " Epoch 11\n",
            "[300/100] batch loss: 0.0005304127931594849 \n",
            " Epoch 12\n",
            "[300/100] batch loss: 0.0005002027377486229 \n",
            " Epoch 13\n",
            "[300/100] batch loss: 0.0005091645568609238 \n",
            " Epoch 14\n",
            "[300/100] batch loss: 0.0005359925329685211 \n",
            " Epoch 15\n",
            "[300/100] batch loss: 0.0005244684591889381 \n",
            " Epoch 16\n",
            "[300/100] batch loss: 0.0005483871325850486 \n",
            " Epoch 17\n",
            "[300/100] batch loss: 0.0005297224596142769 \n",
            " Epoch 18\n",
            "[300/100] batch loss: 0.0005013396963477135 \n",
            " Epoch 19\n",
            "[300/100] batch loss: 0.0005081713944673538 \n",
            " Epoch 20\n",
            "[300/100] batch loss: 0.0005357919260859489 \n",
            " Epoch 21\n",
            "[300/100] batch loss: 0.0004988756403326988 \n",
            " Epoch 22\n",
            "[300/100] batch loss: 0.0004884498566389084 \n",
            " Epoch 23\n",
            "[300/100] batch loss: 0.0005602099373936653 \n",
            " Epoch 24\n",
            "[300/100] batch loss: 0.0005027700215578079 \n",
            " Epoch 25\n",
            "[300/100] batch loss: 0.0005576878041028976 \n",
            " Epoch 26\n",
            "[300/100] batch loss: 0.0005313107371330262 \n",
            " Epoch 27\n",
            "[300/100] batch loss: 0.0005472931638360024 \n",
            " Epoch 28\n",
            "[300/100] batch loss: 0.0005327406525611878 \n",
            " Epoch 29\n",
            "[300/100] batch loss: 0.0005338750034570694 \n",
            " Epoch 30\n",
            "[300/100] batch loss: 0.0005191926285624504 Training process has finished. Saving trained model.\n",
            "Training loss for fold 1: 0.0005327620317576935\n",
            "Validation loss for fold 1: 0.0005254035515051169\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Reset trainable parameters of layer = Linear(in_features=204, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=64, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=64, out_features=128, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=128, out_features=192, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=192, out_features=204, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=50, bias=True)\n",
            "\n",
            " Epoch 1\n",
            "[300/100] batch loss: 0.0005188549682497978 \n",
            " Epoch 2\n",
            "[300/100] batch loss: 0.0005373736843466759 \n",
            " Epoch 3\n",
            "[300/100] batch loss: 0.0005202871933579445 \n",
            " Epoch 4\n",
            "[300/100] batch loss: 0.0005471439659595489 \n",
            " Epoch 5\n",
            "[300/100] batch loss: 0.0005405198037624359 \n",
            " Epoch 6\n",
            "[300/100] batch loss: 0.00046624086797237397 \n",
            " Epoch 7\n",
            "[300/100] batch loss: 0.0005516393482685089 \n",
            " Epoch 8\n",
            "[300/100] batch loss: 0.0005378486216068268 \n",
            " Epoch 9\n",
            "[300/100] batch loss: 0.0005297803506255149 \n",
            " Epoch 10\n",
            "[300/100] batch loss: 0.0005549880117177963 \n",
            " Epoch 11\n",
            "[300/100] batch loss: 0.0005040451884269715 \n",
            " Epoch 12\n",
            "[300/100] batch loss: 0.0004989208281040192 \n",
            " Epoch 13\n",
            "[300/100] batch loss: 0.000533202663064003 \n",
            " Epoch 14\n",
            "[300/100] batch loss: 0.0005519993230700493 \n",
            " Epoch 15\n",
            "[300/100] batch loss: 0.0005375378951430321 \n",
            " Epoch 16\n",
            "[300/100] batch loss: 0.00048449281603097917 \n",
            " Epoch 17\n",
            "[200/100] batch loss: 0.0005196741968393326 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FQbNAa_wH6nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_latents_df = pd.DataFrame(val_latents)\n",
        "val_outputs_df = pd.DataFrame(val_outputs)"
      ],
      "metadata": {
        "id": "1WQrvKcnWRwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_latents_df "
      ],
      "metadata": {
        "id": "D1_-nhkSWfOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_outputs_df"
      ],
      "metadata": {
        "id": "uXCF8InT2uEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_latents_df.corr()"
      ],
      "metadata": {
        "id": "pEidHMgs3tF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as mp\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "\n",
        "dataplot = sb.heatmap(val_latents_df.corr(), cmap=\"RdBu\", vmin=-1, vmax=1)"
      ],
      "metadata": {
        "id": "I-iWcj4t3lty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_outputs_df['mean'] = val_outputs_df.mean(axis=0)\n"
      ],
      "metadata": {
        "id": "u-q4aSXX3T-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (a1, a2, a3) = plt.subplots(3, 1, figsize=(15, 12))\n",
        "fig.subplots_adjust(hspace = 0.3)\n",
        "a1.set_title('Input data vs autocoder generated data 1')\n",
        "a1.plot(test_data[0], label = 'Sample 1 Input')\n",
        "a1.plot(val_outputs[0].round(), label = 'Sample 1 Output')\n",
        "a1.set_ylabel('Value')\n",
        "a1.set_xlabel('Variable')\n",
        "a1.legend()\n",
        "\n",
        "a2.set_title('Input data vs autocoder generated data 2')\n",
        "a2.plot(test_data[1], label = 'Sample 2 Input')\n",
        "a2.plot(val_outputs[1].round(), label = 'Sample 2 Output')\n",
        "a2.set_ylabel('Value')\n",
        "a2.set_xlabel('Variable')\n",
        "a2.legend()\n",
        "\n",
        "'''a3.set_title('Input data vs autocoder generated data mean')\n",
        "a3.plot(test_data.mean(), label = 'Sample Input Mean')\n",
        "a3.plot(val_outputs_df['mean'], label = 'Sample Output Mean')\n",
        "a3.set_ylabel('Value')\n",
        "a3.set_xlabel('Variable')\n",
        "a3.legend()'''"
      ],
      "metadata": {
        "id": "CFra19rlWp1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataplot = sb.heatmap(val_outputs_df.corr(), cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "P05GCS27DDwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = val_latents.mean(axis=0)\n",
        "mean = torch.from_numpy(mean)\n",
        "std = val_latents.std(axis=0)\n",
        "std = torch.from_numpy(std)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MIAuv7L95E5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(std)\n",
        "print(mean)"
      ],
      "metadata": {
        "id": "DGm568j4-bGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_lat = torch.normal(mean=mean, std=std)\n",
        "\n",
        "gen_data = net.decode(gen_lat)\n",
        "\n"
      ],
      "metadata": {
        "id": "D8X64FZd-VDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_data = gen_data.detach().numpy()"
      ],
      "metadata": {
        "id": "EXJUKDYrE734"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (a1, a2, a3) = plt.subplots(3, 1, figsize=(15, 12))\n",
        "fig.subplots_adjust(hspace = 0.3)\n",
        "a1.set_title('Input data vs autocoder generated data 1')\n",
        "a1.plot(gen_data.round(), label = 'Sample 1 Input')\n",
        "a1.plot(gen_data, label = 'Sample 1 Output')\n",
        "a1.set_ylabel('Value')\n",
        "a1.set_xlabel('Variable')\n",
        "a1.legend()\n",
        "\n",
        "a2.set_title('Input data vs autocoder generated data 2')\n",
        "a2.plot(test_data[1], label = 'Sample 2 Input')\n",
        "a2.plot(test_data[1], label = 'Sample 2 Output')\n",
        "a2.set_ylabel('Value')\n",
        "a2.set_xlabel('Variable')\n",
        "a2.legend()"
      ],
      "metadata": {
        "id": "jIw0q2R2BC_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}